{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECDF Generator - Polars Only\n",
    "\n",
    "This notebook uses **Polars exclusively** for the ECDF computation pipeline.\n",
    "\n",
    "**Features:**\n",
    "1. Extract ICU time windows from ADT table\n",
    "2. Filter labs/vitals to values during ICU stays only\n",
    "3. Standardize lab reference units\n",
    "4. Compute ECDF (distinct value/probability pairs)\n",
    "5. Compute quantile bins with auto-extreme-splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# For notebook: set project root relative to notebook location\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configs(\n",
    "    clif_config_path: str = None,\n",
    "    outlier_config_path: str = None,\n",
    "    lab_vital_config_path: str = None\n",
    ") -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Load all required configuration files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (clif_config, outlier_config, lab_vital_config)\n",
    "    \"\"\"\n",
    "    if clif_config_path is None:\n",
    "        clif_config_path = PROJECT_ROOT / 'config' / 'config.json'\n",
    "    if outlier_config_path is None:\n",
    "        outlier_config_path = PROJECT_ROOT / 'modules' / 'ecdf' / 'config' / 'outlier_config.yaml'\n",
    "    if lab_vital_config_path is None:\n",
    "        lab_vital_config_path = PROJECT_ROOT / 'modules' / 'ecdf' / 'config' / 'lab_vital_config.yaml'\n",
    "\n",
    "    with open(clif_config_path, 'r') as f:\n",
    "        clif_config = json.load(f)\n",
    "\n",
    "    with open(outlier_config_path, 'r') as f:\n",
    "        outlier_config = yaml.safe_load(f)\n",
    "\n",
    "    with open(lab_vital_config_path, 'r') as f:\n",
    "        lab_vital_config = yaml.safe_load(f)\n",
    "\n",
    "    return clif_config, outlier_config, lab_vital_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "clif_config, outlier_config, lab_vital_config = load_configs()\n",
    "\n",
    "print(\"CLIF Config:\")\n",
    "print(f\"  tables_path: {clif_config.get('tables_path')}\")\n",
    "print(f\"  file_type: {clif_config.get('file_type')}\")\n",
    "print()\n",
    "print(f\"Outlier config tables: {list(outlier_config.get('tables', {}).keys())}\")\n",
    "print(f\"Lab categories in config: {list(lab_vital_config.get('labs', {}).keys())}\")\n",
    "print(f\"Vital categories in config: {list(lab_vital_config.get('vitals', {}).keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_unit_for_filename(unit) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize unit string for use in filename.\n",
    "    Handles both string and list inputs.\n",
    "    \"\"\"\n",
    "    if unit is None:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # Handle list of units\n",
    "    if isinstance(unit, list):\n",
    "        unit = '_'.join(str(u) for u in unit if u is not None)\n",
    "    \n",
    "    if not unit:\n",
    "        return \"unknown\"\n",
    "\n",
    "    sanitized = unit.replace('/', '_').replace('%', 'pct').replace('Â°', 'deg')\n",
    "    sanitized = ''.join(c if c.isalnum() or c == '_' else '_' for c in sanitized)\n",
    "    while '__' in sanitized:\n",
    "        sanitized = sanitized.replace('__', '_')\n",
    "    sanitized = sanitized.strip('_').lower()\n",
    "\n",
    "    return sanitized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Standardization (using clifpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clifpy\n",
    "from clifpy.tables import Labs\n",
    "print(f\"clifpy location: {clifpy.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_labs_polars(\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    output_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Standardize lab reference units and save as parquet.\n",
    "    Uses clifpy with Polars LazyFrame input.\n",
    "    \"\"\"\n",
    "    labs_path = os.path.join(tables_path, f'clif_labs.{file_type}')\n",
    "    print(f\"Loading labs from {labs_path}...\")\n",
    "\n",
    "    # Load as Polars LazyFrame\n",
    "    labs_lazy = pl.scan_parquet(labs_path)\n",
    "    labs_inst = Labs(data=labs_lazy)\n",
    "    \n",
    "    # Standardize (clifpy handles the conversion internally)\n",
    "    labs_inst.standardize_reference_units(\n",
    "        save=True, \n",
    "        output_directory=str(Path(output_path).parent),\n",
    "        lowercase=True, \n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    # Get the result and save as parquet\n",
    "    # labs_inst.df should now be standardized\n",
    "    if isinstance(labs_inst.df, pl.LazyFrame):\n",
    "        labs_inst.df.collect().write_parquet(output_path)\n",
    "    elif isinstance(labs_inst.df, pl.DataFrame):\n",
    "        labs_inst.df.write_parquet(output_path)\n",
    "    else:\n",
    "        # Convert from pandas if needed\n",
    "        pl.from_pandas(labs_inst.df).write_parquet(output_path)\n",
    "    \n",
    "    print(f\"Saved standardized labs to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize labs (run once)\n",
    "STANDARDIZED_LABS_PATH = '../../output/intermediate/clif_labs_standardized_pl.parquet'\n",
    "os.makedirs(os.path.dirname(STANDARDIZED_LABS_PATH), exist_ok=True)\n",
    "\n",
    "# Uncomment to run standardization\n",
    "standardize_labs_polars(\n",
    "    clif_config['tables_path'],\n",
    "    clif_config['file_type'],\n",
    "    STANDARDIZED_LABS_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICU Time Window Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_icu_time_windows(\n",
    "    tables_path: str,\n",
    "    file_type: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract ICU time windows from ADT table using Polars.\n",
    "\n",
    "    Returns:\n",
    "        Polars DataFrame with columns:\n",
    "        - hospitalization_id: str\n",
    "        - in_dttm: datetime\n",
    "        - out_dttm: datetime\n",
    "    \"\"\"\n",
    "    adt_path = os.path.join(tables_path, f'clif_adt.{file_type}')\n",
    "    print(f\"Loading ICU time windows from {adt_path}...\")\n",
    "\n",
    "    icu_windows_df = (\n",
    "        pl.scan_parquet(adt_path)\n",
    "        .filter(pl.col('location_category').str.to_lowercase() == 'icu')\n",
    "        .select(['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "        .collect()\n",
    "        .with_columns([\n",
    "            pl.col('in_dttm').dt.replace_time_zone(None),\n",
    "            pl.col('out_dttm').dt.replace_time_zone(None)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(icu_windows_df):,} ICU time windows\")\n",
    "    return icu_windows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ICU time windows\n",
    "icu_windows = extract_icu_time_windows(\n",
    "    clif_config['tables_path'],\n",
    "    clif_config['file_type']\n",
    ")\n",
    "\n",
    "print(\"\\nSample ICU time windows:\")\n",
    "icu_windows.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECDF Computation (Polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ecdf_polars(values: pl.Series) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute ECDF using pure Polars operations.\n",
    "\n",
    "    Args:\n",
    "        values: Polars Series of numeric values\n",
    "\n",
    "    Returns:\n",
    "        Polars DataFrame with columns:\n",
    "        - value: float\n",
    "        - probability: float (0 to 1)\n",
    "    \"\"\"\n",
    "    if len(values) == 0:\n",
    "        return pl.DataFrame({'value': [], 'probability': []})\n",
    "\n",
    "    n = len(values)\n",
    "    \n",
    "    # Create DataFrame, sort, and compute cumulative probability\n",
    "    ecdf_df = (\n",
    "        pl.DataFrame({'value': values})\n",
    "        .sort('value')\n",
    "        .with_row_index('rank')\n",
    "        .with_columns(\n",
    "            ((pl.col('rank') + 1) / n).alias('probability')\n",
    "        )\n",
    "        .group_by('value')\n",
    "        .agg(pl.col('probability').max())\n",
    "        .sort('value')\n",
    "    )\n",
    "\n",
    "    return ecdf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ECDF computation\n",
    "test_values = pl.Series([1.0, 2.0, 2.0, 3.0, 4.0, 5.0, 5.0, 5.0, 6.0, 7.0])\n",
    "ecdf_result = compute_ecdf_polars(test_values)\n",
    "\n",
    "print(\"Test ECDF computation:\")\n",
    "print(f\"Input values: {test_values.to_list()}\")\n",
    "print(f\"\\nECDF result:\")\n",
    "ecdf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Functions (Polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quantile_bins_polars(\n",
    "    data: pl.Series,\n",
    "    num_bins: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create quantile bins using Polars.\n",
    "\n",
    "    Args:\n",
    "        data: Polars Series with values\n",
    "        num_bins: Number of bins to create\n",
    "\n",
    "    Returns:\n",
    "        List of bin dictionaries\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return []\n",
    "\n",
    "    if len(data) < num_bins * 2:\n",
    "        num_bins = max(1, len(data) // 2)\n",
    "\n",
    "    # Calculate quantile edges\n",
    "    quantiles = [i / num_bins for i in range(num_bins + 1)]\n",
    "    edges = data.quantile(quantiles)\n",
    "    \n",
    "    # Get unique edges to handle duplicates\n",
    "    edges_list = [data.quantile(q) for q in quantiles]\n",
    "    \n",
    "    bins = []\n",
    "    total_count = len(data)\n",
    "    \n",
    "    for i in range(len(edges_list) - 1):\n",
    "        bin_min = edges_list[i]\n",
    "        bin_max = edges_list[i + 1]\n",
    "        \n",
    "        # Skip duplicate edges\n",
    "        if bin_min == bin_max and i > 0:\n",
    "            continue\n",
    "        \n",
    "        # Count values in bin\n",
    "        if i == 0:\n",
    "            count = data.filter((data >= bin_min) & (data <= bin_max)).len()\n",
    "        else:\n",
    "            count = data.filter((data > bin_min) & (data <= bin_max)).len()\n",
    "        \n",
    "        if count > 0:\n",
    "            bins.append({\n",
    "                'bin_num': len(bins) + 1,\n",
    "                'bin_min': float(bin_min),\n",
    "                'bin_max': float(bin_max),\n",
    "                'count': int(count),\n",
    "                'percentage': float(count / total_count * 100)\n",
    "            })\n",
    "\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins_for_segment_polars(\n",
    "    data: pl.Series,\n",
    "    segment_min: float,\n",
    "    segment_max: float,\n",
    "    num_bins: int,\n",
    "    segment_name: str,\n",
    "    extra_bins_last: int = 0,\n",
    "    split_first: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create quantile-based bins for a segment using Polars.\n",
    "    \"\"\"\n",
    "    # Filter data to segment range\n",
    "    segment_data = data.filter((data >= segment_min) & (data <= segment_max))\n",
    "\n",
    "    if len(segment_data) == 0:\n",
    "        return []\n",
    "\n",
    "    if num_bins == 1 or len(segment_data) < num_bins * 2:\n",
    "        return [{\n",
    "            'segment': segment_name,\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(segment_data.min()),\n",
    "            'bin_max': float(segment_data.max()),\n",
    "            'count': len(segment_data),\n",
    "            'percentage': 100.0\n",
    "        }]\n",
    "\n",
    "    total_segment_count = len(segment_data)\n",
    "    \n",
    "    # Calculate quantile edges\n",
    "    quantiles = [i / num_bins for i in range(num_bins + 1)]\n",
    "    edges_list = [segment_data.quantile(q) for q in quantiles]\n",
    "    \n",
    "    bins = []\n",
    "    prev_max = None\n",
    "    \n",
    "    for i in range(len(edges_list) - 1):\n",
    "        bin_min = edges_list[i]\n",
    "        bin_max = edges_list[i + 1]\n",
    "        \n",
    "        if bin_min == bin_max and i > 0:\n",
    "            continue\n",
    "        \n",
    "        if i == 0:\n",
    "            count = segment_data.filter((segment_data >= bin_min) & (segment_data <= bin_max)).len()\n",
    "        else:\n",
    "            count = segment_data.filter((segment_data > bin_min) & (segment_data <= bin_max)).len()\n",
    "        \n",
    "        if count > 0:\n",
    "            bins.append({\n",
    "                'segment': segment_name,\n",
    "                'bin_num': len(bins) + 1,\n",
    "                'bin_min': float(bin_min),\n",
    "                'bin_max': float(bin_max),\n",
    "                'count': int(count),\n",
    "                'percentage': float(count / total_segment_count * 100)\n",
    "            })\n",
    "\n",
    "    # Handle extra bins for extreme values\n",
    "    if extra_bins_last > 0 and len(bins) > 0:\n",
    "        if split_first:\n",
    "            # Split first bin\n",
    "            extreme_bin = bins[0]\n",
    "            extreme_data = segment_data.filter(\n",
    "                (segment_data >= extreme_bin['bin_min']) & \n",
    "                (segment_data <= extreme_bin['bin_max'])\n",
    "            )\n",
    "            \n",
    "            if len(extreme_data) >= extra_bins_last * 2:\n",
    "                extra_quantiles = [i / extra_bins_last for i in range(extra_bins_last + 1)]\n",
    "                extra_edges = [extreme_data.quantile(q) for q in extra_quantiles]\n",
    "                \n",
    "                new_bins = []\n",
    "                for j in range(len(extra_edges) - 1):\n",
    "                    e_min, e_max = extra_edges[j], extra_edges[j + 1]\n",
    "                    if e_min == e_max and j > 0:\n",
    "                        continue\n",
    "                    if j == 0:\n",
    "                        e_count = extreme_data.filter((extreme_data >= e_min) & (extreme_data <= e_max)).len()\n",
    "                    else:\n",
    "                        e_count = extreme_data.filter((extreme_data > e_min) & (extreme_data <= e_max)).len()\n",
    "                    \n",
    "                    if e_count > 0:\n",
    "                        new_bins.append({\n",
    "                            'segment': segment_name,\n",
    "                            'bin_num': len(new_bins) + 1,\n",
    "                            'bin_min': float(e_min),\n",
    "                            'bin_max': float(e_max),\n",
    "                            'count': int(e_count),\n",
    "                            'percentage': float(e_count / total_segment_count * 100)\n",
    "                        })\n",
    "                \n",
    "                # Renumber and combine\n",
    "                for b in bins[1:]:\n",
    "                    b['bin_num'] = len(new_bins) + 1\n",
    "                    new_bins.append(b)\n",
    "                bins = new_bins\n",
    "        else:\n",
    "            # Split last bin\n",
    "            extreme_bin = bins[-1]\n",
    "            extreme_data = segment_data.filter(\n",
    "                (segment_data >= extreme_bin['bin_min']) & \n",
    "                (segment_data <= extreme_bin['bin_max'])\n",
    "            )\n",
    "            \n",
    "            if len(extreme_data) >= extra_bins_last * 2:\n",
    "                extra_quantiles = [i / extra_bins_last for i in range(extra_bins_last + 1)]\n",
    "                extra_edges = [extreme_data.quantile(q) for q in extra_quantiles]\n",
    "                \n",
    "                bins = bins[:-1]  # Remove last bin\n",
    "                \n",
    "                for j in range(len(extra_edges) - 1):\n",
    "                    e_min, e_max = extra_edges[j], extra_edges[j + 1]\n",
    "                    if e_min == e_max and j > 0:\n",
    "                        continue\n",
    "                    if j == 0:\n",
    "                        e_count = extreme_data.filter((extreme_data >= e_min) & (extreme_data <= e_max)).len()\n",
    "                    else:\n",
    "                        e_count = extreme_data.filter((extreme_data > e_min) & (extreme_data <= e_max)).len()\n",
    "                    \n",
    "                    if e_count > 0:\n",
    "                        bins.append({\n",
    "                            'segment': segment_name,\n",
    "                            'bin_num': len(bins) + 1,\n",
    "                            'bin_min': float(e_min),\n",
    "                            'bin_max': float(e_max),\n",
    "                            'count': int(e_count),\n",
    "                            'percentage': float(e_count / total_segment_count * 100)\n",
    "                        })\n",
    "\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_bins_polars(\n",
    "    data: pl.Series,\n",
    "    normal_lower: float,\n",
    "    normal_upper: float,\n",
    "    outlier_min: float,\n",
    "    outlier_max: float,\n",
    "    bins_below: int,\n",
    "    bins_normal: int,\n",
    "    bins_above: int,\n",
    "    extra_bins_below: int = 0,\n",
    "    extra_bins_above: int = 0\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create bins for all segments using Polars.\n",
    "    \"\"\"\n",
    "    all_bins = []\n",
    "\n",
    "    # Below normal segment\n",
    "    if bins_below > 0 and outlier_min < normal_lower:\n",
    "        below_bins = create_bins_for_segment_polars(\n",
    "            data, outlier_min, normal_lower, bins_below, 'below',\n",
    "            extra_bins_last=extra_bins_below,\n",
    "            split_first=True\n",
    "        )\n",
    "        all_bins.extend(below_bins)\n",
    "\n",
    "    # Normal segment\n",
    "    if bins_normal > 0:\n",
    "        normal_bins = create_bins_for_segment_polars(\n",
    "            data, normal_lower, normal_upper, bins_normal, 'normal'\n",
    "        )\n",
    "        all_bins.extend(normal_bins)\n",
    "\n",
    "    # Above normal segment\n",
    "    if bins_above > 0 and normal_upper < outlier_max:\n",
    "        above_bins = create_bins_for_segment_polars(\n",
    "            data, normal_upper, outlier_max, bins_above, 'above',\n",
    "            extra_bins_last=extra_bins_above,\n",
    "            split_first=False\n",
    "        )\n",
    "        all_bins.extend(above_bins)\n",
    "\n",
    "    return all_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test binning\n",
    "np.random.seed(42)\n",
    "test_values = pl.Series(np.random.randn(10000) * 20 + 100)\n",
    "\n",
    "bins = create_all_bins_polars(\n",
    "    data=test_values,\n",
    "    normal_lower=80,\n",
    "    normal_upper=120,\n",
    "    outlier_min=0,\n",
    "    outlier_max=200,\n",
    "    bins_below=3,\n",
    "    bins_normal=4,\n",
    "    bins_above=3,\n",
    "    extra_bins_below=5,\n",
    "    extra_bins_above=5\n",
    ")\n",
    "\n",
    "print(f\"Created {len(bins)} bins:\")\n",
    "for b in bins:\n",
    "    print(f\"  Bin {b['bin_num']:2d}: [{b['bin_min']:8.2f}, {b['bin_max']:8.2f}] - \"\n",
    "          f\"segment: {b['segment']:10s}, count: {b['count']:5d}, pct: {b['percentage']:5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Single Category (Labs/Vitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_polars(\n",
    "    table_type: str,\n",
    "    category: str,\n",
    "    unit: Optional[str],\n",
    "    icu_windows: pl.DataFrame,\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    outlier_range: Dict[str, float],\n",
    "    cat_config: Dict[str, Any],\n",
    "    output_dir: str = None,\n",
    "    extreme_bins_count: int = 5,\n",
    "    save_output: bool = False\n",
    ") -> Tuple[Dict[str, Any], pl.DataFrame, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process a single lab/vital category using pure Polars.\n",
    "    \"\"\"\n",
    "    # Determine file path and column names\n",
    "    if table_type == 'labs':\n",
    "        file_path = '../../output/intermediate/clif_labs_standardized_pl.parquet'\n",
    "        category_col = 'lab_category'\n",
    "        value_col = 'lab_value_numeric'\n",
    "        datetime_col = 'lab_result_dttm'\n",
    "    else:  # vitals\n",
    "        file_path = os.path.join(tables_path, f'clif_vitals.{file_type}')\n",
    "        category_col = 'vital_category'\n",
    "        value_col = 'vital_value'\n",
    "        datetime_col = 'recorded_dttm'\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "\n",
    "    display_name = f\"{category} ({unit})\" if table_type == 'labs' and unit else category\n",
    "    print(f\"Loading {display_name}...\")\n",
    "\n",
    "    # Build filter for category and unit\n",
    "    data_lazy = pl.scan_parquet(file_path)\n",
    "    \n",
    "    if table_type == 'labs' and unit:\n",
    "        if isinstance(unit, list):\n",
    "            unit_filter = pl.col('reference_unit').is_in(unit)\n",
    "        else:\n",
    "            unit_filter = pl.col('reference_unit') == unit\n",
    "        \n",
    "        data_category = data_lazy.filter(\n",
    "            (pl.col(category_col) == category) & unit_filter\n",
    "        ).select(['hospitalization_id', datetime_col, value_col])\n",
    "    else:\n",
    "        data_category = data_lazy.filter(\n",
    "            pl.col(category_col) == category\n",
    "        ).select(['hospitalization_id', datetime_col, value_col])\n",
    "\n",
    "    # Join with ICU time windows and filter to ICU stay times\n",
    "    values_df = (\n",
    "        data_category\n",
    "        .join(icu_windows.lazy(), on='hospitalization_id', how='inner')\n",
    "        .filter(\n",
    "            (pl.col(datetime_col).dt.replace_time_zone(None) >= pl.col('in_dttm')) &\n",
    "            (pl.col(datetime_col).dt.replace_time_zone(None) <= pl.col('out_dttm'))\n",
    "        )\n",
    "        .select([value_col])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    original_count = len(values_df)\n",
    "    print(f\"  Original count: {original_count:,}\")\n",
    "\n",
    "    if original_count == 0:\n",
    "        print(f\"  WARNING: No data found for {display_name} during ICU stays\")\n",
    "        return (\n",
    "            {'category': category, 'unit': unit, 'original_count': 0, 'clean_count': 0},\n",
    "            pl.DataFrame({'value': [], 'probability': []}),\n",
    "            pl.DataFrame()\n",
    "        )\n",
    "\n",
    "    # Remove outliers\n",
    "    values_clean = values_df.filter(\n",
    "        (pl.col(value_col) >= outlier_range['min']) &\n",
    "        (pl.col(value_col) <= outlier_range['max'])\n",
    "    )\n",
    "\n",
    "    clean_count = len(values_clean)\n",
    "    print(f\"  After outlier removal: {clean_count:,} (removed {original_count - clean_count:,})\")\n",
    "\n",
    "    if clean_count == 0:\n",
    "        print(f\"  WARNING: No data remaining after outlier removal\")\n",
    "        return (\n",
    "            {'category': category, 'unit': unit, 'original_count': original_count, 'clean_count': 0},\n",
    "            pl.DataFrame({'value': [], 'probability': []}),\n",
    "            pl.DataFrame()\n",
    "        )\n",
    "\n",
    "    # Get values as Polars Series\n",
    "    values_series = values_clean[value_col]\n",
    "\n",
    "    # Compute ECDF\n",
    "    ecdf_df = compute_ecdf_polars(values_series)\n",
    "    print(f\"  ECDF: {len(ecdf_df):,} distinct pairs (compression: {clean_count / len(ecdf_df):.1f}x)\")\n",
    "\n",
    "    # Compute Bins\n",
    "    bins_below = cat_config['bins']['below_normal']\n",
    "    bins_normal = cat_config['bins']['normal']\n",
    "    bins_above = cat_config['bins']['above_normal']\n",
    "\n",
    "    extra_bins_below = extreme_bins_count if bins_below > 1 else 0\n",
    "    extra_bins_above = extreme_bins_count if bins_above > 1 else 0\n",
    "\n",
    "    bins = create_all_bins_polars(\n",
    "        data=values_series,\n",
    "        normal_lower=cat_config['normal_range']['lower'],\n",
    "        normal_upper=cat_config['normal_range']['upper'],\n",
    "        outlier_min=outlier_range['min'],\n",
    "        outlier_max=outlier_range['max'],\n",
    "        bins_below=bins_below,\n",
    "        bins_normal=bins_normal,\n",
    "        bins_above=bins_above,\n",
    "        extra_bins_below=extra_bins_below,\n",
    "        extra_bins_above=extra_bins_above\n",
    "    )\n",
    "\n",
    "    # Add interval notation\n",
    "    for bin_info in bins:\n",
    "        if bin_info['bin_num'] == 1:\n",
    "            interval = f\"[{bin_info['bin_min']:.2f}, {bin_info['bin_max']:.2f}]\"\n",
    "        else:\n",
    "            interval = f\"({bin_info['bin_min']:.2f}, {bin_info['bin_max']:.2f}]\"\n",
    "        bin_info['interval'] = interval\n",
    "\n",
    "    bins_df = pl.DataFrame(bins) if bins else pl.DataFrame()\n",
    "    print(f\"  Bins: {len(bins_df)}\")\n",
    "\n",
    "    # Save output\n",
    "    if save_output and output_dir:\n",
    "        if table_type == 'labs' and unit:\n",
    "            unit_for_filename = unit[0] if isinstance(unit, list) else unit\n",
    "            unit_safe = sanitize_unit_for_filename(unit_for_filename)\n",
    "            filename = f'{category}_{unit_safe}.parquet'\n",
    "        else:\n",
    "            filename = f'{category}.parquet'\n",
    "\n",
    "        ecdf_dir = os.path.join(output_dir, 'ecdf', table_type)\n",
    "        os.makedirs(ecdf_dir, exist_ok=True)\n",
    "        ecdf_df.write_parquet(os.path.join(ecdf_dir, filename))\n",
    "\n",
    "        bins_dir = os.path.join(output_dir, 'bins', table_type)\n",
    "        os.makedirs(bins_dir, exist_ok=True)\n",
    "        bins_df.write_parquet(os.path.join(bins_dir, filename))\n",
    "\n",
    "        print(f\"  Saved to {output_dir}\")\n",
    "\n",
    "    stats = {\n",
    "        'category': category,\n",
    "        'unit': unit if table_type == 'labs' else None,\n",
    "        'original_count': original_count,\n",
    "        'clean_count': clean_count,\n",
    "        'ecdf_distinct_pairs': len(ecdf_df),\n",
    "        'num_bins': len(bins)\n",
    "    }\n",
    "\n",
    "    return stats, ecdf_df, bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Process a Single Lab Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_config = lab_vital_config.get('labs', {})\n",
    "labs_outlier = outlier_config['tables']['labs']['lab_value_numeric']\n",
    "\n",
    "print(\"Available lab categories in config:\")\n",
    "for cat in sorted(labs_config.keys()):\n",
    "    if isinstance(labs_config.get(cat), dict):\n",
    "        in_outlier = cat in labs_outlier\n",
    "        print(f\"  {cat}: outlier config = {in_outlier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LAB_CATEGORY = 'albumin'\n",
    "\n",
    "if TEST_LAB_CATEGORY in labs_config and TEST_LAB_CATEGORY in labs_outlier:\n",
    "    cat_config = labs_config[TEST_LAB_CATEGORY]\n",
    "    if cat_config and isinstance(cat_config, dict):\n",
    "        test_unit = cat_config.get('reference_unit')\n",
    "        if isinstance(test_unit, str):\n",
    "            test_unit = test_unit.lower()\n",
    "        elif isinstance(test_unit, list):\n",
    "            test_unit = [u.lower() if isinstance(u, str) else u for u in test_unit]\n",
    "        \n",
    "        stats, ecdf_df, bins_df = process_category_polars(\n",
    "            table_type='labs',\n",
    "            category=TEST_LAB_CATEGORY,\n",
    "            unit=test_unit,\n",
    "            icu_windows=icu_windows,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=labs_outlier[TEST_LAB_CATEGORY],\n",
    "            cat_config=cat_config,\n",
    "            extreme_bins_count=5,\n",
    "            save_output=False\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nStats: {stats}\")\n",
    "else:\n",
    "    print(f\"Category '{TEST_LAB_CATEGORY}' not found in config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ECDF (first 20 rows):\")\n",
    "ecdf_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bins:\")\n",
    "bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Process a Single Vital Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_config = lab_vital_config.get('vitals', {})\n",
    "vitals_outlier = outlier_config['tables']['vitals']['vital_value']\n",
    "\n",
    "TEST_VITAL_CATEGORY = 'heart_rate'\n",
    "\n",
    "if TEST_VITAL_CATEGORY in vitals_config and TEST_VITAL_CATEGORY in vitals_outlier:\n",
    "    extreme_bins = 5 if TEST_VITAL_CATEGORY in ['height_cm', 'weight_kg'] else 10\n",
    "    \n",
    "    vital_stats, vital_ecdf_df, vital_bins_df = process_category_polars(\n",
    "        table_type='vitals',\n",
    "        category=TEST_VITAL_CATEGORY,\n",
    "        unit=None,\n",
    "        icu_windows=icu_windows,\n",
    "        tables_path=clif_config['tables_path'],\n",
    "        file_type=clif_config['file_type'],\n",
    "        outlier_range=vitals_outlier[TEST_VITAL_CATEGORY],\n",
    "        cat_config=vitals_config[TEST_VITAL_CATEGORY],\n",
    "        extreme_bins_count=extreme_bins,\n",
    "        save_output=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStats: {vital_stats}\")\n",
    "else:\n",
    "    print(f\"Category '{TEST_VITAL_CATEGORY}' not found in config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize ECDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ecdf(ecdf_df: pl.DataFrame, title: str = \"ECDF\"):\n",
    "    \"\"\"Plot ECDF from a Polars DataFrame.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    values = ecdf_df['value'].to_numpy()\n",
    "    probs = ecdf_df['probability'].to_numpy()\n",
    "    \n",
    "    ax.step(values, probs, where='post', linewidth=1.5)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Cumulative Probability')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 'ecdf_df' in dir() and len(ecdf_df) > 0:\n",
    "    plot_ecdf(ecdf_df, f\"ECDF: {TEST_LAB_CATEGORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = str(PROJECT_ROOT / 'output' / 'final')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Process all labs from config\n",
    "for category, cat_config in labs_config.items():\n",
    "    if cat_config is None:\n",
    "        print(f\"WARNING: Category '{category}' has None config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(cat_config, dict):\n",
    "        continue\n",
    "\n",
    "    if category not in labs_outlier:\n",
    "        print(f\"WARNING: Category '{category}' not in outlier config, skipping\")\n",
    "        continue\n",
    "\n",
    "    config_unit = cat_config.get('reference_unit')\n",
    "    if config_unit is None:\n",
    "        print(f\"WARNING: Category '{category}' has no reference_unit in config, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Lowercase to match standardized data\n",
    "    if isinstance(config_unit, str):\n",
    "        unit = config_unit.lower()\n",
    "    elif isinstance(config_unit, list):\n",
    "        unit = [u.lower() if isinstance(u, str) else u for u in config_unit]\n",
    "    else:\n",
    "        unit = config_unit\n",
    "\n",
    "    stats, ecdf_df, bins_df = process_category_polars(\n",
    "        table_type='labs',\n",
    "        category=category,\n",
    "        unit=unit,\n",
    "        icu_windows=icu_windows,\n",
    "        tables_path=clif_config['tables_path'],\n",
    "        file_type=clif_config['file_type'],\n",
    "        outlier_range=labs_outlier[category],\n",
    "        cat_config=cat_config,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        extreme_bins_count=5,\n",
    "        save_output=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
