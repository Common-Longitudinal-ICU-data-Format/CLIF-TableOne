{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECDF Generator - Combined (Polars Only)\n",
    "\n",
    "This notebook processes **Labs, Vitals, and Respiratory Support** using **Polars exclusively**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configs(\n",
    "    clif_config_path: str = None,\n",
    "    outlier_config_path: str = None,\n",
    "    lab_vital_config_path: str = None\n",
    ") -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Load all required configuration files.\n",
    "    \"\"\"\n",
    "    if clif_config_path is None:\n",
    "        clif_config_path = PROJECT_ROOT / 'config' / 'config.json'\n",
    "    if outlier_config_path is None:\n",
    "        outlier_config_path = PROJECT_ROOT / 'modules' / 'ecdf' / 'config' / 'outlier_config.yaml'\n",
    "    if lab_vital_config_path is None:\n",
    "        lab_vital_config_path = PROJECT_ROOT / 'modules' / 'ecdf' / 'config' / 'lab_vital_config.yaml'\n",
    "\n",
    "    with open(clif_config_path, 'r') as f:\n",
    "        clif_config = json.load(f)\n",
    "\n",
    "    with open(outlier_config_path, 'r') as f:\n",
    "        outlier_config = yaml.safe_load(f)\n",
    "\n",
    "    with open(lab_vital_config_path, 'r') as f:\n",
    "        lab_vital_config = yaml.safe_load(f)\n",
    "\n",
    "    return clif_config, outlier_config, lab_vital_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clif_config, outlier_config, lab_vital_config = load_configs()\n",
    "\n",
    "print(\"CLIF Config:\")\n",
    "print(f\"  tables_path: {clif_config.get('tables_path')}\")\n",
    "print(f\"  file_type: {clif_config.get('file_type')}\")\n",
    "print()\n",
    "print(f\"Outlier config tables: {list(outlier_config.get('tables', {}).keys())}\")\n",
    "print(f\"Lab categories in config: {list(lab_vital_config.get('labs', {}).keys())}\")\n",
    "print(f\"Vital categories in config: {list(lab_vital_config.get('vitals', {}).keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_unit_for_filename(unit) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize unit string for use in filename.\n",
    "    Handles both string and list inputs.\n",
    "    \"\"\"\n",
    "    if unit is None:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    if isinstance(unit, list):\n",
    "        unit = '_'.join(str(u) for u in unit if u is not None)\n",
    "    \n",
    "    if not unit:\n",
    "        return \"unknown\"\n",
    "\n",
    "    sanitized = unit.replace('/', '_').replace('%', 'pct').replace('Â°', 'deg')\n",
    "    sanitized = ''.join(c if c.isalnum() or c == '_' else '_' for c in sanitized)\n",
    "    while '__' in sanitized:\n",
    "        sanitized = sanitized.replace('__', '_')\n",
    "    sanitized = sanitized.strip('_').lower()\n",
    "\n",
    "    return sanitized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Standardization (using clifpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clifpy\n",
    "from clifpy.tables import Labs\n",
    "print(f\"clifpy location: {clifpy.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_labs_polars(\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    output_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Standardize lab reference units and save as parquet.\n",
    "    \"\"\"\n",
    "    labs_path = os.path.join(tables_path, f'clif_labs.{file_type}')\n",
    "    print(f\"Loading labs from {labs_path}...\")\n",
    "\n",
    "    labs_lazy = pl.scan_parquet(labs_path)\n",
    "    labs_inst = Labs(data=labs_lazy)\n",
    "    \n",
    "    labs_inst.standardize_reference_units(\n",
    "        save=True, \n",
    "        output_directory=str(Path(output_path).parent),\n",
    "        lowercase=True, \n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    if isinstance(labs_inst.df, pl.LazyFrame):\n",
    "        labs_inst.df.collect().write_parquet(output_path)\n",
    "    elif isinstance(labs_inst.df, pl.DataFrame):\n",
    "        labs_inst.df.write_parquet(output_path)\n",
    "    else:\n",
    "        pl.from_pandas(labs_inst.df).write_parquet(output_path)\n",
    "    \n",
    "    print(f\"Saved standardized labs to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize labs (run once)\n",
    "STANDARDIZED_LABS_PATH = '../../output/intermediate/clif_labs_standardized.parquet'\n",
    "os.makedirs(os.path.dirname(STANDARDIZED_LABS_PATH), exist_ok=True)\n",
    "\n",
    "# Uncomment to run standardization\n",
    "standardize_labs_polars(\n",
    "    clif_config['tables_path'],\n",
    "    clif_config['file_type'],\n",
    "    STANDARDIZED_LABS_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICU Time Window Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_icu_time_windows(\n",
    "    tables_path: str,\n",
    "    file_type: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract ICU time windows from ADT table using Polars.\n",
    "    \"\"\"\n",
    "    adt_path = os.path.join(tables_path, f'clif_adt.{file_type}')\n",
    "    print(f\"Loading ICU time windows from {adt_path}...\")\n",
    "\n",
    "    icu_windows_df = (\n",
    "        pl.scan_parquet(adt_path)\n",
    "        .filter(pl.col('location_category').str.to_lowercase() == 'icu')\n",
    "        .select(['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "        .collect()\n",
    "        .with_columns([\n",
    "            pl.col('in_dttm').dt.replace_time_zone(None),\n",
    "            pl.col('out_dttm').dt.replace_time_zone(None)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(icu_windows_df):,} ICU time windows\")\n",
    "    return icu_windows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_windows = extract_icu_time_windows(\n",
    "    clif_config['tables_path'],\n",
    "    clif_config['file_type']\n",
    ")\n",
    "\n",
    "print(\"\\nSample ICU time windows:\")\n",
    "icu_windows.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECDF Computation (Polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ecdf_polars(values: pl.Series) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute ECDF using pure Polars operations.\n",
    "    \"\"\"\n",
    "    if len(values) == 0:\n",
    "        return pl.DataFrame({'value': [], 'probability': []})\n",
    "\n",
    "    n = len(values)\n",
    "    \n",
    "    ecdf_df = (\n",
    "        pl.DataFrame({'value': values})\n",
    "        .sort('value')\n",
    "        .with_row_index('rank')\n",
    "        .with_columns(\n",
    "            ((pl.col('rank') + 1) / n).alias('probability')\n",
    "        )\n",
    "        .group_by('value')\n",
    "        .agg(pl.col('probability').max())\n",
    "        .sort('value')\n",
    "    )\n",
    "\n",
    "    return ecdf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Functions (Polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flat_bins_polars(\n",
    "    data: pl.Series,\n",
    "    num_bins: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create flat quantile bins using Polars.\n",
    "    No segmentation - used for respiratory support columns.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return []\n",
    "\n",
    "    data = data.drop_nulls()\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        return []\n",
    "\n",
    "    if len(data) < num_bins * 2:\n",
    "        num_bins = max(1, len(data) // 2)\n",
    "\n",
    "    total_count = len(data)\n",
    "    \n",
    "    quantiles = [i / num_bins for i in range(num_bins + 1)]\n",
    "    edges_list = [data.quantile(q) for q in quantiles]\n",
    "    \n",
    "    bins = []\n",
    "    \n",
    "    for i in range(len(edges_list) - 1):\n",
    "        bin_min = edges_list[i]\n",
    "        bin_max = edges_list[i + 1]\n",
    "        \n",
    "        if bin_min == bin_max and i > 0:\n",
    "            continue\n",
    "        \n",
    "        if i == 0:\n",
    "            count = data.filter((data >= bin_min) & (data <= bin_max)).len()\n",
    "            interval_str = f\"[{bin_min:.2f}, {bin_max:.2f}]\"\n",
    "        else:\n",
    "            count = data.filter((data > bin_min) & (data <= bin_max)).len()\n",
    "            interval_str = f\"({bin_min:.2f}, {bin_max:.2f}]\"\n",
    "        \n",
    "        if count > 0:\n",
    "            bins.append({\n",
    "                'bin_num': len(bins) + 1,\n",
    "                'bin_min': float(bin_min),\n",
    "                'bin_max': float(bin_max),\n",
    "                'count': int(count),\n",
    "                'percentage': float(count / total_count * 100),\n",
    "                'interval': interval_str\n",
    "            })\n",
    "\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins_for_segment_polars(\n",
    "    data: pl.Series,\n",
    "    segment_min: float,\n",
    "    segment_max: float,\n",
    "    num_bins: int,\n",
    "    segment_name: str,\n",
    "    extra_bins_last: int = 0,\n",
    "    split_first: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create quantile-based bins for a segment using Polars.\n",
    "    \"\"\"\n",
    "    segment_data = data.filter((data >= segment_min) & (data <= segment_max))\n",
    "\n",
    "    if len(segment_data) == 0:\n",
    "        return []\n",
    "\n",
    "    if num_bins == 1 or len(segment_data) < num_bins * 2:\n",
    "        return [{\n",
    "            'segment': segment_name,\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(segment_data.min()),\n",
    "            'bin_max': float(segment_data.max()),\n",
    "            'count': len(segment_data),\n",
    "            'percentage': 100.0\n",
    "        }]\n",
    "\n",
    "    total_segment_count = len(segment_data)\n",
    "    \n",
    "    quantiles = [i / num_bins for i in range(num_bins + 1)]\n",
    "    edges_list = [segment_data.quantile(q) for q in quantiles]\n",
    "    \n",
    "    bins = []\n",
    "    \n",
    "    for i in range(len(edges_list) - 1):\n",
    "        bin_min = edges_list[i]\n",
    "        bin_max = edges_list[i + 1]\n",
    "        \n",
    "        if bin_min == bin_max and i > 0:\n",
    "            continue\n",
    "        \n",
    "        if i == 0:\n",
    "            count = segment_data.filter((segment_data >= bin_min) & (segment_data <= bin_max)).len()\n",
    "        else:\n",
    "            count = segment_data.filter((segment_data > bin_min) & (segment_data <= bin_max)).len()\n",
    "        \n",
    "        if count > 0:\n",
    "            bins.append({\n",
    "                'segment': segment_name,\n",
    "                'bin_num': len(bins) + 1,\n",
    "                'bin_min': float(bin_min),\n",
    "                'bin_max': float(bin_max),\n",
    "                'count': int(count),\n",
    "                'percentage': float(count / total_segment_count * 100)\n",
    "            })\n",
    "\n",
    "    # Handle extra bins for extreme values\n",
    "    if extra_bins_last > 0 and len(bins) > 0:\n",
    "        if split_first:\n",
    "            extreme_bin = bins[0]\n",
    "            extreme_data = segment_data.filter(\n",
    "                (segment_data >= extreme_bin['bin_min']) & \n",
    "                (segment_data <= extreme_bin['bin_max'])\n",
    "            )\n",
    "            \n",
    "            if len(extreme_data) >= extra_bins_last * 2:\n",
    "                extra_quantiles = [i / extra_bins_last for i in range(extra_bins_last + 1)]\n",
    "                extra_edges = [extreme_data.quantile(q) for q in extra_quantiles]\n",
    "                \n",
    "                new_bins = []\n",
    "                for j in range(len(extra_edges) - 1):\n",
    "                    e_min, e_max = extra_edges[j], extra_edges[j + 1]\n",
    "                    if e_min == e_max and j > 0:\n",
    "                        continue\n",
    "                    if j == 0:\n",
    "                        e_count = extreme_data.filter((extreme_data >= e_min) & (extreme_data <= e_max)).len()\n",
    "                    else:\n",
    "                        e_count = extreme_data.filter((extreme_data > e_min) & (extreme_data <= e_max)).len()\n",
    "                    \n",
    "                    if e_count > 0:\n",
    "                        new_bins.append({\n",
    "                            'segment': segment_name,\n",
    "                            'bin_num': len(new_bins) + 1,\n",
    "                            'bin_min': float(e_min),\n",
    "                            'bin_max': float(e_max),\n",
    "                            'count': int(e_count),\n",
    "                            'percentage': float(e_count / total_segment_count * 100)\n",
    "                        })\n",
    "                \n",
    "                for b in bins[1:]:\n",
    "                    b['bin_num'] = len(new_bins) + 1\n",
    "                    new_bins.append(b)\n",
    "                bins = new_bins\n",
    "        else:\n",
    "            extreme_bin = bins[-1]\n",
    "            extreme_data = segment_data.filter(\n",
    "                (segment_data >= extreme_bin['bin_min']) & \n",
    "                (segment_data <= extreme_bin['bin_max'])\n",
    "            )\n",
    "            \n",
    "            if len(extreme_data) >= extra_bins_last * 2:\n",
    "                extra_quantiles = [i / extra_bins_last for i in range(extra_bins_last + 1)]\n",
    "                extra_edges = [extreme_data.quantile(q) for q in extra_quantiles]\n",
    "                \n",
    "                bins = bins[:-1]\n",
    "                \n",
    "                for j in range(len(extra_edges) - 1):\n",
    "                    e_min, e_max = extra_edges[j], extra_edges[j + 1]\n",
    "                    if e_min == e_max and j > 0:\n",
    "                        continue\n",
    "                    if j == 0:\n",
    "                        e_count = extreme_data.filter((extreme_data >= e_min) & (extreme_data <= e_max)).len()\n",
    "                    else:\n",
    "                        e_count = extreme_data.filter((extreme_data > e_min) & (extreme_data <= e_max)).len()\n",
    "                    \n",
    "                    if e_count > 0:\n",
    "                        bins.append({\n",
    "                            'segment': segment_name,\n",
    "                            'bin_num': len(bins) + 1,\n",
    "                            'bin_min': float(e_min),\n",
    "                            'bin_max': float(e_max),\n",
    "                            'count': int(e_count),\n",
    "                            'percentage': float(e_count / total_segment_count * 100)\n",
    "                        })\n",
    "\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_bins_polars(\n",
    "    data: pl.Series,\n",
    "    normal_lower: float,\n",
    "    normal_upper: float,\n",
    "    outlier_min: float,\n",
    "    outlier_max: float,\n",
    "    bins_below: int,\n",
    "    bins_normal: int,\n",
    "    bins_above: int,\n",
    "    extra_bins_below: int = 0,\n",
    "    extra_bins_above: int = 0\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create bins for all segments (below/normal/above) using Polars.\n",
    "    Used for labs and vitals.\n",
    "    \"\"\"\n",
    "    all_bins = []\n",
    "\n",
    "    if bins_below > 0 and outlier_min < normal_lower:\n",
    "        below_bins = create_bins_for_segment_polars(\n",
    "            data, outlier_min, normal_lower, bins_below, 'below',\n",
    "            extra_bins_last=extra_bins_below,\n",
    "            split_first=True\n",
    "        )\n",
    "        all_bins.extend(below_bins)\n",
    "\n",
    "    if bins_normal > 0:\n",
    "        normal_bins = create_bins_for_segment_polars(\n",
    "            data, normal_lower, normal_upper, bins_normal, 'normal'\n",
    "        )\n",
    "        all_bins.extend(normal_bins)\n",
    "\n",
    "    if bins_above > 0 and normal_upper < outlier_max:\n",
    "        above_bins = create_bins_for_segment_polars(\n",
    "            data, normal_upper, outlier_max, bins_above, 'above',\n",
    "            extra_bins_last=extra_bins_above,\n",
    "            split_first=False\n",
    "        )\n",
    "        all_bins.extend(above_bins)\n",
    "\n",
    "    return all_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Labs/Vitals (Polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_polars(\n",
    "    table_type: str,\n",
    "    category: str,\n",
    "    unit: Optional[str],\n",
    "    icu_windows: pl.DataFrame,\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    outlier_range: Dict[str, float],\n",
    "    cat_config: Dict[str, Any],\n",
    "    output_dir: str = None,\n",
    "    extreme_bins_count: int = 5,\n",
    "    save_output: bool = False\n",
    ") -> Tuple[Dict[str, Any], pl.DataFrame, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process a single lab/vital category using pure Polars.\n",
    "    \"\"\"\n",
    "    if table_type == 'labs':\n",
    "        file_path = '../../output/intermediate/clif_labs_standardized.parquet'\n",
    "        category_col = 'lab_category'\n",
    "        value_col = 'lab_value_numeric'\n",
    "        datetime_col = 'lab_result_dttm'\n",
    "    else:\n",
    "        file_path = os.path.join(tables_path, f'clif_vitals.{file_type}')\n",
    "        category_col = 'vital_category'\n",
    "        value_col = 'vital_value'\n",
    "        datetime_col = 'recorded_dttm'\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "\n",
    "    display_name = f\"{category} ({unit})\" if table_type == 'labs' and unit else category\n",
    "    print(f\"Processing {display_name}...\")\n",
    "\n",
    "    data_lazy = pl.scan_parquet(file_path)\n",
    "    \n",
    "    if table_type == 'labs' and unit:\n",
    "        if isinstance(unit, list):\n",
    "            unit_filter = pl.col('reference_unit').is_in(unit)\n",
    "            if '(no units)' in unit:\n",
    "                unit_filter = unit_filter | pl.col('reference_unit').is_null()\n",
    "        else:\n",
    "            if unit == '(no units)':\n",
    "                unit_filter = pl.col('reference_unit').is_null()\n",
    "            else:\n",
    "                unit_filter = pl.col('reference_unit') == unit\n",
    "        \n",
    "        data_category = data_lazy.filter(\n",
    "            (pl.col(category_col) == category) & unit_filter\n",
    "        ).select(['hospitalization_id', datetime_col, value_col])\n",
    "    else:\n",
    "        data_category = data_lazy.filter(\n",
    "            pl.col(category_col) == category\n",
    "        ).select(['hospitalization_id', datetime_col, value_col])\n",
    "\n",
    "    values_df = (\n",
    "        data_category\n",
    "        .join(icu_windows.lazy(), on='hospitalization_id', how='inner')\n",
    "        .filter(\n",
    "            (pl.col(datetime_col).dt.replace_time_zone(None) >= pl.col('in_dttm')) &\n",
    "            (pl.col(datetime_col).dt.replace_time_zone(None) <= pl.col('out_dttm'))\n",
    "        )\n",
    "        .select([value_col])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    original_count = len(values_df)\n",
    "    print(f\"  Original count: {original_count:,}\")\n",
    "\n",
    "    if original_count == 0:\n",
    "        print(f\"  WARNING: No data found for {display_name} during ICU stays\")\n",
    "        return (\n",
    "            {'category': category, 'unit': unit, 'original_count': 0, 'clean_count': 0},\n",
    "            pl.DataFrame({'value': [], 'probability': []}),\n",
    "            pl.DataFrame()\n",
    "        )\n",
    "\n",
    "    values_clean = values_df.filter(\n",
    "        (pl.col(value_col) >= outlier_range['min']) &\n",
    "        (pl.col(value_col) <= outlier_range['max'])\n",
    "    )\n",
    "\n",
    "    clean_count = len(values_clean)\n",
    "    print(f\"  After outlier removal: {clean_count:,} (removed {original_count - clean_count:,})\")\n",
    "\n",
    "    if clean_count == 0:\n",
    "        print(f\"  WARNING: No data remaining after outlier removal\")\n",
    "        return (\n",
    "            {'category': category, 'unit': unit, 'original_count': original_count, 'clean_count': 0},\n",
    "            pl.DataFrame({'value': [], 'probability': []}),\n",
    "            pl.DataFrame()\n",
    "        )\n",
    "\n",
    "    values_series = values_clean[value_col]\n",
    "\n",
    "    # Compute ECDF\n",
    "    ecdf_df = compute_ecdf_polars(values_series)\n",
    "    print(f\"  ECDF: {len(ecdf_df):,} distinct pairs (compression: {clean_count / len(ecdf_df):.1f}x)\")\n",
    "\n",
    "    # Compute Bins\n",
    "    bins_config = cat_config.get('bins', {})\n",
    "    bins_below = bins_config.get('below_normal', 0) or 0\n",
    "    bins_normal = bins_config.get('normal', 0) or 0\n",
    "    bins_above = bins_config.get('above_normal', 0) or 0\n",
    "\n",
    "    extra_bins_below = extreme_bins_count if bins_below > 1 else 0\n",
    "    extra_bins_above = extreme_bins_count if bins_above > 1 else 0\n",
    "\n",
    "    normal_range = cat_config.get('normal_range', {})\n",
    "    normal_lower = normal_range.get('lower', outlier_range['min'])\n",
    "    normal_upper = normal_range.get('upper', outlier_range['max'])\n",
    "\n",
    "    bins = create_all_bins_polars(\n",
    "        data=values_series,\n",
    "        normal_lower=normal_lower,\n",
    "        normal_upper=normal_upper,\n",
    "        outlier_min=outlier_range['min'],\n",
    "        outlier_max=outlier_range['max'],\n",
    "        bins_below=bins_below,\n",
    "        bins_normal=bins_normal,\n",
    "        bins_above=bins_above,\n",
    "        extra_bins_below=extra_bins_below,\n",
    "        extra_bins_above=extra_bins_above\n",
    "    )\n",
    "\n",
    "    for bin_info in bins:\n",
    "        if bin_info['bin_num'] == 1:\n",
    "            interval = f\"[{bin_info['bin_min']:.2f}, {bin_info['bin_max']:.2f}]\"\n",
    "        else:\n",
    "            interval = f\"({bin_info['bin_min']:.2f}, {bin_info['bin_max']:.2f}]\"\n",
    "        bin_info['interval'] = interval\n",
    "\n",
    "    bins_df = pl.DataFrame(bins) if bins else pl.DataFrame()\n",
    "    print(f\"  Bins: {len(bins_df)}\")\n",
    "\n",
    "    if save_output and output_dir:\n",
    "        if table_type == 'labs' and unit:\n",
    "            unit_for_filename = unit[0] if isinstance(unit, list) else unit\n",
    "            unit_safe = sanitize_unit_for_filename(unit_for_filename)\n",
    "            filename = f'{category}_{unit_safe}.parquet'\n",
    "        else:\n",
    "            filename = f'{category}.parquet'\n",
    "\n",
    "        ecdf_dir = os.path.join(output_dir, 'ecdf', table_type)\n",
    "        os.makedirs(ecdf_dir, exist_ok=True)\n",
    "        ecdf_df.write_parquet(os.path.join(ecdf_dir, filename))\n",
    "\n",
    "        bins_dir = os.path.join(output_dir, 'bins', table_type)\n",
    "        os.makedirs(bins_dir, exist_ok=True)\n",
    "        bins_df.write_parquet(os.path.join(bins_dir, filename))\n",
    "\n",
    "        print(f\"  Saved to {output_dir}\")\n",
    "\n",
    "    stats = {\n",
    "        'category': category,\n",
    "        'unit': unit if table_type == 'labs' else None,\n",
    "        'original_count': original_count,\n",
    "        'clean_count': clean_count,\n",
    "        'ecdf_distinct_pairs': len(ecdf_df),\n",
    "        'num_bins': len(bins)\n",
    "    }\n",
    "\n",
    "    return stats, ecdf_df, bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Respiratory Support (Polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_respiratory_column_polars(\n",
    "    column_name: str,\n",
    "    icu_windows: pl.DataFrame,\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    outlier_range: Dict[str, float],\n",
    "    output_dir: str = None,\n",
    "    num_bins: int = 10,\n",
    "    save_output: bool = False\n",
    ") -> Tuple[Dict[str, Any], pl.DataFrame, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process a single respiratory support column using pure Polars.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(tables_path, f'clif_respiratory_support.{file_type}')\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Respiratory support file not found: {file_path}\")\n",
    "\n",
    "    print(f\"Processing {column_name}...\")\n",
    "\n",
    "    values_df = (\n",
    "        pl.scan_parquet(file_path)\n",
    "        .select(['hospitalization_id', 'recorded_dttm', column_name])\n",
    "        .join(icu_windows.lazy(), on='hospitalization_id', how='inner')\n",
    "        .filter(\n",
    "            (pl.col('recorded_dttm').dt.replace_time_zone(None) >= pl.col('in_dttm')) &\n",
    "            (pl.col('recorded_dttm').dt.replace_time_zone(None) <= pl.col('out_dttm'))\n",
    "        )\n",
    "        .select([column_name])\n",
    "        .drop_nulls()\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    original_count = len(values_df)\n",
    "    print(f\"  Original count: {original_count:,}\")\n",
    "\n",
    "    if original_count == 0:\n",
    "        print(f\"  WARNING: No data found for {column_name}\")\n",
    "        return (\n",
    "            {'column': column_name, 'original_count': 0, 'clean_count': 0},\n",
    "            pl.DataFrame({'value': [], 'probability': []}),\n",
    "            pl.DataFrame()\n",
    "        )\n",
    "\n",
    "    values_clean = values_df.filter(\n",
    "        (pl.col(column_name) >= outlier_range['min']) &\n",
    "        (pl.col(column_name) <= outlier_range['max'])\n",
    "    )\n",
    "\n",
    "    clean_count = len(values_clean)\n",
    "    print(f\"  After outlier removal: {clean_count:,} (removed {original_count - clean_count:,})\")\n",
    "\n",
    "    if clean_count == 0:\n",
    "        print(f\"  WARNING: No data remaining after outlier removal\")\n",
    "        return (\n",
    "            {'column': column_name, 'original_count': original_count, 'clean_count': 0},\n",
    "            pl.DataFrame({'value': [], 'probability': []}),\n",
    "            pl.DataFrame()\n",
    "        )\n",
    "\n",
    "    values_series = values_clean[column_name]\n",
    "\n",
    "    # Compute ECDF\n",
    "    ecdf_df = compute_ecdf_polars(values_series)\n",
    "    print(f\"  ECDF: {len(ecdf_df):,} distinct pairs (compression: {clean_count / len(ecdf_df):.1f}x)\")\n",
    "\n",
    "    # Compute Flat Bins\n",
    "    bins = create_flat_bins_polars(values_series, num_bins=num_bins)\n",
    "    bins_df = pl.DataFrame(bins) if bins else pl.DataFrame()\n",
    "    print(f\"  Bins: {len(bins_df)}\")\n",
    "\n",
    "    if save_output and output_dir:\n",
    "        ecdf_dir = os.path.join(output_dir, 'ecdf', 'respiratory_support')\n",
    "        os.makedirs(ecdf_dir, exist_ok=True)\n",
    "        ecdf_df.write_parquet(os.path.join(ecdf_dir, f'{column_name}.parquet'))\n",
    "\n",
    "        bins_dir = os.path.join(output_dir, 'bins', 'respiratory_support')\n",
    "        os.makedirs(bins_dir, exist_ok=True)\n",
    "        bins_df.write_parquet(os.path.join(bins_dir, f'{column_name}.parquet'))\n",
    "\n",
    "        print(f\"  Saved to {output_dir}\")\n",
    "\n",
    "    stats = {\n",
    "        'column': column_name,\n",
    "        'original_count': original_count,\n",
    "        'clean_count': clean_count,\n",
    "        'ecdf_distinct_pairs': len(ecdf_df),\n",
    "        'num_bins': len(bins)\n",
    "    }\n",
    "\n",
    "    return stats, ecdf_df, bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_config = lab_vital_config.get('labs', {})\n",
    "labs_outlier = outlier_config['tables']['labs']['lab_value_numeric']\n",
    "\n",
    "vitals_config = lab_vital_config.get('vitals', {})\n",
    "vitals_outlier = outlier_config['tables']['vitals']['vital_value']\n",
    "\n",
    "resp_outlier = outlier_config['tables'].get('respiratory_support', {})\n",
    "\n",
    "resp_columns = [\n",
    "    'fio2_set', 'lpm_set', 'tidal_volume_set', 'resp_rate_set',\n",
    "    'pressure_control_set', 'pressure_support_set', 'flow_rate_set',\n",
    "    'peak_inspiratory_pressure_set', 'inspiratory_time_set', 'peep_set',\n",
    "    'tidal_volume_obs', 'resp_rate_obs', 'plateau_pressure_obs',\n",
    "    'peak_inspiratory_pressure_obs', 'peep_obs', 'minute_vent_obs',\n",
    "    'mean_airway_pressure_obs'\n",
    "]\n",
    "\n",
    "print(f\"Labs: {len(labs_config)} categories\")\n",
    "print(f\"Vitals: {len(vitals_config)} categories\")\n",
    "print(f\"Respiratory: {len(resp_columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = str(PROJECT_ROOT / 'output' / 'final')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Labs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "labs_stats = []\n",
    "\n",
    "for category, cat_config in labs_config.items():\n",
    "    if cat_config is None:\n",
    "        print(f\"WARNING: Category '{category}' has None config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(cat_config, dict):\n",
    "        continue\n",
    "\n",
    "    if category not in labs_outlier:\n",
    "        print(f\"WARNING: Category '{category}' not in outlier config, skipping\")\n",
    "        continue\n",
    "\n",
    "    config_unit = cat_config.get('reference_unit')\n",
    "    if config_unit is None:\n",
    "        print(f\"WARNING: Category '{category}' has no reference_unit in config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if isinstance(config_unit, str):\n",
    "        unit = config_unit.lower()\n",
    "    elif isinstance(config_unit, list):\n",
    "        unit = [u.lower() if isinstance(u, str) else u for u in config_unit]\n",
    "    else:\n",
    "        unit = config_unit\n",
    "\n",
    "    try:\n",
    "        stats, ecdf_df, bins_df = process_category_polars(\n",
    "            table_type='labs',\n",
    "            category=category,\n",
    "            unit=unit,\n",
    "            icu_windows=icu_windows,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=labs_outlier[category],\n",
    "            cat_config=cat_config,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            extreme_bins_count=5,\n",
    "            save_output=True\n",
    "        )\n",
    "        labs_stats.append(stats)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {category}: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(labs_stats)} lab categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Vitals\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vitals_stats = []\n",
    "\n",
    "for category, cat_config in vitals_config.items():\n",
    "    if cat_config is None:\n",
    "        print(f\"WARNING: Category '{category}' has None config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(cat_config, dict):\n",
    "        continue\n",
    "\n",
    "    if category not in vitals_outlier:\n",
    "        print(f\"WARNING: Category '{category}' not in outlier config, skipping\")\n",
    "        continue\n",
    "\n",
    "    extreme_bins = 5 if category in ['height_cm', 'weight_kg'] else 10\n",
    "\n",
    "    try:\n",
    "        stats, ecdf_df, bins_df = process_category_polars(\n",
    "            table_type='vitals',\n",
    "            category=category,\n",
    "            unit=None,\n",
    "            icu_windows=icu_windows,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=vitals_outlier[category],\n",
    "            cat_config=cat_config,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            extreme_bins_count=extreme_bins,\n",
    "            save_output=True\n",
    "        )\n",
    "        vitals_stats.append(stats)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {category}: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(vitals_stats)} vital categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Respiratory Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Respiratory Support\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "resp_stats = []\n",
    "\n",
    "for column_name in resp_columns:\n",
    "    if column_name not in resp_outlier:\n",
    "        print(f\"WARNING: Column '{column_name}' not in outlier config, skipping\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        stats, ecdf_df, bins_df = process_respiratory_column_polars(\n",
    "            column_name=column_name,\n",
    "            icu_windows=icu_windows,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=resp_outlier[column_name],\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            num_bins=10,\n",
    "            save_output=True\n",
    "        )\n",
    "        resp_stats.append(stats)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {column_name}: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(resp_stats)} respiratory columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nLabs: {len(labs_stats)} categories processed\")\n",
    "print(f\"Vitals: {len(vitals_stats)} categories processed\")\n",
    "print(f\"Respiratory: {len(resp_stats)} columns processed\")\n",
    "print(f\"\\nOutput saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
