{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECDF Generator with Yearly Breakdown\n",
    "\n",
    "This notebook processes **Labs, Vitals, and Respiratory Support** using **DuckDB**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"DuckDB version: {duckdb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configs(\n",
    "    clif_config_path: str = None,\n",
    "    outlier_config_path: str = None,\n",
    "    lab_vital_config_path: str = None\n",
    ") -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Load all required configuration files.\n",
    "    \"\"\"\n",
    "    if clif_config_path is None:\n",
    "        clif_config_path = PROJECT_ROOT / 'config' / 'config.json'\n",
    "    if outlier_config_path is None:\n",
    "        outlier_config_path = PROJECT_ROOT / 'modules' / 'ecdf' / 'config' / 'outlier_config.yaml'\n",
    "    if lab_vital_config_path is None:\n",
    "        lab_vital_config_path = PROJECT_ROOT / 'modules' / 'ecdf' / 'config' / 'lab_vital_config.yaml'\n",
    "\n",
    "    with open(clif_config_path, 'r') as f:\n",
    "        clif_config = json.load(f)\n",
    "\n",
    "    with open(outlier_config_path, 'r') as f:\n",
    "        outlier_config = yaml.safe_load(f)\n",
    "\n",
    "    with open(lab_vital_config_path, 'r') as f:\n",
    "        lab_vital_config = yaml.safe_load(f)\n",
    "\n",
    "    return clif_config, outlier_config, lab_vital_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clif_config, outlier_config, lab_vital_config = load_configs()\n",
    "\n",
    "print(\"CLIF Config:\")\n",
    "print(f\"  tables_path: {clif_config.get('tables_path')}\")\n",
    "print(f\"  file_type: {clif_config.get('file_type')}\")\n",
    "print()\n",
    "print(f\"Outlier config tables: {list(outlier_config.get('tables', {}).keys())}\")\n",
    "print(f\"Lab categories in config: {list(lab_vital_config.get('labs', {}).keys())}\")\n",
    "print(f\"Vital categories in config: {list(lab_vital_config.get('vitals', {}).keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in-memory DuckDB connection\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "print(\"DuckDB connection established\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_unit_for_filename(unit) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize unit string for use in filename.\n",
    "    Handles both string and list inputs.\n",
    "    \"\"\"\n",
    "    if unit is None:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    if isinstance(unit, list):\n",
    "        unit = '_'.join(str(u) for u in unit if u is not None)\n",
    "    \n",
    "    if not unit:\n",
    "        return \"unknown\"\n",
    "\n",
    "    sanitized = unit.replace('/', '_').replace('%', 'pct').replace('Â°', 'deg')\n",
    "    sanitized = ''.join(c if c.isalnum() or c == '_' else '_' for c in sanitized)\n",
    "    while '__' in sanitized:\n",
    "        sanitized = sanitized.replace('__', '_')\n",
    "    sanitized = sanitized.strip('_').lower()\n",
    "\n",
    "    return sanitized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Standardization (using clifpy)\n",
    "\n",
    "Note: clifpy returns pandas DataFrames, so we still use pandas for this step.\n",
    "DuckDB can efficiently read the output parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clifpy\n",
    "from clifpy.tables import Labs\n",
    "print(f\"clifpy location: {clifpy.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_labs(tables_path: str, file_type: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Standardize lab reference units and save as parquet.\n",
    "    Uses pandas/clifpy for standardization, then saves for DuckDB consumption.\n",
    "    \"\"\"\n",
    "    labs_path = os.path.join(tables_path, f'clif_labs.{file_type}')\n",
    "    print(f\"Loading labs from {labs_path}...\")\n",
    "\n",
    "    labs_df = pd.read_parquet(labs_path)\n",
    "    print(f\"Loaded {len(labs_df):,} rows\")\n",
    "    \n",
    "    labs_inst = Labs(data=labs_df)\n",
    "    labs_inst.get_lab_reference_units(save=True, output_directory=\"../../output/final/ecdf_yearly/labs\")\n",
    "    \n",
    "    labs_standard = labs_inst.standardize_reference_units(\n",
    "        save=True, \n",
    "        output_directory=str(Path(output_path).parent),\n",
    "        lowercase=True, \n",
    "        inplace=False\n",
    "    )\n",
    "    \n",
    "    labs_standard.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved standardized labs to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize labs (run once)\n",
    "STANDARDIZED_LABS_PATH = '../../output/intermediate/clif_labs_standardized.parquet'\n",
    "os.makedirs(os.path.dirname(STANDARDIZED_LABS_PATH), exist_ok=True)\n",
    "\n",
    "# Run standardization\n",
    "standardize_labs(\n",
    "    clif_config['tables_path'],\n",
    "    clif_config['file_type'],\n",
    "    STANDARDIZED_LABS_PATH\n",
    ")\n",
    "\n",
    "# Also save CSV copy\n",
    "labs_standardized = pd.read_csv('../../output/intermediate/lab_reference_unit_standardized.csv') \n",
    "labs_standardized.to_csv('../../output/final/ecdf_yearly/labs/clif_labs_standardized.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ICU Windows into Memory Table\n",
    "\n",
    "Materialize ICU windows ONCE as a table, not a view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_icu_windows_duckdb(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    tables_path: str,\n",
    "    file_type: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load ICU time windows into a materialized table (not a view).\n",
    "    \"\"\"\n",
    "    adt_path = os.path.join(tables_path, f'clif_adt.{file_type}')\n",
    "    print(f\"Loading ICU time windows from {adt_path}...\")\n",
    "\n",
    "    # Create a TABLE (not view) - this materializes the data once\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE icu_windows AS\n",
    "        SELECT \n",
    "            hospitalization_id,\n",
    "            in_dttm::TIMESTAMP AS in_dttm,\n",
    "            out_dttm::TIMESTAMP AS out_dttm\n",
    "        FROM read_parquet('{adt_path}')\n",
    "        WHERE LOWER(location_category) = 'icu'\n",
    "    \"\"\")\n",
    "    \n",
    "    count = con.execute(\"SELECT COUNT(*) FROM icu_windows\").fetchone()[0]\n",
    "    print(f\"Loaded {count:,} ICU time windows into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_icu_windows_duckdb(\n",
    "    con,\n",
    "    clif_config['tables_path'],\n",
    "    clif_config['file_type']\n",
    ")\n",
    "\n",
    "print(\"\\nSample ICU time windows:\")\n",
    "con.execute(\"SELECT * FROM icu_windows LIMIT 10\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECDF Computation with Yearly Breakdown (DuckDB)\n",
    "\n",
    "Computes:\n",
    "- Overall ECDF across all years\n",
    "- Per-year ECDF for each year in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_yearly_ecdf_duckdb(\n",
    "    con: duckdb.DuckDBPyConnection, \n",
    "    table_name: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute ECDF with overall and per-year probabilities.\n",
    "    \n",
    "    The table must have columns: value, year\n",
    "    \n",
    "    Returns DataFrame with columns:\n",
    "        value, overall_probability, 2018_probability, 2019_probability, ...\n",
    "    \"\"\"\n",
    "    # Get distinct years\n",
    "    years = con.execute(f\"SELECT DISTINCT year FROM {table_name} ORDER BY year\").fetchall()\n",
    "    years = [int(y[0]) for y in years]\n",
    "    print(f\"  Years found: {years}\")\n",
    "    \n",
    "    # Compute overall ECDF\n",
    "    overall_ecdf = con.execute(f\"\"\"\n",
    "        WITH ranked AS (\n",
    "            SELECT \n",
    "                value,\n",
    "                CUME_DIST() OVER (ORDER BY value) as probability\n",
    "            FROM {table_name}\n",
    "        )\n",
    "        SELECT \n",
    "            value,\n",
    "            MAX(probability) as overall_probability\n",
    "        FROM ranked\n",
    "        GROUP BY value\n",
    "        ORDER BY value\n",
    "    \"\"\").df()\n",
    "    \n",
    "    # Compute per-year ECDFs and join\n",
    "    result_df = overall_ecdf.copy()\n",
    "    \n",
    "    for year in years:\n",
    "        year_ecdf = con.execute(f\"\"\"\n",
    "            WITH year_data AS (\n",
    "                SELECT value FROM {table_name} WHERE year = {year}\n",
    "            ),\n",
    "            ranked AS (\n",
    "                SELECT \n",
    "                    value,\n",
    "                    CUME_DIST() OVER (ORDER BY value) as probability\n",
    "                FROM year_data\n",
    "            )\n",
    "            SELECT \n",
    "                value,\n",
    "                MAX(probability) as probability\n",
    "            FROM ranked\n",
    "            GROUP BY value\n",
    "            ORDER BY value\n",
    "        \"\"\").df()\n",
    "        \n",
    "        year_ecdf = year_ecdf.rename(columns={'probability': f'{year}_probability'})\n",
    "        \n",
    "        # Merge with result - use left join to keep all values from overall\n",
    "        result_df = result_df.merge(year_ecdf, on='value', how='left')\n",
    "    \n",
    "    # Sort by value\n",
    "    result_df = result_df.sort_values('value').reset_index(drop=True)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quantile_edges(con: duckdb.DuckDBPyConnection, table_name: str, num_bins: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Compute quantile bin edges in a single query.\n",
    "    \"\"\"\n",
    "    quantiles = [i / num_bins for i in range(num_bins + 1)]\n",
    "    quantile_str = ', '.join([f\"QUANTILE_CONT(value, {q})\" for q in quantiles])\n",
    "    \n",
    "    result = con.execute(f\"SELECT {quantile_str} FROM {table_name}\").fetchone()\n",
    "    edges = list(result)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_edges = []\n",
    "    for edge in edges:\n",
    "        if not unique_edges or abs(edge - unique_edges[-1]) > 1e-10:\n",
    "            unique_edges.append(edge)\n",
    "    \n",
    "    return unique_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_bins_single_query(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    table_name: str,\n",
    "    bin_edges: List[float]\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Count all bins in a SINGLE query using CASE WHEN.\n",
    "    This is the key optimization - no more N separate queries!\n",
    "    \"\"\"\n",
    "    if len(bin_edges) < 2:\n",
    "        count = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "        return [count]\n",
    "    \n",
    "    # Build CASE WHEN for each bin\n",
    "    case_parts = []\n",
    "    for i in range(len(bin_edges) - 1):\n",
    "        lower = bin_edges[i]\n",
    "        upper = bin_edges[i + 1]\n",
    "        \n",
    "        if i == 0:\n",
    "            # First bin: inclusive on both ends\n",
    "            case_parts.append(f\"SUM(CASE WHEN value >= {lower} AND value <= {upper} THEN 1 ELSE 0 END) as bin_{i}\")\n",
    "        else:\n",
    "            # Other bins: exclusive lower, inclusive upper\n",
    "            case_parts.append(f\"SUM(CASE WHEN value > {lower} AND value <= {upper} THEN 1 ELSE 0 END) as bin_{i}\")\n",
    "    \n",
    "    query = f\"SELECT {', '.join(case_parts)} FROM {table_name}\"\n",
    "    result = con.execute(query).fetchone()\n",
    "    \n",
    "    return [int(c) for c in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flat_bins_duckdb(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    table_name: str,\n",
    "    num_bins: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create flat quantile bins using DuckDB - optimized version.\n",
    "    \"\"\"\n",
    "    count = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "    \n",
    "    if count == 0:\n",
    "        return []\n",
    "    \n",
    "    if count < num_bins * 2:\n",
    "        num_bins = max(1, count // 2)\n",
    "    \n",
    "    # Get bin edges\n",
    "    bin_edges = compute_quantile_edges(con, table_name, num_bins)\n",
    "    \n",
    "    if len(bin_edges) < 2:\n",
    "        min_val, max_val = con.execute(f\"SELECT MIN(value), MAX(value) FROM {table_name}\").fetchone()\n",
    "        return [{\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(min_val),\n",
    "            'bin_max': float(max_val),\n",
    "            'count': count,\n",
    "            'percentage': 100.0,\n",
    "            'interval': f\"[{min_val:.2f}, {max_val:.2f}]\"\n",
    "        }]\n",
    "    \n",
    "    # Count all bins in ONE query\n",
    "    bin_counts = count_all_bins_single_query(con, table_name, bin_edges)\n",
    "    \n",
    "    bins = []\n",
    "    for i in range(len(bin_edges) - 1):\n",
    "        lower = bin_edges[i]\n",
    "        upper = bin_edges[i + 1]\n",
    "        bin_count = bin_counts[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            interval_str = f\"[{lower:.2f}, {upper:.2f}]\"\n",
    "        else:\n",
    "            interval_str = f\"({lower:.2f}, {upper:.2f}]\"\n",
    "        \n",
    "        bins.append({\n",
    "            'bin_num': i + 1,\n",
    "            'bin_min': float(lower),\n",
    "            'bin_max': float(upper),\n",
    "            'count': bin_count,\n",
    "            'percentage': float(bin_count / count * 100),\n",
    "            'interval': interval_str\n",
    "        })\n",
    "    \n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins_for_segment_duckdb(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    table_name: str,\n",
    "    segment_min: float,\n",
    "    segment_max: float,\n",
    "    num_bins: int,\n",
    "    segment_name: str,\n",
    "    extra_bins_last: int = 0,\n",
    "    split_first: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create quantile-based bins for a segment - optimized version.\n",
    "    \"\"\"\n",
    "    # Create temp table for segment (materialized, not view)\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP TABLE segment_data AS\n",
    "        SELECT value FROM {table_name}\n",
    "        WHERE value >= {segment_min} AND value <= {segment_max}\n",
    "    \"\"\")\n",
    "    \n",
    "    segment_count = con.execute(\"SELECT COUNT(*) FROM segment_data\").fetchone()[0]\n",
    "    \n",
    "    if segment_count == 0:\n",
    "        return []\n",
    "    \n",
    "    if num_bins == 1 or segment_count < num_bins * 2:\n",
    "        min_val, max_val = con.execute(\"SELECT MIN(value), MAX(value) FROM segment_data\").fetchone()\n",
    "        return [{\n",
    "            'segment': segment_name,\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(min_val),\n",
    "            'bin_max': float(max_val),\n",
    "            'count': segment_count,\n",
    "            'percentage': 100.0\n",
    "        }]\n",
    "    \n",
    "    # Get bin edges\n",
    "    bin_edges = compute_quantile_edges(con, 'segment_data', num_bins)\n",
    "    \n",
    "    if len(bin_edges) < 2:\n",
    "        min_val, max_val = con.execute(\"SELECT MIN(value), MAX(value) FROM segment_data\").fetchone()\n",
    "        return [{\n",
    "            'segment': segment_name,\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(min_val),\n",
    "            'bin_max': float(max_val),\n",
    "            'count': segment_count,\n",
    "            'percentage': 100.0\n",
    "        }]\n",
    "    \n",
    "    # Count all bins in ONE query\n",
    "    bin_counts = count_all_bins_single_query(con, 'segment_data', bin_edges)\n",
    "    \n",
    "    bins = []\n",
    "    for i in range(len(bin_edges) - 1):\n",
    "        bins.append({\n",
    "            'segment': segment_name,\n",
    "            'bin_num': i + 1,\n",
    "            'bin_min': float(bin_edges[i]),\n",
    "            'bin_max': float(bin_edges[i + 1]),\n",
    "            'count': bin_counts[i],\n",
    "            'percentage': float(bin_counts[i] / segment_count * 100)\n",
    "        })\n",
    "    \n",
    "    # Handle extra bins for extreme values\n",
    "    if extra_bins_last > 0 and len(bins) > 1:\n",
    "        if split_first:\n",
    "            # Split the first bin\n",
    "            extreme_bin = bins[0]\n",
    "            con.execute(f\"\"\"\n",
    "                CREATE OR REPLACE TEMP TABLE extreme_data AS\n",
    "                SELECT value FROM segment_data\n",
    "                WHERE value >= {extreme_bin['bin_min']} AND value <= {extreme_bin['bin_max']}\n",
    "            \"\"\")\n",
    "            \n",
    "            extreme_count = con.execute(\"SELECT COUNT(*) FROM extreme_data\").fetchone()[0]\n",
    "            \n",
    "            if extreme_count >= extra_bins_last * 2:\n",
    "                tail_edges = compute_quantile_edges(con, 'extreme_data', extra_bins_last)\n",
    "                \n",
    "                if len(tail_edges) > 1:\n",
    "                    tail_counts = count_all_bins_single_query(con, 'extreme_data', tail_edges)\n",
    "                    \n",
    "                    new_bins = []\n",
    "                    for j in range(len(tail_edges) - 1):\n",
    "                        new_bins.append({\n",
    "                            'segment': segment_name,\n",
    "                            'bin_num': j + 1,\n",
    "                            'bin_min': float(tail_edges[j]),\n",
    "                            'bin_max': float(tail_edges[j + 1]),\n",
    "                            'count': tail_counts[j],\n",
    "                            'percentage': float(tail_counts[j] / segment_count * 100)\n",
    "                        })\n",
    "                    \n",
    "                    for bin_info in bins[1:]:\n",
    "                        bin_info['bin_num'] = len(new_bins) + 1\n",
    "                        new_bins.append(bin_info)\n",
    "                    \n",
    "                    bins = new_bins\n",
    "        else:\n",
    "            # Split the last bin\n",
    "            extreme_bin = bins[-1]\n",
    "            con.execute(f\"\"\"\n",
    "                CREATE OR REPLACE TEMP TABLE extreme_data AS\n",
    "                SELECT value FROM segment_data\n",
    "                WHERE value > {extreme_bin['bin_min']} AND value <= {extreme_bin['bin_max']}\n",
    "            \"\"\")\n",
    "            \n",
    "            extreme_count = con.execute(\"SELECT COUNT(*) FROM extreme_data\").fetchone()[0]\n",
    "            \n",
    "            if extreme_count >= extra_bins_last * 2:\n",
    "                tail_edges = compute_quantile_edges(con, 'extreme_data', extra_bins_last)\n",
    "                \n",
    "                if len(tail_edges) > 1:\n",
    "                    tail_counts = count_all_bins_single_query(con, 'extreme_data', tail_edges)\n",
    "                    \n",
    "                    bins = bins[:-1]\n",
    "                    for j in range(len(tail_edges) - 1):\n",
    "                        bins.append({\n",
    "                            'segment': segment_name,\n",
    "                            'bin_num': len(bins) + 1,\n",
    "                            'bin_min': float(tail_edges[j]),\n",
    "                            'bin_max': float(tail_edges[j + 1]),\n",
    "                            'count': tail_counts[j],\n",
    "                            'percentage': float(tail_counts[j] / segment_count * 100)\n",
    "                        })\n",
    "    \n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_bins_duckdb(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    table_name: str,\n",
    "    normal_lower: float,\n",
    "    normal_upper: float,\n",
    "    outlier_min: float,\n",
    "    outlier_max: float,\n",
    "    bins_below: int,\n",
    "    bins_normal: int,\n",
    "    bins_above: int,\n",
    "    extra_bins_below: int = 0,\n",
    "    extra_bins_above: int = 0\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create bins for all segments (below/normal/above).\n",
    "    \"\"\"\n",
    "    all_bins = []\n",
    "\n",
    "    if bins_below > 0 and outlier_min < normal_lower:\n",
    "        below_bins = create_bins_for_segment_duckdb(\n",
    "            con, table_name,\n",
    "            outlier_min, normal_lower, bins_below, 'below',\n",
    "            extra_bins_last=extra_bins_below,\n",
    "            split_first=True\n",
    "        )\n",
    "        all_bins.extend(below_bins)\n",
    "\n",
    "    if bins_normal > 0:\n",
    "        normal_bins = create_bins_for_segment_duckdb(\n",
    "            con, table_name,\n",
    "            normal_lower, normal_upper, bins_normal, 'normal'\n",
    "        )\n",
    "        all_bins.extend(normal_bins)\n",
    "\n",
    "    if bins_above > 0 and normal_upper < outlier_max:\n",
    "        above_bins = create_bins_for_segment_duckdb(\n",
    "            con, table_name,\n",
    "            normal_upper, outlier_max, bins_above, 'above',\n",
    "            extra_bins_last=extra_bins_above,\n",
    "            split_first=False\n",
    "        )\n",
    "        all_bins.extend(above_bins)\n",
    "\n",
    "    return all_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Labs/Vitals with Yearly ECDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_duckdb(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    table_type: str,\n",
    "    category: str,\n",
    "    unit: Optional[str],\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    outlier_range: Dict[str, float],\n",
    "    cat_config: Dict[str, Any],\n",
    "    output_dir: str = None,\n",
    "    extreme_bins_count: int = 5,\n",
    "    save_output: bool = False\n",
    ") -> Tuple[Dict[str, Any], Any, Any]:\n",
    "    \"\"\"\n",
    "    Process a single lab/vital category using DuckDB.\n",
    "    KEY: Materializes data into a temp table ONCE, then all ops run on that.\n",
    "    Now includes year column for yearly ECDF computation.\n",
    "    \"\"\"\n",
    "    if table_type == 'labs':\n",
    "        file_path = '../../output/intermediate/clif_labs_standardized.parquet'\n",
    "        category_col = 'lab_category'\n",
    "        value_col = 'lab_value_numeric'\n",
    "        datetime_col = 'lab_result_dttm'\n",
    "    else:\n",
    "        file_path = os.path.join(tables_path, f'clif_vitals.{file_type}')\n",
    "        category_col = 'vital_category'\n",
    "        value_col = 'vital_value'\n",
    "        datetime_col = 'recorded_dttm'\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "\n",
    "    display_name = f\"{category} ({unit})\" if table_type == 'labs' and unit else category\n",
    "    print(f\"Processing {display_name}...\")\n",
    "\n",
    "    # Build unit filter for SQL\n",
    "    unit_filter = \"\"\n",
    "    if table_type == 'labs' and unit:\n",
    "        if isinstance(unit, list):\n",
    "            unit_values = [f\"'{u}'\" for u in unit if u != '(no units)']\n",
    "            if unit_values:\n",
    "                unit_filter = f\"AND (reference_unit IN ({', '.join(unit_values)})\"\n",
    "                if '(no units)' in unit:\n",
    "                    unit_filter += \" OR reference_unit IS NULL)\"\n",
    "                else:\n",
    "                    unit_filter += \")\"\n",
    "            elif '(no units)' in unit:\n",
    "                unit_filter = \"AND reference_unit IS NULL\"\n",
    "        else:\n",
    "            if unit == '(no units)':\n",
    "                unit_filter = \"AND reference_unit IS NULL\"\n",
    "            else:\n",
    "                unit_filter = f\"AND reference_unit = '{unit}'\"\n",
    "\n",
    "    # CRITICAL: Materialize into a TEMP TABLE, not a view!\n",
    "    # Include year column for yearly ECDF\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP TABLE clean_data AS\n",
    "        SELECT \n",
    "            d.{value_col} as value,\n",
    "            EXTRACT(YEAR FROM d.{datetime_col}::TIMESTAMP)::INTEGER as year\n",
    "        FROM read_parquet('{file_path}') d\n",
    "        INNER JOIN icu_windows w ON d.hospitalization_id = w.hospitalization_id\n",
    "        WHERE d.{category_col} = '{category}'\n",
    "            {unit_filter}\n",
    "            AND d.{datetime_col}::TIMESTAMP >= w.in_dttm\n",
    "            AND d.{datetime_col}::TIMESTAMP <= w.out_dttm\n",
    "            AND d.{value_col} >= {outlier_range['min']}\n",
    "            AND d.{value_col} <= {outlier_range['max']}\n",
    "    \"\"\")\n",
    "\n",
    "    clean_count = con.execute(\"SELECT COUNT(*) FROM clean_data\").fetchone()[0]\n",
    "    print(f\"  Clean count: {clean_count:,}\")\n",
    "\n",
    "    if clean_count == 0:\n",
    "        print(f\"  WARNING: No data found for {display_name}\")\n",
    "        return (\n",
    "            {'category': category, 'unit': unit, 'original_count': 0, 'clean_count': 0},\n",
    "            None,\n",
    "            None\n",
    "        )\n",
    "\n",
    "    # Compute Yearly ECDF\n",
    "    ecdf_df = compute_yearly_ecdf_duckdb(con, 'clean_data')\n",
    "    print(f\"  ECDF: {len(ecdf_df):,} distinct values (compression: {clean_count / len(ecdf_df):.1f}x)\")\n",
    "\n",
    "    # Compute Bins\n",
    "    bins_config = cat_config.get('bins', {})\n",
    "    bins_below = bins_config.get('below_normal', 0) or 0\n",
    "    bins_normal = bins_config.get('normal', 0) or 0\n",
    "    bins_above = bins_config.get('above_normal', 0) or 0\n",
    "\n",
    "    extra_bins_below = extreme_bins_count if bins_below > 1 else 0\n",
    "    extra_bins_above = extreme_bins_count if bins_above > 1 else 0\n",
    "\n",
    "    normal_range = cat_config.get('normal_range', {})\n",
    "    normal_lower = normal_range.get('lower', outlier_range['min'])\n",
    "    normal_upper = normal_range.get('upper', outlier_range['max'])\n",
    "\n",
    "    bins = create_all_bins_duckdb(\n",
    "        con=con,\n",
    "        table_name='clean_data',\n",
    "        normal_lower=normal_lower,\n",
    "        normal_upper=normal_upper,\n",
    "        outlier_min=outlier_range['min'],\n",
    "        outlier_max=outlier_range['max'],\n",
    "        bins_below=bins_below,\n",
    "        bins_normal=bins_normal,\n",
    "        bins_above=bins_above,\n",
    "        extra_bins_below=extra_bins_below,\n",
    "        extra_bins_above=extra_bins_above\n",
    "    )\n",
    "\n",
    "    for bin_info in bins:\n",
    "        if bin_info['bin_num'] == 1:\n",
    "            interval = f\"[{bin_info['bin_min']:.2f}, {bin_info['bin_max']:.2f}]\"\n",
    "        else:\n",
    "            interval = f\"({bin_info['bin_min']:.2f}, {bin_info['bin_max']:.2f}]\"\n",
    "        bin_info['interval'] = interval\n",
    "\n",
    "    bins_df = pd.DataFrame(bins) if bins else pd.DataFrame()\n",
    "    print(f\"  Bins: {len(bins_df)}\")\n",
    "\n",
    "    if save_output and output_dir:\n",
    "        if table_type == 'labs' and unit:\n",
    "            unit_for_filename = unit[0] if isinstance(unit, list) else unit\n",
    "            unit_safe = sanitize_unit_for_filename(unit_for_filename)\n",
    "            filename = f'{category}_{unit_safe}.parquet'\n",
    "        else:\n",
    "            filename = f'{category}.parquet'\n",
    "\n",
    "        ecdf_dir = os.path.join(output_dir, 'ecdf_yearly', table_type)\n",
    "        os.makedirs(ecdf_dir, exist_ok=True)\n",
    "        ecdf_df.to_parquet(os.path.join(ecdf_dir, filename), index=False)\n",
    "\n",
    "        bins_dir = os.path.join(output_dir, 'bins', table_type)\n",
    "        os.makedirs(bins_dir, exist_ok=True)\n",
    "        bins_df.to_parquet(os.path.join(bins_dir, filename), index=False)\n",
    "\n",
    "        print(f\"  Saved to {output_dir}\")\n",
    "\n",
    "    # Cleanup temp table\n",
    "    con.execute(\"DROP TABLE IF EXISTS clean_data\")\n",
    "\n",
    "    stats = {\n",
    "        'category': category,\n",
    "        'unit': unit if table_type == 'labs' else None,\n",
    "        'clean_count': clean_count,\n",
    "        'ecdf_distinct_values': len(ecdf_df),\n",
    "        'num_bins': len(bins)\n",
    "    }\n",
    "\n",
    "    return stats, ecdf_df, bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Respiratory Support with Yearly ECDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_respiratory_column_duckdb(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    column_name: str,\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    outlier_range: Dict[str, float],\n",
    "    output_dir: str = None,\n",
    "    num_bins: int = 10,\n",
    "    save_output: bool = False\n",
    ") -> Tuple[Dict[str, Any], Any, Any]:\n",
    "    \"\"\"\n",
    "    Process a single respiratory support column using DuckDB.\n",
    "    Now includes year column for yearly ECDF computation.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(tables_path, f'clif_respiratory_support.{file_type}')\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Respiratory support file not found: {file_path}\")\n",
    "\n",
    "    print(f\"Processing {column_name}...\")\n",
    "\n",
    "    # CRITICAL: Materialize into a TEMP TABLE with year column\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP TABLE clean_resp AS\n",
    "        SELECT \n",
    "            d.{column_name} as value,\n",
    "            EXTRACT(YEAR FROM d.recorded_dttm::TIMESTAMP)::INTEGER as year\n",
    "        FROM read_parquet('{file_path}') d\n",
    "        INNER JOIN icu_windows w ON d.hospitalization_id = w.hospitalization_id\n",
    "        WHERE d.recorded_dttm::TIMESTAMP >= w.in_dttm\n",
    "            AND d.recorded_dttm::TIMESTAMP <= w.out_dttm\n",
    "            AND d.{column_name} IS NOT NULL\n",
    "            AND d.{column_name} >= {outlier_range['min']}\n",
    "            AND d.{column_name} <= {outlier_range['max']}\n",
    "    \"\"\")\n",
    "\n",
    "    clean_count = con.execute(\"SELECT COUNT(*) FROM clean_resp\").fetchone()[0]\n",
    "    print(f\"  Clean count: {clean_count:,}\")\n",
    "\n",
    "    if clean_count == 0:\n",
    "        print(f\"  WARNING: No data found for {column_name}\")\n",
    "        return (\n",
    "            {'column': column_name, 'clean_count': 0},\n",
    "            None,\n",
    "            None\n",
    "        )\n",
    "\n",
    "    # Compute Yearly ECDF\n",
    "    ecdf_df = compute_yearly_ecdf_duckdb(con, 'clean_resp')\n",
    "    print(f\"  ECDF: {len(ecdf_df):,} distinct values (compression: {clean_count / len(ecdf_df):.1f}x)\")\n",
    "\n",
    "    # Compute Flat Bins\n",
    "    bins = create_flat_bins_duckdb(con, 'clean_resp', num_bins=num_bins)\n",
    "    bins_df = pd.DataFrame(bins) if bins else pd.DataFrame()\n",
    "    print(f\"  Bins: {len(bins_df)}\")\n",
    "\n",
    "    if save_output and output_dir:\n",
    "        ecdf_dir = os.path.join(output_dir, 'ecdf_yearly', 'respiratory_support')\n",
    "        os.makedirs(ecdf_dir, exist_ok=True)\n",
    "        ecdf_df.to_parquet(os.path.join(ecdf_dir, f'{column_name}.parquet'), index=False)\n",
    "\n",
    "        bins_dir = os.path.join(output_dir, 'bins', 'respiratory_support')\n",
    "        os.makedirs(bins_dir, exist_ok=True)\n",
    "        bins_df.to_parquet(os.path.join(bins_dir, f'{column_name}.parquet'), index=False)\n",
    "\n",
    "        print(f\"  Saved to {output_dir}\")\n",
    "\n",
    "    # Cleanup temp table\n",
    "    con.execute(\"DROP TABLE IF EXISTS clean_resp\")\n",
    "\n",
    "    stats = {\n",
    "        'column': column_name,\n",
    "        'clean_count': clean_count,\n",
    "        'ecdf_distinct_values': len(ecdf_df),\n",
    "        'num_bins': len(bins)\n",
    "    }\n",
    "\n",
    "    return stats, ecdf_df, bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_config = lab_vital_config.get('labs', {})\n",
    "labs_outlier = outlier_config['tables']['labs']['lab_value_numeric']\n",
    "\n",
    "vitals_config = lab_vital_config.get('vitals', {})\n",
    "vitals_outlier = outlier_config['tables']['vitals']['vital_value']\n",
    "\n",
    "resp_outlier = outlier_config['tables'].get('respiratory_support', {})\n",
    "\n",
    "resp_columns = [\n",
    "    'fio2_set', 'lpm_set', 'tidal_volume_set', 'resp_rate_set',\n",
    "    'pressure_control_set', 'pressure_support_set', 'flow_rate_set',\n",
    "    'peak_inspiratory_pressure_set', 'inspiratory_time_set', 'peep_set',\n",
    "    'tidal_volume_obs', 'resp_rate_obs', 'plateau_pressure_obs',\n",
    "    'peak_inspiratory_pressure_obs', 'peep_obs', 'minute_vent_obs',\n",
    "    'mean_airway_pressure_obs'\n",
    "]\n",
    "\n",
    "print(f\"Labs: {len(labs_config)} categories\")\n",
    "print(f\"Vitals: {len(vitals_config)} categories\")\n",
    "print(f\"Respiratory: {len(resp_columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = str(PROJECT_ROOT / 'output' / 'final')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Labs - Yearly ECDF\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "labs_stats = []\n",
    "\n",
    "for category, cat_config in labs_config.items():\n",
    "    if cat_config is None:\n",
    "        print(f\"WARNING: Category '{category}' has None config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(cat_config, dict):\n",
    "        continue\n",
    "\n",
    "    if category not in labs_outlier:\n",
    "        print(f\"WARNING: Category '{category}' not in outlier config, skipping\")\n",
    "        continue\n",
    "\n",
    "    config_unit = cat_config.get('reference_unit')\n",
    "    if config_unit is None:\n",
    "        print(f\"WARNING: Category '{category}' has no reference_unit in config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if isinstance(config_unit, str):\n",
    "        unit = config_unit.lower()\n",
    "    elif isinstance(config_unit, list):\n",
    "        unit = [u.lower() if isinstance(u, str) else u for u in config_unit]\n",
    "    else:\n",
    "        unit = config_unit\n",
    "\n",
    "    try:\n",
    "        stats, ecdf_df, bins_df = process_category_duckdb(\n",
    "            con=con,\n",
    "            table_type='labs',\n",
    "            category=category,\n",
    "            unit=unit,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=labs_outlier[category],\n",
    "            cat_config=cat_config,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            extreme_bins_count=5,\n",
    "            save_output=True\n",
    "        )\n",
    "        labs_stats.append(stats)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {category}: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(labs_stats)} lab categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Vitals - Yearly ECDF\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vitals_stats = []\n",
    "\n",
    "for category, cat_config in vitals_config.items():\n",
    "    if cat_config is None:\n",
    "        print(f\"WARNING: Category '{category}' has None config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(cat_config, dict):\n",
    "        continue\n",
    "\n",
    "    if category not in vitals_outlier:\n",
    "        print(f\"WARNING: Category '{category}' not in outlier config, skipping\")\n",
    "        continue\n",
    "\n",
    "    extreme_bins = 5 if category in ['height_cm', 'weight_kg'] else 10\n",
    "\n",
    "    try:\n",
    "        stats, ecdf_df, bins_df = process_category_duckdb(\n",
    "            con=con,\n",
    "            table_type='vitals',\n",
    "            category=category,\n",
    "            unit=None,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=vitals_outlier[category],\n",
    "            cat_config=cat_config,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            extreme_bins_count=extreme_bins,\n",
    "            save_output=True\n",
    "        )\n",
    "        vitals_stats.append(stats)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {category}: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(vitals_stats)} vital categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Respiratory Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Respiratory Support - Yearly ECDF\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "resp_stats = []\n",
    "\n",
    "for column_name in resp_columns:\n",
    "    if column_name not in resp_outlier:\n",
    "        print(f\"WARNING: Column '{column_name}' not in outlier config, skipping\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        stats, ecdf_df, bins_df = process_respiratory_column_duckdb(\n",
    "            con=con,\n",
    "            column_name=column_name,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=resp_outlier[column_name],\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            num_bins=10,\n",
    "            save_output=True\n",
    "        )\n",
    "        resp_stats.append(stats)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {column_name}: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(resp_stats)} respiratory columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Processing Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nLabs: {len(labs_stats)} categories processed\")\n",
    "print(f\"Vitals: {len(vitals_stats)} categories processed\")\n",
    "print(f\"Respiratory: {len(resp_stats)} columns processed\")\n",
    "print(f\"\\nOutput saved to: {OUTPUT_DIR}\")\n",
    "print(f\"  - ECDF (yearly): {OUTPUT_DIR}/\")\n",
    "print(f\"  - Bins: {OUTPUT_DIR}/bins/\")\n",
    "print(f\"\\nElapsed: {elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example output for one lab\n",
    "example_file = os.path.join(OUTPUT_DIR, 'labs', 'hemoglobin_g_dl.parquet')\n",
    "if os.path.exists(example_file):\n",
    "    example_df = pd.read_parquet(example_file)\n",
    "    print(\"Example output (hemoglobin):\")\n",
    "    print(f\"Columns: {list(example_df.columns)}\")\n",
    "    print()\n",
    "    print(example_df.head(20))\n",
    "else:\n",
    "    print(\"Example file not found - run the pipeline first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the DuckDB connection when done\n",
    "con.close()\n",
    "print(\"DuckDB connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
