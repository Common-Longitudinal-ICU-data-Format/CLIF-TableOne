{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECDF Generator - Combined (Pandas Only)\n",
    "\n",
    "This notebook processes **Labs, Vitals, and Respiratory Support** using **Pandas exclusively**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configs(\n",
    "    clif_config_path: str = None,\n",
    "    outlier_config_path: str = None,\n",
    "    lab_vital_config_path: str = None\n",
    ") -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Load all required configuration files.\n",
    "    \"\"\"\n",
    "    if clif_config_path is None:\n",
    "        clif_config_path = PROJECT_ROOT / 'config' / 'config.json'\n",
    "    if outlier_config_path is None:\n",
    "        outlier_config_path = PROJECT_ROOT / 'modules' / 'ecdf' / 'config' / 'outlier_config.yaml'\n",
    "    if lab_vital_config_path is None:\n",
    "        lab_vital_config_path = PROJECT_ROOT / 'modules' / 'ecdf' / 'config' / 'lab_vital_config.yaml'\n",
    "\n",
    "    with open(clif_config_path, 'r') as f:\n",
    "        clif_config = json.load(f)\n",
    "\n",
    "    with open(outlier_config_path, 'r') as f:\n",
    "        outlier_config = yaml.safe_load(f)\n",
    "\n",
    "    with open(lab_vital_config_path, 'r') as f:\n",
    "        lab_vital_config = yaml.safe_load(f)\n",
    "\n",
    "    return clif_config, outlier_config, lab_vital_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clif_config, outlier_config, lab_vital_config = load_configs()\n",
    "\n",
    "print(\"CLIF Config:\")\n",
    "print(f\"  tables_path: {clif_config.get('tables_path')}\")\n",
    "print(f\"  file_type: {clif_config.get('file_type')}\")\n",
    "print()\n",
    "print(f\"Outlier config tables: {list(outlier_config.get('tables', {}).keys())}\")\n",
    "print(f\"Lab categories in config: {list(lab_vital_config.get('labs', {}).keys())}\")\n",
    "print(f\"Vital categories in config: {list(lab_vital_config.get('vitals', {}).keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_unit_for_filename(unit) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize unit string for use in filename.\n",
    "    Handles both string and list inputs.\n",
    "    \"\"\"\n",
    "    if unit is None:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    if isinstance(unit, list):\n",
    "        unit = '_'.join(str(u) for u in unit if u is not None)\n",
    "    \n",
    "    if not unit:\n",
    "        return \"unknown\"\n",
    "\n",
    "    sanitized = unit.replace('/', '_').replace('%', 'pct').replace('Â°', 'deg')\n",
    "    sanitized = ''.join(c if c.isalnum() or c == '_' else '_' for c in sanitized)\n",
    "    while '__' in sanitized:\n",
    "        sanitized = sanitized.replace('__', '_')\n",
    "    sanitized = sanitized.strip('_').lower()\n",
    "\n",
    "    return sanitized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Standardization (using clifpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clifpy\n",
    "from clifpy.tables import Labs\n",
    "print(f\"clifpy location: {clifpy.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_labs_pandas(\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    output_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Standardize lab reference units and save as parquet.\n",
    "    \"\"\"\n",
    "    labs_path = os.path.join(tables_path, f'clif_labs.{file_type}')\n",
    "    print(f\"Loading labs from {labs_path}...\")\n",
    "\n",
    "    labs_df = pd.read_parquet(labs_path)\n",
    "    print(f\"Loaded {len(labs_df):,} rows\")\n",
    "    \n",
    "    labs_inst = Labs(data=labs_df)\n",
    "    \n",
    "    labs_standard = labs_inst.standardize_reference_units(\n",
    "        save=True, \n",
    "        output_directory=str(Path(output_path).parent),\n",
    "        lowercase=True, \n",
    "        inplace=False\n",
    "    )\n",
    "    \n",
    "    labs_standard.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved standardized labs to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize labs (run once)\n",
    "STANDARDIZED_LABS_PATH = '../../output/intermediate/clif_labs_standardized.parquet'\n",
    "os.makedirs(os.path.dirname(STANDARDIZED_LABS_PATH), exist_ok=True)\n",
    "\n",
    "## Uncomment to run standardization\n",
    "standardize_labs_pandas(\n",
    "    clif_config['tables_path'],\n",
    "    clif_config['file_type'],\n",
    "    STANDARDIZED_LABS_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICU Time Window Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_icu_time_windows(\n",
    "    tables_path: str,\n",
    "    file_type: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract ICU time windows from ADT table using Pandas.\n",
    "    \"\"\"\n",
    "    adt_path = os.path.join(tables_path, f'clif_adt.{file_type}')\n",
    "    print(f\"Loading ICU time windows from {adt_path}...\")\n",
    "\n",
    "    adt_df = pd.read_parquet(adt_path, columns=['hospitalization_id', 'location_category', 'in_dttm', 'out_dttm'])\n",
    "    \n",
    "    icu_windows_df = adt_df[adt_df['location_category'].str.lower() == 'icu'][['hospitalization_id', 'in_dttm', 'out_dttm']].copy()\n",
    "    \n",
    "    if icu_windows_df['in_dttm'].dt.tz is not None:\n",
    "        icu_windows_df['in_dttm'] = icu_windows_df['in_dttm'].dt.tz_localize(None)\n",
    "    if icu_windows_df['out_dttm'].dt.tz is not None:\n",
    "        icu_windows_df['out_dttm'] = icu_windows_df['out_dttm'].dt.tz_localize(None)\n",
    "\n",
    "    print(f\"Found {len(icu_windows_df):,} ICU time windows\")\n",
    "    return icu_windows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_windows = extract_icu_time_windows(\n",
    "    clif_config['tables_path'],\n",
    "    clif_config['file_type']\n",
    ")\n",
    "\n",
    "print(\"\\nSample ICU time windows:\")\n",
    "icu_windows.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECDF Computation (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ecdf_pandas(values: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute ECDF using pure Pandas operations.\n",
    "    \"\"\"\n",
    "    if len(values) == 0:\n",
    "        return pd.DataFrame({'value': [], 'probability': []})\n",
    "\n",
    "    n = len(values)\n",
    "    \n",
    "    sorted_values = values.sort_values().reset_index(drop=True)\n",
    "    probabilities = (np.arange(1, n + 1)) / n\n",
    "    \n",
    "    ecdf_df = pd.DataFrame({\n",
    "        'value': sorted_values,\n",
    "        'probability': probabilities\n",
    "    })\n",
    "    \n",
    "    ecdf_df = ecdf_df.groupby('value', as_index=False)['probability'].max()\n",
    "    ecdf_df = ecdf_df.sort_values('value').reset_index(drop=True)\n",
    "\n",
    "    return ecdf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Functions (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flat_bins_pandas(\n",
    "    data: pd.Series,\n",
    "    num_bins: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create flat quantile bins using Pandas qcut.\n",
    "    No segmentation - used for respiratory support columns.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return []\n",
    "\n",
    "    data = data.dropna()\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        return []\n",
    "\n",
    "    if len(data) < num_bins * 2:\n",
    "        num_bins = max(1, len(data) // 2)\n",
    "\n",
    "    try:\n",
    "        bins_cut, bin_edges = pd.qcut(data, q=num_bins, retbins=True, duplicates='drop')\n",
    "        bin_counts = bins_cut.value_counts().sort_index()\n",
    "\n",
    "        bins = []\n",
    "        for i, (interval, count) in enumerate(bin_counts.items(), 1):\n",
    "            if i == 1:\n",
    "                interval_str = f\"[{interval.left:.2f}, {interval.right:.2f}]\"\n",
    "            else:\n",
    "                interval_str = f\"({interval.left:.2f}, {interval.right:.2f}]\"\n",
    "\n",
    "            bins.append({\n",
    "                'bin_num': i,\n",
    "                'bin_min': float(interval.left),\n",
    "                'bin_max': float(interval.right),\n",
    "                'count': int(count),\n",
    "                'percentage': float(count / len(data) * 100),\n",
    "                'interval': interval_str\n",
    "            })\n",
    "\n",
    "        return bins\n",
    "\n",
    "    except Exception as e:\n",
    "        return [{\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(data.min()),\n",
    "            'bin_max': float(data.max()),\n",
    "            'count': len(data),\n",
    "            'percentage': 100.0,\n",
    "            'interval': f\"[{data.min():.2f}, {data.max():.2f}]\"\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins_for_segment_pandas(\n",
    "    data: pd.Series,\n",
    "    segment_min: float,\n",
    "    segment_max: float,\n",
    "    num_bins: int,\n",
    "    segment_name: str,\n",
    "    extra_bins_last: int = 0,\n",
    "    split_first: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create quantile-based bins for a segment using Pandas.\n",
    "    \"\"\"\n",
    "    segment_data = data[(data >= segment_min) & (data <= segment_max)]\n",
    "\n",
    "    if len(segment_data) == 0:\n",
    "        return []\n",
    "\n",
    "    if num_bins == 1 or len(segment_data) < num_bins * 2:\n",
    "        return [{\n",
    "            'segment': segment_name,\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(segment_data.min()),\n",
    "            'bin_max': float(segment_data.max()),\n",
    "            'count': len(segment_data),\n",
    "            'percentage': 100.0\n",
    "        }]\n",
    "\n",
    "    try:\n",
    "        quantiles = np.linspace(0, 1, num_bins + 1)\n",
    "        bins_cut, bin_edges = pd.qcut(segment_data, q=quantiles, retbins=True, duplicates='drop')\n",
    "        bin_counts = bins_cut.value_counts().sort_index()\n",
    "\n",
    "        bins = []\n",
    "        for i, (interval, count) in enumerate(bin_counts.items(), 1):\n",
    "            bins.append({\n",
    "                'segment': segment_name,\n",
    "                'bin_num': i,\n",
    "                'bin_min': float(interval.left),\n",
    "                'bin_max': float(interval.right),\n",
    "                'count': int(count),\n",
    "                'percentage': float(count / len(segment_data) * 100)\n",
    "            })\n",
    "\n",
    "        # Handle extra bins for extreme values\n",
    "        if extra_bins_last > 0 and len(bins) > 0:\n",
    "            if split_first:\n",
    "                extreme_bin = bins[0]\n",
    "                extreme_data = segment_data[\n",
    "                    (segment_data >= extreme_bin['bin_min']) &\n",
    "                    (segment_data <= extreme_bin['bin_max'])\n",
    "                ]\n",
    "\n",
    "                if len(extreme_data) >= extra_bins_last * 2:\n",
    "                    tail_quantiles = np.linspace(0, 1, extra_bins_last + 1)\n",
    "                    tail_bins_cut, tail_edges = pd.qcut(\n",
    "                        extreme_data, q=tail_quantiles, retbins=True, duplicates='drop'\n",
    "                    )\n",
    "\n",
    "                    bins = bins[1:]\n",
    "                    tail_counts = tail_bins_cut.value_counts().sort_index()\n",
    "                    \n",
    "                    new_bins = []\n",
    "                    for j, (interval, count) in enumerate(tail_counts.items(), 1):\n",
    "                        new_bins.append({\n",
    "                            'segment': segment_name,\n",
    "                            'bin_num': j,\n",
    "                            'bin_min': float(interval.left),\n",
    "                            'bin_max': float(interval.right),\n",
    "                            'count': int(count),\n",
    "                            'percentage': float(count / len(segment_data) * 100)\n",
    "                        })\n",
    "\n",
    "                    for bin_info in bins:\n",
    "                        bin_info['bin_num'] = len(new_bins) + 1\n",
    "                        new_bins.append(bin_info)\n",
    "\n",
    "                    bins = new_bins\n",
    "\n",
    "            else:\n",
    "                extreme_bin = bins[-1]\n",
    "                extreme_data = segment_data[\n",
    "                    (segment_data >= extreme_bin['bin_min']) &\n",
    "                    (segment_data <= extreme_bin['bin_max'])\n",
    "                ]\n",
    "\n",
    "                if len(extreme_data) >= extra_bins_last * 2:\n",
    "                    tail_quantiles = np.linspace(0, 1, extra_bins_last + 1)\n",
    "                    tail_bins_cut, tail_edges = pd.qcut(\n",
    "                        extreme_data, q=tail_quantiles, retbins=True, duplicates='drop'\n",
    "                    )\n",
    "\n",
    "                    bins = bins[:-1]\n",
    "                    tail_counts = tail_bins_cut.value_counts().sort_index()\n",
    "                    \n",
    "                    for j, (interval, count) in enumerate(tail_counts.items(), 1):\n",
    "                        bins.append({\n",
    "                            'segment': segment_name,\n",
    "                            'bin_num': len(bins) + 1,\n",
    "                            'bin_min': float(interval.left),\n",
    "                            'bin_max': float(interval.right),\n",
    "                            'count': int(count),\n",
    "                            'percentage': float(count / len(segment_data) * 100)\n",
    "                        })\n",
    "\n",
    "        return bins\n",
    "\n",
    "    except Exception as e:\n",
    "        return [{\n",
    "            'segment': segment_name,\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(segment_data.min()),\n",
    "            'bin_max': float(segment_data.max()),\n",
    "            'count': len(segment_data),\n",
    "            'percentage': 100.0\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_bins_pandas(\n",
    "    data: pd.Series,\n",
    "    normal_lower: float,\n",
    "    normal_upper: float,\n",
    "    outlier_min: float,\n",
    "    outlier_max: float,\n",
    "    bins_below: int,\n",
    "    bins_normal: int,\n",
    "    bins_above: int,\n",
    "    extra_bins_below: int = 0,\n",
    "    extra_bins_above: int = 0\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create bins for all segments (below/normal/above) using Pandas.\n",
    "    Used for labs and vitals.\n",
    "    \"\"\"\n",
    "    all_bins = []\n",
    "\n",
    "    if bins_below > 0 and outlier_min < normal_lower:\n",
    "        below_bins = create_bins_for_segment_pandas(\n",
    "            data, outlier_min, normal_lower, bins_below, 'below',\n",
    "            extra_bins_last=extra_bins_below,\n",
    "            split_first=True\n",
    "        )\n",
    "        all_bins.extend(below_bins)\n",
    "\n",
    "    if bins_normal > 0:\n",
    "        normal_bins = create_bins_for_segment_pandas(\n",
    "            data, normal_lower, normal_upper, bins_normal, 'normal'\n",
    "        )\n",
    "        all_bins.extend(normal_bins)\n",
    "\n",
    "    if bins_above > 0 and normal_upper < outlier_max:\n",
    "        above_bins = create_bins_for_segment_pandas(\n",
    "            data, normal_upper, outlier_max, bins_above, 'above',\n",
    "            extra_bins_last=extra_bins_above,\n",
    "            split_first=False\n",
    "        )\n",
    "        all_bins.extend(above_bins)\n",
    "\n",
    "    return all_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Labs/Vitals (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_pandas(\n",
    "    table_type: str,\n",
    "    category: str,\n",
    "    unit: Optional[str],\n",
    "    icu_windows: pd.DataFrame,\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    outlier_range: Dict[str, float],\n",
    "    cat_config: Dict[str, Any],\n",
    "    output_dir: str = None,\n",
    "    extreme_bins_count: int = 5,\n",
    "    save_output: bool = False\n",
    ") -> Tuple[Dict[str, Any], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process a single lab/vital category using pure Pandas.\n",
    "    \"\"\"\n",
    "    if table_type == 'labs':\n",
    "        file_path = '../../output/intermediate/clif_labs_standardized.parquet'\n",
    "        category_col = 'lab_category'\n",
    "        value_col = 'lab_value_numeric'\n",
    "        datetime_col = 'lab_result_dttm'\n",
    "    else:\n",
    "        file_path = os.path.join(tables_path, f'clif_vitals.{file_type}')\n",
    "        category_col = 'vital_category'\n",
    "        value_col = 'vital_value'\n",
    "        datetime_col = 'recorded_dttm'\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "\n",
    "    display_name = f\"{category} ({unit})\" if table_type == 'labs' and unit else category\n",
    "    print(f\"Processing {display_name}...\")\n",
    "\n",
    "    columns_to_load = ['hospitalization_id', datetime_col, value_col, category_col]\n",
    "    if table_type == 'labs':\n",
    "        columns_to_load.append('reference_unit')\n",
    "    \n",
    "    data_df = pd.read_parquet(file_path, columns=columns_to_load)\n",
    "    \n",
    "    data_df = data_df[data_df[category_col] == category].copy()\n",
    "    \n",
    "    if table_type == 'labs' and unit:\n",
    "        if isinstance(unit, list):\n",
    "            unit_filter = data_df['reference_unit'].isin(unit)\n",
    "            # Also match null if '(no units)' is in the list\n",
    "            if '(no units)' in unit:\n",
    "                unit_filter = unit_filter | data_df['reference_unit'].isna()\n",
    "        else:\n",
    "            if unit == '(no units)':\n",
    "                unit_filter = data_df['reference_unit'].isna()\n",
    "            else:\n",
    "                unit_filter = data_df['reference_unit'] == unit\n",
    "\n",
    "        data_df = data_df[unit_filter]\n",
    "    \n",
    "    if data_df[datetime_col].dt.tz is not None:\n",
    "        data_df[datetime_col] = data_df[datetime_col].dt.tz_localize(None)\n",
    "\n",
    "    data_df = data_df.merge(icu_windows, on='hospitalization_id', how='inner')\n",
    "    \n",
    "    data_df = data_df[\n",
    "        (data_df[datetime_col] >= data_df['in_dttm']) &\n",
    "        (data_df[datetime_col] <= data_df['out_dttm'])\n",
    "    ]\n",
    "\n",
    "    original_count = len(data_df)\n",
    "    print(f\"  Original count: {original_count:,}\")\n",
    "\n",
    "    if original_count == 0:\n",
    "        print(f\"  WARNING: No data found for {display_name} during ICU stays\")\n",
    "        return (\n",
    "            {'category': category, 'unit': unit, 'original_count': 0, 'clean_count': 0},\n",
    "            pd.DataFrame({'value': [], 'probability': []}),\n",
    "            pd.DataFrame()\n",
    "        )\n",
    "\n",
    "    values_clean = data_df[\n",
    "        (data_df[value_col] >= outlier_range['min']) &\n",
    "        (data_df[value_col] <= outlier_range['max'])\n",
    "    ][value_col]\n",
    "\n",
    "    clean_count = len(values_clean)\n",
    "    print(f\"  After outlier removal: {clean_count:,} (removed {original_count - clean_count:,})\")\n",
    "\n",
    "    if clean_count == 0:\n",
    "        print(f\"  WARNING: No data remaining after outlier removal\")\n",
    "        return (\n",
    "            {'category': category, 'unit': unit, 'original_count': original_count, 'clean_count': 0},\n",
    "            pd.DataFrame({'value': [], 'probability': []}),\n",
    "            pd.DataFrame()\n",
    "        )\n",
    "\n",
    "    # Compute ECDF\n",
    "    ecdf_df = compute_ecdf_pandas(values_clean)\n",
    "    print(f\"  ECDF: {len(ecdf_df):,} distinct pairs (compression: {clean_count / len(ecdf_df):.1f}x)\")\n",
    "\n",
    "    # Compute Bins\n",
    "    bins_config = cat_config.get('bins', {})\n",
    "    bins_below = bins_config.get('below_normal', 0) or 0\n",
    "    bins_normal = bins_config.get('normal', 0) or 0\n",
    "    bins_above = bins_config.get('above_normal', 0) or 0\n",
    "\n",
    "    extra_bins_below = extreme_bins_count if bins_below > 1 else 0\n",
    "    extra_bins_above = extreme_bins_count if bins_above > 1 else 0\n",
    "\n",
    "    normal_range = cat_config.get('normal_range', {})\n",
    "    normal_lower = normal_range.get('lower', outlier_range['min'])\n",
    "    normal_upper = normal_range.get('upper', outlier_range['max'])\n",
    "\n",
    "    bins = create_all_bins_pandas(\n",
    "        data=values_clean,\n",
    "        normal_lower=normal_lower,\n",
    "        normal_upper=normal_upper,\n",
    "        outlier_min=outlier_range['min'],\n",
    "        outlier_max=outlier_range['max'],\n",
    "        bins_below=bins_below,\n",
    "        bins_normal=bins_normal,\n",
    "        bins_above=bins_above,\n",
    "        extra_bins_below=extra_bins_below,\n",
    "        extra_bins_above=extra_bins_above\n",
    "    )\n",
    "\n",
    "    for bin_info in bins:\n",
    "        if bin_info['bin_num'] == 1:\n",
    "            interval = f\"[{bin_info['bin_min']:.2f}, {bin_info['bin_max']:.2f}]\"\n",
    "        else:\n",
    "            interval = f\"({bin_info['bin_min']:.2f}, {bin_info['bin_max']:.2f}]\"\n",
    "        bin_info['interval'] = interval\n",
    "\n",
    "    bins_df = pd.DataFrame(bins) if bins else pd.DataFrame()\n",
    "    print(f\"  Bins: {len(bins_df)}\")\n",
    "\n",
    "    if save_output and output_dir:\n",
    "        if table_type == 'labs' and unit:\n",
    "            unit_for_filename = unit[0] if isinstance(unit, list) else unit\n",
    "            unit_safe = sanitize_unit_for_filename(unit_for_filename)\n",
    "            filename = f'{category}_{unit_safe}.parquet'\n",
    "        else:\n",
    "            filename = f'{category}.parquet'\n",
    "\n",
    "        ecdf_dir = os.path.join(output_dir, 'ecdf', table_type)\n",
    "        os.makedirs(ecdf_dir, exist_ok=True)\n",
    "        ecdf_df.to_parquet(os.path.join(ecdf_dir, filename), index=False)\n",
    "\n",
    "        bins_dir = os.path.join(output_dir, 'bins', table_type)\n",
    "        os.makedirs(bins_dir, exist_ok=True)\n",
    "        bins_df.to_parquet(os.path.join(bins_dir, filename), index=False)\n",
    "\n",
    "        print(f\"  Saved to {output_dir}\")\n",
    "\n",
    "    stats = {\n",
    "        'category': category,\n",
    "        'unit': unit if table_type == 'labs' else None,\n",
    "        'original_count': original_count,\n",
    "        'clean_count': clean_count,\n",
    "        'ecdf_distinct_pairs': len(ecdf_df),\n",
    "        'num_bins': len(bins)\n",
    "    }\n",
    "\n",
    "    return stats, ecdf_df, bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Respiratory Support (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_respiratory_column_pandas(\n",
    "    column_name: str,\n",
    "    icu_windows: pd.DataFrame,\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    outlier_range: Dict[str, float],\n",
    "    output_dir: str = None,\n",
    "    num_bins: int = 10,\n",
    "    save_output: bool = False\n",
    ") -> Tuple[Dict[str, Any], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process a single respiratory support column using pure Pandas.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(tables_path, f'clif_respiratory_support.{file_type}')\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Respiratory support file not found: {file_path}\")\n",
    "\n",
    "    print(f\"Processing {column_name}...\")\n",
    "\n",
    "    data_df = pd.read_parquet(file_path, columns=['hospitalization_id', 'recorded_dttm', column_name])\n",
    "    \n",
    "    if data_df['recorded_dttm'].dt.tz is not None:\n",
    "        data_df['recorded_dttm'] = data_df['recorded_dttm'].dt.tz_localize(None)\n",
    "    \n",
    "    data_df = data_df.merge(icu_windows, on='hospitalization_id', how='inner')\n",
    "    \n",
    "    data_df = data_df[\n",
    "        (data_df['recorded_dttm'] >= data_df['in_dttm']) &\n",
    "        (data_df['recorded_dttm'] <= data_df['out_dttm'])\n",
    "    ]\n",
    "    \n",
    "    data_df = data_df.dropna(subset=[column_name])\n",
    "\n",
    "    original_count = len(data_df)\n",
    "    print(f\"  Original count: {original_count:,}\")\n",
    "\n",
    "    if original_count == 0:\n",
    "        print(f\"  WARNING: No data found for {column_name}\")\n",
    "        return (\n",
    "            {'column': column_name, 'original_count': 0, 'clean_count': 0},\n",
    "            pd.DataFrame({'value': [], 'probability': []}),\n",
    "            pd.DataFrame()\n",
    "        )\n",
    "\n",
    "    values_clean = data_df[\n",
    "        (data_df[column_name] >= outlier_range['min']) &\n",
    "        (data_df[column_name] <= outlier_range['max'])\n",
    "    ][column_name]\n",
    "\n",
    "    clean_count = len(values_clean)\n",
    "    print(f\"  After outlier removal: {clean_count:,} (removed {original_count - clean_count:,})\")\n",
    "\n",
    "    if clean_count == 0:\n",
    "        print(f\"  WARNING: No data remaining after outlier removal\")\n",
    "        return (\n",
    "            {'column': column_name, 'original_count': original_count, 'clean_count': 0},\n",
    "            pd.DataFrame({'value': [], 'probability': []}),\n",
    "            pd.DataFrame()\n",
    "        )\n",
    "\n",
    "    # Compute ECDF\n",
    "    ecdf_df = compute_ecdf_pandas(values_clean)\n",
    "    print(f\"  ECDF: {len(ecdf_df):,} distinct pairs (compression: {clean_count / len(ecdf_df):.1f}x)\")\n",
    "\n",
    "    # Compute Flat Bins\n",
    "    bins = create_flat_bins_pandas(values_clean, num_bins=num_bins)\n",
    "    bins_df = pd.DataFrame(bins) if bins else pd.DataFrame()\n",
    "    print(f\"  Bins: {len(bins_df)}\")\n",
    "\n",
    "    if save_output and output_dir:\n",
    "        ecdf_dir = os.path.join(output_dir, 'ecdf', 'respiratory_support')\n",
    "        os.makedirs(ecdf_dir, exist_ok=True)\n",
    "        ecdf_df.to_parquet(os.path.join(ecdf_dir, f'{column_name}.parquet'), index=False)\n",
    "\n",
    "        bins_dir = os.path.join(output_dir, 'bins', 'respiratory_support')\n",
    "        os.makedirs(bins_dir, exist_ok=True)\n",
    "        bins_df.to_parquet(os.path.join(bins_dir, f'{column_name}.parquet'), index=False)\n",
    "\n",
    "        print(f\"  Saved to {output_dir}\")\n",
    "\n",
    "    stats = {\n",
    "        'column': column_name,\n",
    "        'original_count': original_count,\n",
    "        'clean_count': clean_count,\n",
    "        'ecdf_distinct_pairs': len(ecdf_df),\n",
    "        'num_bins': len(bins)\n",
    "    }\n",
    "\n",
    "    return stats, ecdf_df, bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_config = lab_vital_config.get('labs', {})\n",
    "labs_outlier = outlier_config['tables']['labs']['lab_value_numeric']\n",
    "\n",
    "vitals_config = lab_vital_config.get('vitals', {})\n",
    "vitals_outlier = outlier_config['tables']['vitals']['vital_value']\n",
    "\n",
    "resp_outlier = outlier_config['tables'].get('respiratory_support', {})\n",
    "\n",
    "resp_columns = [\n",
    "    'fio2_set', 'lpm_set', 'tidal_volume_set', 'resp_rate_set',\n",
    "    'pressure_control_set', 'pressure_support_set', 'flow_rate_set',\n",
    "    'peak_inspiratory_pressure_set', 'inspiratory_time_set', 'peep_set',\n",
    "    'tidal_volume_obs', 'resp_rate_obs', 'plateau_pressure_obs',\n",
    "    'peak_inspiratory_pressure_obs', 'peep_obs', 'minute_vent_obs',\n",
    "    'mean_airway_pressure_obs'\n",
    "]\n",
    "\n",
    "print(f\"Labs: {len(labs_config)} categories\")\n",
    "print(f\"Vitals: {len(vitals_config)} categories\")\n",
    "print(f\"Respiratory: {len(resp_columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = str(PROJECT_ROOT / 'output' / 'final')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Labs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "labs_stats = []\n",
    "\n",
    "for category, cat_config in labs_config.items():\n",
    "    if cat_config is None:\n",
    "        print(f\"WARNING: Category '{category}' has None config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(cat_config, dict):\n",
    "        continue\n",
    "\n",
    "    if category not in labs_outlier:\n",
    "        print(f\"WARNING: Category '{category}' not in outlier config, skipping\")\n",
    "        continue\n",
    "\n",
    "    config_unit = cat_config.get('reference_unit')\n",
    "    if config_unit is None:\n",
    "        print(f\"WARNING: Category '{category}' has no reference_unit in config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if isinstance(config_unit, str):\n",
    "        unit = config_unit.lower()\n",
    "    elif isinstance(config_unit, list):\n",
    "        unit = [u.lower() if isinstance(u, str) else u for u in config_unit]\n",
    "    else:\n",
    "        unit = config_unit\n",
    "\n",
    "    try:\n",
    "        stats, ecdf_df, bins_df = process_category_pandas(\n",
    "            table_type='labs',\n",
    "            category=category,\n",
    "            unit=unit,\n",
    "            icu_windows=icu_windows,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=labs_outlier[category],\n",
    "            cat_config=cat_config,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            extreme_bins_count=5,\n",
    "            save_output=True\n",
    "        )\n",
    "        labs_stats.append(stats)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {category}: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(labs_stats)} lab categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Vitals\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vitals_stats = []\n",
    "\n",
    "for category, cat_config in vitals_config.items():\n",
    "    if cat_config is None:\n",
    "        print(f\"WARNING: Category '{category}' has None config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(cat_config, dict):\n",
    "        continue\n",
    "\n",
    "    if category not in vitals_outlier:\n",
    "        print(f\"WARNING: Category '{category}' not in outlier config, skipping\")\n",
    "        continue\n",
    "\n",
    "    extreme_bins = 5 if category in ['height_cm', 'weight_kg'] else 10\n",
    "\n",
    "    try:\n",
    "        stats, ecdf_df, bins_df = process_category_pandas(\n",
    "            table_type='vitals',\n",
    "            category=category,\n",
    "            unit=None,\n",
    "            icu_windows=icu_windows,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=vitals_outlier[category],\n",
    "            cat_config=cat_config,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            extreme_bins_count=extreme_bins,\n",
    "            save_output=True\n",
    "        )\n",
    "        vitals_stats.append(stats)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {category}: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(vitals_stats)} vital categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Respiratory Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Respiratory Support\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "resp_stats = []\n",
    "\n",
    "for column_name in resp_columns:\n",
    "    if column_name not in resp_outlier:\n",
    "        print(f\"WARNING: Column '{column_name}' not in outlier config, skipping\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        stats, ecdf_df, bins_df = process_respiratory_column_pandas(\n",
    "            column_name=column_name,\n",
    "            icu_windows=icu_windows,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=resp_outlier[column_name],\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            num_bins=10,\n",
    "            save_output=True\n",
    "        )\n",
    "        resp_stats.append(stats)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {column_name}: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(resp_stats)} respiratory columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Processing Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nLabs: {len(labs_stats)} categories processed\")\n",
    "print(f\"Vitals: {len(vitals_stats)} categories processed\")\n",
    "print(f\"Respiratory: {len(resp_stats)} columns processed\")\n",
    "print(f\"\\nOutput saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
