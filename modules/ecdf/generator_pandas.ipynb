{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECDF Generator - Pandas Only\n",
    "\n",
    "This notebook uses **Pandas exclusively** for the ECDF computation pipeline.\n",
    "\n",
    "**Features:**\n",
    "1. Extract ICU time windows from ADT table\n",
    "2. Filter labs/vitals to values during ICU stays only\n",
    "3. Standardize lab reference units\n",
    "4. Compute ECDF (distinct value/probability pairs)\n",
    "5. Compute quantile bins with auto-extreme-splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# For notebook: set project root relative to notebook location\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configs(\n",
    "    clif_config_path: str = None,\n",
    "    outlier_config_path: str = None,\n",
    "    lab_vital_config_path: str = None\n",
    ") -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Load all required configuration files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (clif_config, outlier_config, lab_vital_config)\n",
    "    \"\"\"\n",
    "    if clif_config_path is None:\n",
    "        clif_config_path = PROJECT_ROOT / 'config' / 'config.json'\n",
    "    if outlier_config_path is None:\n",
    "        outlier_config_path = PROJECT_ROOT / 'modules' / 'ecdf' / 'config' / 'outlier_config.yaml'\n",
    "    if lab_vital_config_path is None:\n",
    "        lab_vital_config_path = PROJECT_ROOT / 'modules' / 'ecdf' / 'config' / 'lab_vital_config.yaml'\n",
    "\n",
    "    with open(clif_config_path, 'r') as f:\n",
    "        clif_config = json.load(f)\n",
    "\n",
    "    with open(outlier_config_path, 'r') as f:\n",
    "        outlier_config = yaml.safe_load(f)\n",
    "\n",
    "    with open(lab_vital_config_path, 'r') as f:\n",
    "        lab_vital_config = yaml.safe_load(f)\n",
    "\n",
    "    return clif_config, outlier_config, lab_vital_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "clif_config, outlier_config, lab_vital_config = load_configs()\n",
    "\n",
    "print(\"CLIF Config:\")\n",
    "print(f\"  tables_path: {clif_config.get('tables_path')}\")\n",
    "print(f\"  file_type: {clif_config.get('file_type')}\")\n",
    "print()\n",
    "print(f\"Outlier config tables: {list(outlier_config.get('tables', {}).keys())}\")\n",
    "print(f\"Lab categories in config: {list(lab_vital_config.get('labs', {}).keys())}\")\n",
    "print(f\"Vital categories in config: {list(lab_vital_config.get('vitals', {}).keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_unit_for_filename(unit) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize unit string for use in filename.\n",
    "    Handles both string and list inputs.\n",
    "    \"\"\"\n",
    "    if unit is None:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # Handle list of units\n",
    "    if isinstance(unit, list):\n",
    "        unit = '_'.join(str(u) for u in unit if u is not None)\n",
    "    \n",
    "    if not unit:\n",
    "        return \"unknown\"\n",
    "\n",
    "    sanitized = unit.replace('/', '_').replace('%', 'pct').replace('Â°', 'deg')\n",
    "    sanitized = ''.join(c if c.isalnum() or c == '_' else '_' for c in sanitized)\n",
    "    while '__' in sanitized:\n",
    "        sanitized = sanitized.replace('__', '_')\n",
    "    sanitized = sanitized.strip('_').lower()\n",
    "\n",
    "    return sanitized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Standardization (using clifpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clifpy\n",
    "from clifpy.tables import Labs\n",
    "print(f\"clifpy location: {clifpy.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_labs_pandas(\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    output_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Standardize lab reference units and save as parquet.\n",
    "    Uses clifpy with Pandas DataFrame input.\n",
    "    \"\"\"\n",
    "    labs_path = os.path.join(tables_path, f'clif_labs.{file_type}')\n",
    "    print(f\"Loading labs from {labs_path}...\")\n",
    "\n",
    "    # Load as Pandas DataFrame\n",
    "    labs_df = pd.read_parquet(labs_path)\n",
    "    print(f\"Loaded {len(labs_df):,} rows\")\n",
    "    \n",
    "    labs_inst = Labs(data=labs_df)\n",
    "    \n",
    "    # Standardize (returns pandas DataFrame when inplace=False)\n",
    "    labs_standard = labs_inst.standardize_reference_units(\n",
    "        save=True, \n",
    "        output_directory=str(Path(output_path).parent),\n",
    "        lowercase=True, \n",
    "        inplace=False\n",
    "    )\n",
    "    \n",
    "    # Save as parquet\n",
    "    labs_standard.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved standardized labs to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize labs (run once)\n",
    "STANDARDIZED_LABS_PATH = '../../output/intermediate/clif_labs_standardized.parquet'\n",
    "os.makedirs(os.path.dirname(STANDARDIZED_LABS_PATH), exist_ok=True)\n",
    "\n",
    "# Uncomment to run standardization\n",
    "# standardize_labs_pandas(\n",
    "#     clif_config['tables_path'],\n",
    "#     clif_config['file_type'],\n",
    "#     STANDARDIZED_LABS_PATH\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICU Time Window Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_icu_time_windows(\n",
    "    tables_path: str,\n",
    "    file_type: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract ICU time windows from ADT table using Pandas.\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame with columns:\n",
    "        - hospitalization_id: str\n",
    "        - in_dttm: datetime\n",
    "        - out_dttm: datetime\n",
    "    \"\"\"\n",
    "    adt_path = os.path.join(tables_path, f'clif_adt.{file_type}')\n",
    "    print(f\"Loading ICU time windows from {adt_path}...\")\n",
    "\n",
    "    # Load ADT table\n",
    "    adt_df = pd.read_parquet(adt_path, columns=['hospitalization_id', 'location_category', 'in_dttm', 'out_dttm'])\n",
    "    \n",
    "    # Filter to ICU locations\n",
    "    icu_windows_df = adt_df[adt_df['location_category'].str.lower() == 'icu'][['hospitalization_id', 'in_dttm', 'out_dttm']].copy()\n",
    "    \n",
    "    # Remove timezone info for comparison\n",
    "    if icu_windows_df['in_dttm'].dt.tz is not None:\n",
    "        icu_windows_df['in_dttm'] = icu_windows_df['in_dttm'].dt.tz_localize(None)\n",
    "    if icu_windows_df['out_dttm'].dt.tz is not None:\n",
    "        icu_windows_df['out_dttm'] = icu_windows_df['out_dttm'].dt.tz_localize(None)\n",
    "\n",
    "    print(f\"Found {len(icu_windows_df):,} ICU time windows\")\n",
    "    return icu_windows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ICU time windows\n",
    "icu_windows = extract_icu_time_windows(\n",
    "    clif_config['tables_path'],\n",
    "    clif_config['file_type']\n",
    ")\n",
    "\n",
    "print(\"\\nSample ICU time windows:\")\n",
    "icu_windows.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECDF Computation (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ecdf_pandas(values: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute ECDF using pure Pandas operations.\n",
    "\n",
    "    Args:\n",
    "        values: Pandas Series of numeric values\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame with columns:\n",
    "        - value: float\n",
    "        - probability: float (0 to 1)\n",
    "    \"\"\"\n",
    "    if len(values) == 0:\n",
    "        return pd.DataFrame({'value': [], 'probability': []})\n",
    "\n",
    "    n = len(values)\n",
    "    \n",
    "    # Sort values and compute cumulative probability\n",
    "    sorted_values = values.sort_values().reset_index(drop=True)\n",
    "    probabilities = (np.arange(1, n + 1)) / n\n",
    "    \n",
    "    ecdf_df = pd.DataFrame({\n",
    "        'value': sorted_values,\n",
    "        'probability': probabilities\n",
    "    })\n",
    "    \n",
    "    # Group by value and keep max probability (for duplicates)\n",
    "    ecdf_df = ecdf_df.groupby('value', as_index=False)['probability'].max()\n",
    "    ecdf_df = ecdf_df.sort_values('value').reset_index(drop=True)\n",
    "\n",
    "    return ecdf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ECDF computation\n",
    "test_values = pd.Series([1.0, 2.0, 2.0, 3.0, 4.0, 5.0, 5.0, 5.0, 6.0, 7.0])\n",
    "ecdf_result = compute_ecdf_pandas(test_values)\n",
    "\n",
    "print(\"Test ECDF computation:\")\n",
    "print(f\"Input values: {test_values.tolist()}\")\n",
    "print(f\"\\nECDF result:\")\n",
    "ecdf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Functions (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quantile_bins_pandas(\n",
    "    data: pd.Series,\n",
    "    num_bins: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create quantile bins using Pandas qcut.\n",
    "\n",
    "    Args:\n",
    "        data: Pandas Series with values\n",
    "        num_bins: Number of bins to create\n",
    "\n",
    "    Returns:\n",
    "        List of bin dictionaries\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return []\n",
    "\n",
    "    if len(data) < num_bins * 2:\n",
    "        num_bins = max(1, len(data) // 2)\n",
    "\n",
    "    try:\n",
    "        bins_cut, bin_edges = pd.qcut(data, q=num_bins, retbins=True, duplicates='drop')\n",
    "        bin_counts = bins_cut.value_counts().sort_index()\n",
    "\n",
    "        bins = []\n",
    "        for i, (interval, count) in enumerate(bin_counts.items(), 1):\n",
    "            if i == 1:\n",
    "                interval_str = f\"[{interval.left:.2f}, {interval.right:.2f}]\"\n",
    "            else:\n",
    "                interval_str = f\"({interval.left:.2f}, {interval.right:.2f}]\"\n",
    "\n",
    "            bins.append({\n",
    "                'bin_num': i,\n",
    "                'bin_min': float(interval.left),\n",
    "                'bin_max': float(interval.right),\n",
    "                'count': int(count),\n",
    "                'percentage': float(count / len(data) * 100),\n",
    "                'interval': interval_str\n",
    "            })\n",
    "\n",
    "        return bins\n",
    "\n",
    "    except Exception as e:\n",
    "        return [{\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(data.min()),\n",
    "            'bin_max': float(data.max()),\n",
    "            'count': len(data),\n",
    "            'percentage': 100.0,\n",
    "            'interval': f\"[{data.min():.2f}, {data.max():.2f}]\"\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins_for_segment_pandas(\n",
    "    data: pd.Series,\n",
    "    segment_min: float,\n",
    "    segment_max: float,\n",
    "    num_bins: int,\n",
    "    segment_name: str,\n",
    "    extra_bins_last: int = 0,\n",
    "    split_first: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create quantile-based bins for a segment using Pandas.\n",
    "    \"\"\"\n",
    "    # Filter data to segment range\n",
    "    segment_data = data[(data >= segment_min) & (data <= segment_max)]\n",
    "\n",
    "    if len(segment_data) == 0:\n",
    "        return []\n",
    "\n",
    "    if num_bins == 1 or len(segment_data) < num_bins * 2:\n",
    "        return [{\n",
    "            'segment': segment_name,\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(segment_data.min()),\n",
    "            'bin_max': float(segment_data.max()),\n",
    "            'count': len(segment_data),\n",
    "            'percentage': 100.0\n",
    "        }]\n",
    "\n",
    "    try:\n",
    "        quantiles = np.linspace(0, 1, num_bins + 1)\n",
    "        bins_cut, bin_edges = pd.qcut(segment_data, q=quantiles, retbins=True, duplicates='drop')\n",
    "        bin_counts = bins_cut.value_counts().sort_index()\n",
    "\n",
    "        bins = []\n",
    "        for i, (interval, count) in enumerate(bin_counts.items(), 1):\n",
    "            bins.append({\n",
    "                'segment': segment_name,\n",
    "                'bin_num': i,\n",
    "                'bin_min': float(interval.left),\n",
    "                'bin_max': float(interval.right),\n",
    "                'count': int(count),\n",
    "                'percentage': float(count / len(segment_data) * 100)\n",
    "            })\n",
    "\n",
    "        # Handle extra bins for extreme values\n",
    "        if extra_bins_last > 0 and len(bins) > 0:\n",
    "            if split_first:\n",
    "                # Split first bin\n",
    "                extreme_bin = bins[0]\n",
    "                extreme_data = segment_data[\n",
    "                    (segment_data >= extreme_bin['bin_min']) &\n",
    "                    (segment_data <= extreme_bin['bin_max'])\n",
    "                ]\n",
    "\n",
    "                if len(extreme_data) >= extra_bins_last * 2:\n",
    "                    tail_quantiles = np.linspace(0, 1, extra_bins_last + 1)\n",
    "                    tail_bins_cut, tail_edges = pd.qcut(\n",
    "                        extreme_data, q=tail_quantiles, retbins=True, duplicates='drop'\n",
    "                    )\n",
    "\n",
    "                    bins = bins[1:]  # Remove original first bin\n",
    "                    tail_counts = tail_bins_cut.value_counts().sort_index()\n",
    "                    \n",
    "                    new_bins = []\n",
    "                    for j, (interval, count) in enumerate(tail_counts.items(), 1):\n",
    "                        new_bins.append({\n",
    "                            'segment': segment_name,\n",
    "                            'bin_num': j,\n",
    "                            'bin_min': float(interval.left),\n",
    "                            'bin_max': float(interval.right),\n",
    "                            'count': int(count),\n",
    "                            'percentage': float(count / len(segment_data) * 100)\n",
    "                        })\n",
    "\n",
    "                    for bin_info in bins:\n",
    "                        bin_info['bin_num'] = len(new_bins) + 1\n",
    "                        new_bins.append(bin_info)\n",
    "\n",
    "                    bins = new_bins\n",
    "\n",
    "            else:\n",
    "                # Split last bin\n",
    "                extreme_bin = bins[-1]\n",
    "                extreme_data = segment_data[\n",
    "                    (segment_data >= extreme_bin['bin_min']) &\n",
    "                    (segment_data <= extreme_bin['bin_max'])\n",
    "                ]\n",
    "\n",
    "                if len(extreme_data) >= extra_bins_last * 2:\n",
    "                    tail_quantiles = np.linspace(0, 1, extra_bins_last + 1)\n",
    "                    tail_bins_cut, tail_edges = pd.qcut(\n",
    "                        extreme_data, q=tail_quantiles, retbins=True, duplicates='drop'\n",
    "                    )\n",
    "\n",
    "                    bins = bins[:-1]  # Remove original last bin\n",
    "                    tail_counts = tail_bins_cut.value_counts().sort_index()\n",
    "                    \n",
    "                    for j, (interval, count) in enumerate(tail_counts.items(), 1):\n",
    "                        bins.append({\n",
    "                            'segment': segment_name,\n",
    "                            'bin_num': len(bins) + 1,\n",
    "                            'bin_min': float(interval.left),\n",
    "                            'bin_max': float(interval.right),\n",
    "                            'count': int(count),\n",
    "                            'percentage': float(count / len(segment_data) * 100)\n",
    "                        })\n",
    "\n",
    "        return bins\n",
    "\n",
    "    except Exception as e:\n",
    "        return [{\n",
    "            'segment': segment_name,\n",
    "            'bin_num': 1,\n",
    "            'bin_min': float(segment_data.min()),\n",
    "            'bin_max': float(segment_data.max()),\n",
    "            'count': len(segment_data),\n",
    "            'percentage': 100.0\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_bins_pandas(\n",
    "    data: pd.Series,\n",
    "    normal_lower: float,\n",
    "    normal_upper: float,\n",
    "    outlier_min: float,\n",
    "    outlier_max: float,\n",
    "    bins_below: int,\n",
    "    bins_normal: int,\n",
    "    bins_above: int,\n",
    "    extra_bins_below: int = 0,\n",
    "    extra_bins_above: int = 0\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create bins for all segments using Pandas.\n",
    "    \"\"\"\n",
    "    all_bins = []\n",
    "\n",
    "    # Below normal segment\n",
    "    if bins_below > 0 and outlier_min < normal_lower:\n",
    "        below_bins = create_bins_for_segment_pandas(\n",
    "            data, outlier_min, normal_lower, bins_below, 'below',\n",
    "            extra_bins_last=extra_bins_below,\n",
    "            split_first=True\n",
    "        )\n",
    "        all_bins.extend(below_bins)\n",
    "\n",
    "    # Normal segment\n",
    "    if bins_normal > 0:\n",
    "        normal_bins = create_bins_for_segment_pandas(\n",
    "            data, normal_lower, normal_upper, bins_normal, 'normal'\n",
    "        )\n",
    "        all_bins.extend(normal_bins)\n",
    "\n",
    "    # Above normal segment\n",
    "    if bins_above > 0 and normal_upper < outlier_max:\n",
    "        above_bins = create_bins_for_segment_pandas(\n",
    "            data, normal_upper, outlier_max, bins_above, 'above',\n",
    "            extra_bins_last=extra_bins_above,\n",
    "            split_first=False\n",
    "        )\n",
    "        all_bins.extend(above_bins)\n",
    "\n",
    "    return all_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test binning\n",
    "np.random.seed(42)\n",
    "test_values = pd.Series(np.random.randn(10000) * 20 + 100)\n",
    "\n",
    "bins = create_all_bins_pandas(\n",
    "    data=test_values,\n",
    "    normal_lower=80,\n",
    "    normal_upper=120,\n",
    "    outlier_min=0,\n",
    "    outlier_max=200,\n",
    "    bins_below=3,\n",
    "    bins_normal=4,\n",
    "    bins_above=3,\n",
    "    extra_bins_below=5,\n",
    "    extra_bins_above=5\n",
    ")\n",
    "\n",
    "print(f\"Created {len(bins)} bins:\")\n",
    "for b in bins:\n",
    "    print(f\"  Bin {b['bin_num']:2d}: [{b['bin_min']:8.2f}, {b['bin_max']:8.2f}] - \"\n",
    "          f\"segment: {b['segment']:10s}, count: {b['count']:5d}, pct: {b['percentage']:5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Single Category (Labs/Vitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_pandas(\n",
    "    table_type: str,\n",
    "    category: str,\n",
    "    unit: Optional[str],\n",
    "    icu_windows: pd.DataFrame,\n",
    "    tables_path: str,\n",
    "    file_type: str,\n",
    "    outlier_range: Dict[str, float],\n",
    "    cat_config: Dict[str, Any],\n",
    "    output_dir: str = None,\n",
    "    extreme_bins_count: int = 5,\n",
    "    save_output: bool = False\n",
    ") -> Tuple[Dict[str, Any], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process a single lab/vital category using pure Pandas.\n",
    "    \"\"\"\n",
    "    # Determine file path and column names\n",
    "    if table_type == 'labs':\n",
    "        file_path = '../../output/intermediate/clif_labs_standardized.parquet'\n",
    "        category_col = 'lab_category'\n",
    "        value_col = 'lab_value_numeric'\n",
    "        datetime_col = 'lab_result_dttm'\n",
    "    else:  # vitals\n",
    "        file_path = os.path.join(tables_path, f'clif_vitals.{file_type}')\n",
    "        category_col = 'vital_category'\n",
    "        value_col = 'vital_value'\n",
    "        datetime_col = 'recorded_dttm'\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "\n",
    "    display_name = f\"{category} ({unit})\" if table_type == 'labs' and unit else category\n",
    "    print(f\"Loading {display_name}...\")\n",
    "\n",
    "    # Load data - only necessary columns\n",
    "    columns_to_load = ['hospitalization_id', datetime_col, value_col, category_col]\n",
    "    if table_type == 'labs':\n",
    "        columns_to_load.append('reference_unit')\n",
    "    \n",
    "    data_df = pd.read_parquet(file_path, columns=columns_to_load)\n",
    "    \n",
    "    # Filter to category\n",
    "    data_df = data_df[data_df[category_col] == category].copy()\n",
    "    \n",
    "    # Filter by unit for labs\n",
    "    if table_type == 'labs' and unit:\n",
    "        if isinstance(unit, list):\n",
    "            data_df = data_df[data_df['reference_unit'].isin(unit)]\n",
    "        else:\n",
    "            data_df = data_df[data_df['reference_unit'] == unit]\n",
    "    \n",
    "    # Remove timezone from datetime if present\n",
    "    if data_df[datetime_col].dt.tz is not None:\n",
    "        data_df[datetime_col] = data_df[datetime_col].dt.tz_localize(None)\n",
    "\n",
    "    # Merge with ICU windows\n",
    "    data_df = data_df.merge(icu_windows, on='hospitalization_id', how='inner')\n",
    "    \n",
    "    # Filter to values during ICU stay\n",
    "    data_df = data_df[\n",
    "        (data_df[datetime_col] >= data_df['in_dttm']) &\n",
    "        (data_df[datetime_col] <= data_df['out_dttm'])\n",
    "    ]\n",
    "\n",
    "    original_count = len(data_df)\n",
    "    print(f\"  Original count: {original_count:,}\")\n",
    "\n",
    "    if original_count == 0:\n",
    "        print(f\"  WARNING: No data found for {display_name} during ICU stays\")\n",
    "        return (\n",
    "            {'category': category, 'unit': unit, 'original_count': 0, 'clean_count': 0},\n",
    "            pd.DataFrame({'value': [], 'probability': []}),\n",
    "            pd.DataFrame()\n",
    "        )\n",
    "\n",
    "    # Remove outliers\n",
    "    values_clean = data_df[\n",
    "        (data_df[value_col] >= outlier_range['min']) &\n",
    "        (data_df[value_col] <= outlier_range['max'])\n",
    "    ][value_col]\n",
    "\n",
    "    clean_count = len(values_clean)\n",
    "    print(f\"  After outlier removal: {clean_count:,} (removed {original_count - clean_count:,})\")\n",
    "\n",
    "    if clean_count == 0:\n",
    "        print(f\"  WARNING: No data remaining after outlier removal\")\n",
    "        return (\n",
    "            {'category': category, 'unit': unit, 'original_count': original_count, 'clean_count': 0},\n",
    "            pd.DataFrame({'value': [], 'probability': []}),\n",
    "            pd.DataFrame()\n",
    "        )\n",
    "\n",
    "    # Compute ECDF\n",
    "    ecdf_df = compute_ecdf_pandas(values_clean)\n",
    "    print(f\"  ECDF: {len(ecdf_df):,} distinct pairs (compression: {clean_count / len(ecdf_df):.1f}x)\")\n",
    "\n",
    "    # Compute Bins\n",
    "    bins_below = cat_config['bins']['below_normal']\n",
    "    bins_normal = cat_config['bins']['normal']\n",
    "    bins_above = cat_config['bins']['above_normal']\n",
    "\n",
    "    extra_bins_below = extreme_bins_count if bins_below > 1 else 0\n",
    "    extra_bins_above = extreme_bins_count if bins_above > 1 else 0\n",
    "\n",
    "    bins = create_all_bins_pandas(\n",
    "        data=values_clean,\n",
    "        normal_lower=cat_config['normal_range']['lower'],\n",
    "        normal_upper=cat_config['normal_range']['upper'],\n",
    "        outlier_min=outlier_range['min'],\n",
    "        outlier_max=outlier_range['max'],\n",
    "        bins_below=bins_below,\n",
    "        bins_normal=bins_normal,\n",
    "        bins_above=bins_above,\n",
    "        extra_bins_below=extra_bins_below,\n",
    "        extra_bins_above=extra_bins_above\n",
    "    )\n",
    "\n",
    "    # Add interval notation\n",
    "    for bin_info in bins:\n",
    "        if bin_info['bin_num'] == 1:\n",
    "            interval = f\"[{bin_info['bin_min']:.2f}, {bin_info['bin_max']:.2f}]\"\n",
    "        else:\n",
    "            interval = f\"({bin_info['bin_min']:.2f}, {bin_info['bin_max']:.2f}]\"\n",
    "        bin_info['interval'] = interval\n",
    "\n",
    "    bins_df = pd.DataFrame(bins) if bins else pd.DataFrame()\n",
    "    print(f\"  Bins: {len(bins_df)}\")\n",
    "\n",
    "    # Save output\n",
    "    if save_output and output_dir:\n",
    "        if table_type == 'labs' and unit:\n",
    "            unit_for_filename = unit[0] if isinstance(unit, list) else unit\n",
    "            unit_safe = sanitize_unit_for_filename(unit_for_filename)\n",
    "            filename = f'{category}_{unit_safe}.parquet'\n",
    "        else:\n",
    "            filename = f'{category}.parquet'\n",
    "\n",
    "        ecdf_dir = os.path.join(output_dir, 'ecdf', table_type)\n",
    "        os.makedirs(ecdf_dir, exist_ok=True)\n",
    "        ecdf_df.to_parquet(os.path.join(ecdf_dir, filename), index=False)\n",
    "\n",
    "        bins_dir = os.path.join(output_dir, 'bins', table_type)\n",
    "        os.makedirs(bins_dir, exist_ok=True)\n",
    "        bins_df.to_parquet(os.path.join(bins_dir, filename), index=False)\n",
    "\n",
    "        print(f\"  Saved to {output_dir}\")\n",
    "\n",
    "    stats = {\n",
    "        'category': category,\n",
    "        'unit': unit if table_type == 'labs' else None,\n",
    "        'original_count': original_count,\n",
    "        'clean_count': clean_count,\n",
    "        'ecdf_distinct_pairs': len(ecdf_df),\n",
    "        'num_bins': len(bins)\n",
    "    }\n",
    "\n",
    "    return stats, ecdf_df, bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Process a Single Lab Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_config = lab_vital_config.get('labs', {})\n",
    "labs_outlier = outlier_config['tables']['labs']['lab_value_numeric']\n",
    "\n",
    "print(\"Available lab categories in config:\")\n",
    "for cat in sorted(labs_config.keys()):\n",
    "    if isinstance(labs_config.get(cat), dict):\n",
    "        in_outlier = cat in labs_outlier\n",
    "        print(f\"  {cat}: outlier config = {in_outlier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LAB_CATEGORY = 'albumin'\n",
    "\n",
    "if TEST_LAB_CATEGORY in labs_config and TEST_LAB_CATEGORY in labs_outlier:\n",
    "    cat_config = labs_config[TEST_LAB_CATEGORY]\n",
    "    if cat_config and isinstance(cat_config, dict):\n",
    "        test_unit = cat_config.get('reference_unit')\n",
    "        if isinstance(test_unit, str):\n",
    "            test_unit = test_unit.lower()\n",
    "        elif isinstance(test_unit, list):\n",
    "            test_unit = [u.lower() if isinstance(u, str) else u for u in test_unit]\n",
    "        \n",
    "        stats, ecdf_df, bins_df = process_category_pandas(\n",
    "            table_type='labs',\n",
    "            category=TEST_LAB_CATEGORY,\n",
    "            unit=test_unit,\n",
    "            icu_windows=icu_windows,\n",
    "            tables_path=clif_config['tables_path'],\n",
    "            file_type=clif_config['file_type'],\n",
    "            outlier_range=labs_outlier[TEST_LAB_CATEGORY],\n",
    "            cat_config=cat_config,\n",
    "            extreme_bins_count=5,\n",
    "            save_output=False\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nStats: {stats}\")\n",
    "else:\n",
    "    print(f\"Category '{TEST_LAB_CATEGORY}' not found in config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ECDF (first 20 rows):\")\n",
    "ecdf_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bins:\")\n",
    "bins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Process a Single Vital Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_config = lab_vital_config.get('vitals', {})\n",
    "vitals_outlier = outlier_config['tables']['vitals']['vital_value']\n",
    "\n",
    "TEST_VITAL_CATEGORY = 'heart_rate'\n",
    "\n",
    "if TEST_VITAL_CATEGORY in vitals_config and TEST_VITAL_CATEGORY in vitals_outlier:\n",
    "    extreme_bins = 5 if TEST_VITAL_CATEGORY in ['height_cm', 'weight_kg'] else 10\n",
    "    \n",
    "    vital_stats, vital_ecdf_df, vital_bins_df = process_category_pandas(\n",
    "        table_type='vitals',\n",
    "        category=TEST_VITAL_CATEGORY,\n",
    "        unit=None,\n",
    "        icu_windows=icu_windows,\n",
    "        tables_path=clif_config['tables_path'],\n",
    "        file_type=clif_config['file_type'],\n",
    "        outlier_range=vitals_outlier[TEST_VITAL_CATEGORY],\n",
    "        cat_config=vitals_config[TEST_VITAL_CATEGORY],\n",
    "        extreme_bins_count=extreme_bins,\n",
    "        save_output=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStats: {vital_stats}\")\n",
    "else:\n",
    "    print(f\"Category '{TEST_VITAL_CATEGORY}' not found in config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize ECDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ecdf(ecdf_df: pd.DataFrame, title: str = \"ECDF\"):\n",
    "    \"\"\"Plot ECDF from a Pandas DataFrame.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    values = ecdf_df['value'].values\n",
    "    probs = ecdf_df['probability'].values\n",
    "    \n",
    "    ax.step(values, probs, where='post', linewidth=1.5)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Cumulative Probability')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 'ecdf_df' in dir() and len(ecdf_df) > 0:\n",
    "    plot_ecdf(ecdf_df, f\"ECDF: {TEST_LAB_CATEGORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = str(PROJECT_ROOT / 'output' / 'final')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Process all labs from config\n",
    "for category, cat_config in labs_config.items():\n",
    "    if cat_config is None:\n",
    "        print(f\"WARNING: Category '{category}' has None config, skipping\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(cat_config, dict):\n",
    "        continue\n",
    "\n",
    "    if category not in labs_outlier:\n",
    "        print(f\"WARNING: Category '{category}' not in outlier config, skipping\")\n",
    "        continue\n",
    "\n",
    "    config_unit = cat_config.get('reference_unit')\n",
    "    if config_unit is None:\n",
    "        print(f\"WARNING: Category '{category}' has no reference_unit in config, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Lowercase to match standardized data\n",
    "    if isinstance(config_unit, str):\n",
    "        unit = config_unit.lower()\n",
    "    elif isinstance(config_unit, list):\n",
    "        unit = [u.lower() if isinstance(u, str) else u for u in config_unit]\n",
    "    else:\n",
    "        unit = config_unit\n",
    "\n",
    "    stats, ecdf_df, bins_df = process_category_pandas(\n",
    "        table_type='labs',\n",
    "        category=category,\n",
    "        unit=unit,\n",
    "        icu_windows=icu_windows,\n",
    "        tables_path=clif_config['tables_path'],\n",
    "        file_type=clif_config['file_type'],\n",
    "        outlier_range=labs_outlier[category],\n",
    "        cat_config=cat_config,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        extreme_bins_count=5,\n",
    "        save_output=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
