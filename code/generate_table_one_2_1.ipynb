{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cc294e",
   "metadata": {},
   "source": [
    "## CLIF Table One\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "Date v1: May 13, 2025\n",
    "\n",
    "This script identifies the cohort of encounters with at least one ICU stay and then summarizes the cohort data into one table. \n",
    "\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`, `clif_patient_assessments`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **clif_patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **clif_hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`,`discharge_dttm`, `age_at_admission` | - |\n",
    "| **clif_adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category` | - |\n",
    "| **clif_vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | weight_kg |\n",
    "| **clif_labs** | `hospitalization_id`, `lab_result_dttm`, `lab_order_dttm`, `lab_category`, `lab_value_numeric` | creatinine, bilirubin_total, po2_arterial, platelet_count |\n",
    "| **clif_medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin(optional) |\n",
    "| **clif_respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`,  `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs`, `tidal_volume_set`, `pressure_control_set`, `pressure_support_set` | - |\n",
    "| **clif_patient_assessments** | `hospitalization_id`, `recorded_dttm` , `assessment_category`, `numerical_value`| `gcs_total` |\n",
    "| **clif_crrt_therapy** | `hospitalization_id`, `recorded_dttm` | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bf46d",
   "metadata": {},
   "source": [
    "# Cohort Identification\n",
    "\n",
    "\n",
    "## Inclusion \n",
    "1. Adults\n",
    "2. Patients with at least one ICU stay or those who had only emergency department or ward encounters and either died or received life support at any point. Life support is defined as the administration of any vasoactive drugs or respiratory support exceeding low-flow oxygen.\n",
    "\n",
    "Respiratory support device: 'IMV', 'NIPPV', 'CPAP', 'High Flow NC'  \n",
    "\n",
    "Vasoactive: 'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d51b416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Verification ===\n",
      "Python executable: /Users/kavenchhikara/Desktop/CLIF/CLIF-TableOne/.clif_table_one/bin/python\n",
      "Python version: 3.9.6 (default, Apr 30 2025, 02:07:17) \n",
      "[Clang 17.0.0 (clang-1700.0.13.5)]\n",
      "clifpy version: 0.2.0\n",
      "clifpy location: /Users/kavenchhikara/Desktop/CLIF/CLIF-TableOne/.clif_table_one/lib/python3.9/site-packages/clifpy/__init__.py\n",
      "\n",
      "=== Python Path Check ===\n",
      "✅ Clean environment - no local CLIFpy in path\n",
      "\n",
      "=== Working Directory ===\n",
      "Current directory: /Users/kavenchhikara/Desktop/CLIF/CLIF-TableOne/code\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import clifpy\n",
    "import os\n",
    "\n",
    "print(\"=== Environment Verification ===\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"clifpy version: {clifpy.__version__}\")\n",
    "print(f\"clifpy location: {clifpy.__file__}\")\n",
    "\n",
    "print(\"\\n=== Python Path Check ===\")\n",
    "local_clifpy_path = \"/Users/kavenchhikara/Desktop/CLIF/CLIFpy\"\n",
    "if any(local_clifpy_path in path for path in sys.path):\n",
    "    print(\"⚠️  WARNING: Local CLIFpy still in path!\")\n",
    "    for path in sys.path:\n",
    "        if local_clifpy_path in path:\n",
    "            print(f\"   Found: {path}\")\n",
    "else:\n",
    "    print(\"✅ Clean environment - no local CLIFpy in path\")\n",
    "\n",
    "print(f\"\\n=== Working Directory ===\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb9a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=� Configuration:\n",
      "   Data directory: /Users/kavenchhikara/Library/CloudStorage/Box-Box/RCLIF_data/CLIF_2018_24/WIP_2_1\n",
      "   File type: parquet\n",
      "   Timezone: US/Central\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = \"../config/config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "## import outlier json\n",
    "# with open('../config/outlier_config.json', 'r', encoding='utf-8') as f:\n",
    "#     outlier_cfg = json.load(f)\n",
    "\n",
    "print(f\"\\n=� Configuration:\")\n",
    "print(f\"   Data directory: {config['tables_path']}\")\n",
    "print(f\"   File type: {config['file_type']}\")\n",
    "print(f\"   Timezone: {config['timezone']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127edcbe",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4319503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Defining Required Data Elements\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Defining Required Data Elements\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Full patient table \n",
    "\n",
    "# Full hospitalization table \n",
    "\n",
    "# Full ADT table\n",
    "\n",
    "# Vitals\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "#Labs\n",
    "# labs_required_columns = [\n",
    "#     'hospitalization_id',\n",
    "#     'lab_result_dttm',\n",
    "#     'lab_category',\n",
    "#     'lab_value',\n",
    "#     'lab_value_numeric'\n",
    "# ]\n",
    "# labs_of_interest = ['po2_arterial','pco2_arterial', 'ph_arterial','ph_venous', 'bicarbonate','so2_arterial',\n",
    "#                     'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', \n",
    "#                     'bun', 'glucose_serum', 'lactate', 'hemoglobin' ]\n",
    "\n",
    "# # Continuous administered meds\n",
    "# meds_required_columns = [\n",
    "#     'hospitalization_id',\n",
    "#     'admin_dttm',\n",
    "#     'med_name',\n",
    "#     'med_category',\n",
    "#     'med_dose',\n",
    "#     'med_dose_unit'\n",
    "# ]\n",
    "# meds_of_interest = [\n",
    "#     'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "#     'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol',\n",
    "#     'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', \n",
    "#     'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium'\n",
    "# ]\n",
    "\n",
    "# # Respiratory Support \n",
    "# rst_required_columns = [\n",
    "#     'hospitalization_id',\n",
    "#     'recorded_dttm',\n",
    "#     'device_name',\n",
    "#     'device_category',\n",
    "#     'mode_name', \n",
    "#     'mode_category',\n",
    "#     'tracheostomy',\n",
    "#     'fio2_set',\n",
    "#     'lpm_set',\n",
    "#     'resp_rate_set',\n",
    "#     'peep_set',\n",
    "#     'resp_rate_obs',\n",
    "#     'tidal_volume_set', \n",
    "#     'pressure_control_set',\n",
    "#     'pressure_support_set',\n",
    "#     'peak_inspiratory_pressure_set',\n",
    "#     'peak_inspiratory_pressure_obs',\n",
    "#     'plateau_pressure_obs',\n",
    "#     'minute_vent_obs'\n",
    "# ]\n",
    "\n",
    "# # Full crrt table\n",
    "# crrt_required_columns = [\n",
    "#     'hospitalization_id',\n",
    "#     'recorded_dttm',\n",
    "#     'crrt_mode_category',\n",
    "#     'blood_flow_rate',\n",
    "#     'pre_filter_replacement_fluid_rate',\n",
    "#     'post_filter_replacement_fluid_rate',\n",
    "#     'dialysate_flow_rate',\n",
    "#     'ultrafiltration_out'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee461357",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7cbe42",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_hourly_sequence(group):\n",
    "#     blk = group.name  # use group name from groupby\n",
    "#     start_time = group['vent_episode_start_dttm'].iloc[0]\n",
    "#     end_time   = group['vent_end_dttm_72h'].iloc[0]\n",
    "#     hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "#     return pd.DataFrame({\n",
    "#         'hospitalization_id': blk,\n",
    "#         'recorded_dttm': hourly_timestamps\n",
    "#     })\n",
    "\n",
    "# def calculate_ibw(height_cm, sex):\n",
    "#     if pd.isna(height_cm) or pd.isna(sex):\n",
    "#         return np.nan\n",
    "#     height_inches = height_cm / 2.54\n",
    "#     sex = str(sex).lower()\n",
    "#     if sex == 'male':\n",
    "#         return 50 + 2.3 * (height_inches - 60)\n",
    "#     elif sex == 'female':\n",
    "#         return 45.5 + 2.3 * (height_inches - 60)\n",
    "#     else:\n",
    "#         return np.nan\n",
    "\n",
    "# def calculate_base_excess(ph, hco3):\n",
    "#     \"\"\"\n",
    "#     Calculate Base Excess using simplified formula\n",
    "#     BE = (HCO3 - 24.4) + (8.3 * (pH - 7.4))\n",
    "#     \"\"\"\n",
    "#     return (hco3 - 24.4) + (8.3 * (ph - 7.4))\n",
    "\n",
    "# def calculate_pf_ratio(po2, fio2):\n",
    "#     \"\"\"\n",
    "#     Vectorized calculation of P/F ratio (PaO2/FiO2)\n",
    "#     FiO2 should be as fraction (0.21-1.0), not percentage\n",
    "#     Handles pandas Series input.\n",
    "#     \"\"\"\n",
    "#     fio2 = fio2.copy()\n",
    "#     # Convert percentage to fraction if needed\n",
    "#     mask_pct = fio2 > 1\n",
    "#     fio2[mask_pct] = fio2[mask_pct] / 100\n",
    "#     # Set minimum fio2 to 0.21 (room air)\n",
    "#     fio2 = fio2.clip(lower=0.21)\n",
    "#     return po2 / fio2\n",
    "\n",
    "# def process_crrt_waterfall(\n",
    "#     crrt: pd.DataFrame,\n",
    "#     *,\n",
    "#     id_col: str = \"hospitalization_id\",\n",
    "#     gap_thresh: Union[str, pd.Timedelta] = \"2h\",\n",
    "#     infer_modes: bool = True,          # infer missing mode from numeric pattern\n",
    "#     flag_missing_bfr: bool = True,     # add QC flag if blood-flow still NaN\n",
    "#     wipe_unused: bool = True,          # null parameters not used by the mode\n",
    "#     fix_islands: bool = True,          # relabel single-row SCUF islands\n",
    "#     verbose: bool = True,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Clean + episode-aware forward-fill for the CLIF `crrt_therapy` table.\n",
    "#     Episode-aware clean-up and forward-fill of the CLIF `crrt_therapy` table.\n",
    "\n",
    "#     The function mirrors the respiratory-support “waterfall” logic but adapts it to\n",
    "#     the quirks of Continuous Renal Replacement Therapy (CRRT):\n",
    "\n",
    "#     1. **Episode detection** - a new `crrt_episode_id` starts whenever  \n",
    "#        • `crrt_mode_category` changes **OR**  \n",
    "#        • the gap between successive rows exceeds *gap_thresh* (default 2 h).\n",
    "#     2. **Numeric forward-fill inside an episode** - fills *only* the parameters\n",
    "#        that are clinically relevant for the active mode.\n",
    "#     3. **Mode-specific wiping** after filling, parameters that are **not used**\n",
    "#        in the current mode (e.g. `dialysate_flow_rate` in SCUF) are nulled so\n",
    "#        stale data never bleed across modes.\n",
    "#     4. **Deduplication & ordering** guarantees exactly **one row per\n",
    "#        `(id_col, recorded_dttm)`**, chronologically ordered.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     crrt : pd.DataFrame\n",
    "#         Raw `crrt_therapy` table **in UTC**. Must contain the schema columns\n",
    "#         defined on the CLIF website (see docstring footer).\n",
    "#     id_col : str, default ``\"hospitalization_id\"``\n",
    "#         Encounter-level identifier.\n",
    "#     gap_thresh : str or pd.Timedelta, default ``\"2h\"``\n",
    "#         Maximum tolerated gap **inside** an episode before a new episode is\n",
    "#         forced. Accepts any pandas-parsable offset string (``\"90min\"``, ``\"3h\"``,\n",
    "#         …) or a ``pd.Timedelta``.\n",
    "#     verbose : bool, default ``True``\n",
    "#         If *True* prints progress banners.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         Processed CRRT DataFrame with\n",
    "\n",
    "#         * ``crrt_episode_id`` (int32) - sequential per encounter,\n",
    "#         * forward-filled numeric parameters **within** each episode,\n",
    "#         * unused parameters blanked per mode,\n",
    "#         * unique, ordered rows ``id_col, recorded_dttm``.\n",
    "\n",
    "#     Add-ons v2.0\n",
    "#     ------------\n",
    "#     • Optional numeric-pattern inference of `crrt_mode_category`.\n",
    "#     • Flags rows that *should* have blood-flow but don't.\n",
    "#     • Optional fix for single-row modality islands (sandwiched rows).\n",
    "#     • Optional wipe vs. keep of parameters not used by the active mode.\n",
    "\n",
    "#     Key steps\n",
    "#     ----------\n",
    "#     0.  Lower-case strings, coerce numerics, **infer** mode when blank.\n",
    "#     1.  **Relabel single-row SCUF islands** (if *fix_islands*).\n",
    "#     2.  Detect `crrt_episode_id` (mode change or >gap_thresh).\n",
    "#     3.  Forward-fill numeric parameters *within* an episode.\n",
    "#     4.  QC flag → `blood_flow_missing_after_ffill` (optional).\n",
    "#     5.  Wipe / flag parameters not valid for the mode (configurable).\n",
    "#     6.  Deduplicate & order ⇒ one row per ``(id_col, recorded_dttm)``.\n",
    "#     \"\"\"\n",
    "#     p = print if verbose else (lambda *_, **__: None)\n",
    "#     gap_thresh = pd.Timedelta(gap_thresh)\n",
    "\n",
    "#     # ───────────── Phase 0 — prep, numeric coercion, optional inference\n",
    "#     p(\"✦ Phase 0: prep & numeric coercion (+optional mode inference)\")\n",
    "#     df = crrt.copy()\n",
    "\n",
    "#     df[\"crrt_mode_category\"] = df[\"crrt_mode_category\"].str.lower()\n",
    "#     # save original dialysate_flow_rate values\n",
    "#     df[\"_orig_df\"] = df[\"dialysate_flow_rate\"]\n",
    "\n",
    "#     # 0a) RAW SCUF DF‐OUT sanity check\n",
    "#     # look for rows that are already labeled “scuf”\n",
    "#     # and that have a non‐zero dialysate_flow_rate in the raw data\n",
    "#     raw_scuf = df[\"crrt_mode_category\"].str.lower() == \"scuf\"\n",
    "#     raw_df_positive = df[\"_orig_df\"].fillna(0) > 0\n",
    "\n",
    "#     n_bad = (raw_scuf & raw_df_positive).sum()\n",
    "#     if n_bad:\n",
    "#         print(f\"!!!  Found {n_bad} raw SCUF rows with dialysate_flow_rate > 0 (should be 0 or NA)\")\n",
    "#         print(\" Converting these mode category to NA, keep recorded numerical values as the ground truth\")\n",
    "#         df.loc[raw_df_positive, \"crrt_mode_category\"] = np.nan\n",
    "#     else:\n",
    "#         print(\"!!! No raw SCUF rows had dialysate_flow_rate > 0\")\n",
    "\n",
    "#     NUM_COLS = [\n",
    "#         \"blood_flow_rate\",\n",
    "#         \"pre_filter_replacement_fluid_rate\",\n",
    "#         \"post_filter_replacement_fluid_rate\",\n",
    "#         \"dialysate_flow_rate\",\n",
    "#         \"ultrafiltration_out\",\n",
    "#     ]\n",
    "#     NUM_COLS = [c for c in NUM_COLS if c in df.columns]\n",
    "#     df[NUM_COLS] = df[NUM_COLS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "#     #  any row whose original ultrafiltration_out was >0 must never be SCUF\n",
    "#     def drop_scuf_on_positive_df(df, p):\n",
    "#         bad_df  = df[\"_orig_df\"].fillna(0) > 0\n",
    "#         scuf_now = df[\"crrt_mode_category\"] == \"scuf\"\n",
    "#         n = (bad_df & scuf_now).sum()\n",
    "#         if n:\n",
    "#             p(f\"→ Removing {n:,} SCUF labels on rows with DF>0\")\n",
    "#             df.loc[bad_df & scuf_now, \"crrt_mode_category\"] = np.nan\n",
    "            \n",
    "\n",
    "#     if infer_modes:\n",
    "#         miss = df[\"crrt_mode_category\"].isna()\n",
    "#         pre  = df[\"pre_filter_replacement_fluid_rate\"].notna()\n",
    "#         post = df[\"post_filter_replacement_fluid_rate\"].notna()\n",
    "#         dial = df[\"dialysate_flow_rate\"].notna()\n",
    "#         bf   = df[\"blood_flow_rate\"].notna()\n",
    "#         uf   = df[\"ultrafiltration_out\"].notna()\n",
    "#         all_num_present = df[NUM_COLS].notna().all(axis=1)\n",
    "\n",
    "#         df.loc[miss & all_num_present,                       \"crrt_mode_category\"] = \"cvvhdf\"\n",
    "#         df.loc[miss & (~dial) & pre & post,                  \"crrt_mode_category\"] = \"cvvh\"\n",
    "#         df.loc[miss & dial & (~pre) & (~post),               \"crrt_mode_category\"] = \"cvvhd\"\n",
    "#         df.loc[miss & (~dial) & (~pre) & (~post) & bf & uf,  \"crrt_mode_category\"] = \"scuf\"\n",
    "\n",
    "#         filled = (miss & df[\"crrt_mode_category\"].notna()).sum()\n",
    "#         p(f\"  • numeric-pattern inference filled {filled:,} missing modes\")\n",
    "#         drop_scuf_on_positive_df(df, p)\n",
    "\n",
    "#     # ───────────── Phase 1 — sort and *fix islands before episodes*\n",
    "#     p(\"✦ Phase 1: sort + SCUF-island fix\")\n",
    "#     df = df.sort_values([id_col, \"recorded_dttm\"]).reset_index(drop=True)\n",
    "\n",
    "#     if fix_islands:\n",
    "#         # after sorting, BEFORE episode detection\n",
    "#         prev_mode = df.groupby(id_col)[\"crrt_mode_category\"].shift()\n",
    "#         next_mode = df.groupby(id_col)[\"crrt_mode_category\"].shift(-1)\n",
    "\n",
    "#         scuf_island = (\n",
    "#             (df[\"crrt_mode_category\"] == \"scuf\") &\n",
    "#             (prev_mode.notna()) & (next_mode.notna()) &     # ensure we have neighbours\n",
    "#             (prev_mode == next_mode)                        # both neighbours agree\n",
    "#         )\n",
    "\n",
    "#         df.loc[scuf_island, \"crrt_mode_category\"] = prev_mode[scuf_island]\n",
    "#         n_fixed = scuf_island.sum()\n",
    "#         p(f\"  • relabelled {n_fixed:,} SCUF-island rows\")\n",
    "#         drop_scuf_on_positive_df(df, p)\n",
    "\n",
    "\n",
    "#     # ───────────── Phase 2 — episode detection (now with fixed modes)\n",
    "#     p(\"✦ Phase 2: derive `crrt_episode_id`\")\n",
    "#     mode_change = (\n",
    "#         df.groupby(id_col)[\"crrt_mode_category\"]\n",
    "#           .apply(lambda s: s != s.shift())\n",
    "#           .reset_index(level=0, drop=True)\n",
    "#     )\n",
    "#     time_gap = df.groupby(id_col)[\"recorded_dttm\"].diff().gt(gap_thresh).fillna(False)\n",
    "#     df[\"crrt_episode_id\"] = ((mode_change | time_gap)\n",
    "#                               .groupby(df[id_col]).cumsum()\n",
    "#                               .astype(\"int32\"))\n",
    "\n",
    "#     # ───────────── Phase 3 — forward-fill numerics inside episodes\n",
    "#     p(\"✦ Phase 3: forward-fill numeric vars inside episodes\")\n",
    "#     tqdm.pandas(disable=not verbose, desc=\"ffill per episode\")\n",
    "#     df[NUM_COLS] = (\n",
    "#         df.groupby([id_col, \"crrt_episode_id\"], sort=False, group_keys=False)[NUM_COLS]\n",
    "#           .progress_apply(lambda g: g.ffill())\n",
    "#     )\n",
    "\n",
    "#     # QC: blood-flow still missing?\n",
    "#     if flag_missing_bfr and \"blood_flow_rate\" in NUM_COLS:\n",
    "#         need_bfr = df[\"crrt_mode_category\"].isin([\"scuf\", \"cvvh\", \"cvvhd\", \"cvvhdf\"])\n",
    "#         df[\"blood_flow_missing_after_ffill\"] = need_bfr & df[\"blood_flow_rate\"].isna()\n",
    "#         p(f\"  • blood-flow still missing where required: \"\n",
    "#           f\"{df['blood_flow_missing_after_ffill'].mean():.1%}\")\n",
    "        \n",
    "#     # Bridge tiny episodes\n",
    "    \n",
    "#     single_row_ep = (\n",
    "#         df.groupby([id_col, \"crrt_episode_id\"]).size() == 1\n",
    "#     ).reset_index(name=\"n\").query(\"n == 1\")\n",
    "#     print(\"Bridging single row episodes\")\n",
    "\n",
    "#     rows_to_bridge = df.merge(single_row_ep[[id_col, \"crrt_episode_id\"]],\n",
    "#                             on=[id_col, \"crrt_episode_id\"]).index\n",
    "    \n",
    "#     CAT_COLS = [c for c in [\"crrt_mode_category\"] if c in df.columns]\n",
    "\n",
    "#     # Combine with the numeric columns we already had\n",
    "#     BRIDGE_COLS = NUM_COLS + CAT_COLS\n",
    "\n",
    "#     # Forward-fill (and back-fill just in case the island is the first row of the encounter)\n",
    "#     df.loc[rows_to_bridge, BRIDGE_COLS] = (\n",
    "#         df.loc[rows_to_bridge, BRIDGE_COLS]\n",
    "#         .groupby(df.loc[rows_to_bridge, id_col])      # keep encounter boundaries\n",
    "#         .apply(lambda g: g.ffill())          \n",
    "#         .reset_index(level=0, drop=True)\n",
    "#     )\n",
    "#     drop_scuf_on_positive_df(df, p)\n",
    "#     # ───────────── Phase 4 — wipe / flag unused parameters\n",
    "#     p(\"✦ Phase 4: handle parameters not valid for the mode\")\n",
    "#     MODE_PARAM_MAP = {\n",
    "#         \"scuf\":   {\"blood_flow_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvh\":   {\"blood_flow_rate\", \"pre_filter_replacement_fluid_rate\",\n",
    "#                    \"post_filter_replacement_fluid_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvhd\":  {\"blood_flow_rate\", \"dialysate_flow_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvhdf\": {\"blood_flow_rate\", \"pre_filter_replacement_fluid_rate\",\"post_filter_replacement_fluid_rate\",\n",
    "#                    \"dialysate_flow_rate\", \"ultrafiltration_out\"},\n",
    "#     }\n",
    "\n",
    "#     wiped_totals = {c: 0 for c in NUM_COLS}\n",
    "#     for mode, keep in MODE_PARAM_MAP.items():\n",
    "#         mask = df[\"crrt_mode_category\"] == mode\n",
    "#         drop_cols = list(set(NUM_COLS) - keep)\n",
    "#         if wipe_unused:\n",
    "#             for col in drop_cols:\n",
    "#                 wiped_totals[col] += df.loc[mask, col].notna().sum()\n",
    "#             df.loc[mask, drop_cols] = np.nan\n",
    "#         else:\n",
    "#             for col in drop_cols:\n",
    "#                 df.loc[mask & df[col].notna(), f\"{col}_unexpected\"] = True\n",
    "\n",
    "#     if verbose and wipe_unused:\n",
    "#         p(\"  • cells set → NA by wipe:\")\n",
    "#         for col, n in wiped_totals.items():\n",
    "#             p(f\"    {col:<35} {n:>8,}\")\n",
    "#     # ───────────── Phase 4a — SCUF‐specific sanity check\n",
    "#     if \"dialysate_flow_rate\" in df.columns:\n",
    "#         # only consider rows that were originally SCUF mode\n",
    "#         # and whose original _orig_df was non‐zero/non‐NA\n",
    "#         scuf_rows = df[\"crrt_mode_category\"] == \"scuf\"\n",
    "#         orig_bad = df[\"_orig_df\"].fillna(0) > 0\n",
    "\n",
    "#         # these are rows where the *original* data had UF>0 despite SCUF\n",
    "#         bad_orig_scuf = scuf_rows & orig_bad\n",
    "\n",
    "#         n_bad_orig = bad_orig_scuf.sum()\n",
    "#         if n_bad_orig:\n",
    "#             p(f\"!!! {n_bad_orig} rows originally labeled SCUF had DF>0 (raw data); forcing DF→NA for those\")\n",
    "#             df.loc[bad_orig_scuf, \"dialysate_flow_rate\"] = np.nan\n",
    "#         else:\n",
    "#             p(\"!!! No SCUF rows with DF>0\")\n",
    "\n",
    "#     # then drop the helper column\n",
    "#     df = df.drop(columns=\"_orig_df\")\n",
    "\n",
    "#     # ───────────── Phase 5 — deduplicate & order\n",
    "#     p(\"✦ Phase 5: deduplicate & order\")\n",
    "#     pre = len(df)\n",
    "#     df = (\n",
    "#         df.drop_duplicates(subset=[id_col, \"recorded_dttm\"])\n",
    "#           .sort_values([id_col, \"recorded_dttm\"])\n",
    "#           .reset_index(drop=True)\n",
    "#     )\n",
    "#     p(f\"  • dropped {pre - len(df):,} duplicate rows\")\n",
    "\n",
    "#     if verbose:\n",
    "#         sparse = df[NUM_COLS].isna().all(axis=1).mean()\n",
    "#         p(f\"  • rows with all NUM_COLS missing: {sparse:.1%}\")\n",
    "\n",
    "#     p(\"[OK] CRRT waterfall complete.\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffe16d",
   "metadata": {},
   "source": [
    "## Cohort identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c726691f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading CLIF Tables\n",
      "================================================================================\n",
      "ClifOrchestrator initialized.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Loading CLIF Tables\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from clifpy.clif_orchestrator import ClifOrchestrator\n",
    "\n",
    "# Initialize ClifOrchestrator\n",
    "clif = ClifOrchestrator(\n",
    "    data_directory=config['tables_path'],\n",
    "    filetype=config['file_type'],\n",
    "    timezone=config['timezone']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553499b",
   "metadata": {},
   "source": [
    "## Step0: Load Core Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2dc9ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Step 0: Load Core Tables (Patient, Hospitalization, ADT)\n",
      "================================================================================\n",
      "\n",
      "Loading 3 core tables...\n",
      "   Loading patient... ✓ (97,254 rows)\n",
      "   Loading hospitalization... ✓ (166,814 rows)\n",
      "   Loading adt... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "in_dttm: Naive datetime localized to US/Central. Please verify this is correct.\n",
      "out_dttm: Naive datetime localized to US/Central. Please verify this is correct.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ (425,353 rows)\n",
      "\n",
      "Core tables loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 0: Load Core Tables (Patient, Hospitalization, ADT)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 0: Load Core Tables (Patient, Hospitalization, ADT)\")\n",
    "print(\"=\" * 80)\n",
    "core_tables = ['patient', 'hospitalization', 'adt']\n",
    "\n",
    "print(f\"\\nLoading {len(core_tables)} core tables...\")\n",
    "for table_name in core_tables:\n",
    "    print(f\"   Loading {table_name}...\", end=\" \")\n",
    "    try:\n",
    "        clif.load_table(table_name)\n",
    "        table = getattr(clif, table_name)\n",
    "        print(f\"✓ ({len(table.df):,} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nCore tables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36bd52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_df = clif.hospitalization.df\n",
    "adt_df = clif.adt.df\n",
    "\n",
    "# Merge to get age information\n",
    "all_encounters = pd.merge(\n",
    "    hosp_df[[\"patient_id\", \"hospitalization_id\", \"admission_dttm\", \"discharge_dttm\", \n",
    "             \"age_at_admission\", \"discharge_category\"]],\n",
    "    adt_df[[\"hospitalization_id\", \"hospital_id\", \"in_dttm\", \"out_dttm\", \n",
    "            \"location_category\", \"location_type\"]],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683b68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates by ['hospitalization_id', 'in_dttm', 'out_dttm']\n",
    "dup_counts = all_encounters.duplicated(subset=['hospitalization_id', 'in_dttm', 'out_dttm']).sum()\n",
    "if dup_counts > 0:\n",
    "    print(f\"Warning: {dup_counts} duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")\n",
    "else:\n",
    "    print(\"No duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5f8fc",
   "metadata": {},
   "source": [
    "## Step1: Date & Age filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d378297e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
       "       'age_at_admission', 'discharge_category', 'hospital_id', 'in_dttm',\n",
       "       'out_dttm', 'location_category', 'location_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_encounters.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc74b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Step 1: Identifying Adult Patients (Age >= 18) and Admissions 2018-2024\n",
      "================================================================================\n",
      "Applying initial cohort filters...\n",
      "\n",
      "Filtering Results:\n",
      "   Total hospitalizations: 166,781\n",
      "   Adult hospitalizations (age >= 18, 2018-2024): 166,781\n",
      "   Excluded (age < 18 or outside 2018-2024): 0\n",
      "\n",
      "   Unique adult hospitalization IDs: 166,781\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Identify Adult Patients (Age >= 18) and Admissions 2018-2024\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 1: Identifying Adult Patients (Age >= 18) and Admissions 2018-2024\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Applying initial cohort filters...\")\n",
    "\n",
    "# Use only the relevant columns from all_encounters\n",
    "adult_encounters = all_encounters[\n",
    "    [\n",
    "        'patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
    "        'age_at_admission', 'discharge_category', 'hospital_id',\n",
    "        'in_dttm', 'out_dttm', 'location_category', 'location_type'\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "if config['timezone'].lower() == \"mimic\":\n",
    "    # MIMIC: only age >= 18, no admit year restriction\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['age_at_admission'] >= 18) & (adult_encounters['age_at_admission'].notna())\n",
    "    ]\n",
    "else:\n",
    "    # Other sites: age >= 18 and admission between 2018-2024 inclusive\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['age_at_admission'] >= 18) &\n",
    "        (adult_encounters['age_at_admission'].notna()) &\n",
    "        (adult_encounters['admission_dttm'].dt.year >= 2018) &\n",
    "        (adult_encounters['admission_dttm'].dt.year <= 2024)\n",
    "    ]\n",
    "\n",
    "\n",
    "print(f\"\\nFiltering Results:\")\n",
    "print(f\"   Total hospitalizations: {len(all_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Adult hospitalizations (age >= 18, 2018-2024): {len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Excluded (age < 18 or outside 2018-2024): {len(all_encounters['hospitalization_id'].unique()) - len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "\n",
    "\n",
    "strobe_counts[\"0_total_hospitalizations\"] = len(all_encounters['hospitalization_id'].unique())\n",
    "strobe_counts[\"1_adult_hospitalizations\"] = len(adult_encounters['hospitalization_id'].unique())\n",
    "# Get list of adult hospitalization IDs for filtering\n",
    "adult_hosp_ids = set(adult_encounters['hospitalization_id'].unique())\n",
    "print(f\"\\n   Unique adult hospitalization IDs: {len(adult_hosp_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66edf5",
   "metadata": {},
   "source": [
    "### Stitch hospitalizations \n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_id`, then we combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block.\n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_joined_id` from the hospitalization table, then we consider the user has already stitched similar encounters, and we will consider that as the primary id column for all table joins moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de4651cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from clifpy.utils.stitching_encounters import stitch_encounters\n",
    "\n",
    "# Instead of multiple copies, work with references and clean up\n",
    "hosp_filtered = clif.hospitalization.df[clif.hospitalization.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "adt_filtered = clif.adt.df[clif.adt.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "\n",
    "hosp_stitched, adt_stitched, encounter_mapping = stitch_encounters(\n",
    "    hospitalization=hosp_filtered,\n",
    "    adt=adt_filtered,\n",
    "    time_interval=6  \n",
    ")\n",
    "\n",
    "# Direct assignment without additional copies\n",
    "clif.hospitalization.df = hosp_stitched\n",
    "clif.adt.df = adt_stitched\n",
    "\n",
    "# Store the encounter mapping in the orchestrator for later use\n",
    "clif.encounter_mapping = encounter_mapping\n",
    "\n",
    "# Clean up intermediate variables\n",
    "del hosp_filtered, adt_filtered\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb77b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encounter Stitching Results:\n",
      "   Number of unique hospitalizations before stitching: 166,781\n",
      "   Number of unique encounter blocks after stitching: 166,644\n",
      "   Number of linked hospitalization ids: 137\n",
      "\n",
      "Encounter Mapping Details:\n",
      "   Total encounter mappings created: 166,781\n",
      "   Encounter blocks with multiple hospitalizations: 136\n",
      "   Maximum hospitalizations combined into one block: 3\n"
     ]
    }
   ],
   "source": [
    "# After your stitching code, add these calculations:\n",
    "\n",
    "# Calculate stitching statistics\n",
    "strobe_counts['1b_before_stitching'] = len(adult_hosp_ids)  # Original adult hospitalizations\n",
    "strobe_counts['1b_after_stitching'] = len(hosp_stitched['encounter_block'].unique())  # Unique encounter blocks after stitching\n",
    "strobe_counts['1b_stitched_hosp_ids'] = strobe_counts['1b_before_stitching'] - strobe_counts['1b_after_stitching']  # Number of hospitalizations that were linked\n",
    "\n",
    "print(f\"\\nEncounter Stitching Results:\")\n",
    "print(f\"   Number of unique hospitalizations before stitching: {strobe_counts['1b_before_stitching']:,}\")\n",
    "print(f\"   Number of unique encounter blocks after stitching: {strobe_counts['1b_after_stitching']:,}\")\n",
    "print(f\"   Number of linked hospitalization ids: {strobe_counts['1b_stitched_hosp_ids']:,}\")\n",
    "\n",
    "# Optional: Show the encounter mapping details\n",
    "print(f\"\\nEncounter Mapping Details:\")\n",
    "print(f\"   Total encounter mappings created: {len(encounter_mapping):,}\")\n",
    "if len(encounter_mapping) > 0:\n",
    "    # Show some examples of how many original hospitalizations were combined\n",
    "    mapping_counts = encounter_mapping.groupby('encounter_block').size()\n",
    "    print(f\"   Encounter blocks with multiple hospitalizations: {(mapping_counts > 1).sum():,}\")\n",
    "    print(f\"   Maximum hospitalizations combined into one block: {mapping_counts.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68c040e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df = encounter_mapping.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806ca75",
   "metadata": {},
   "source": [
    "## Step2: Identify CRRT encounters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0d4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading crrt_therapy table...\n",
      "   CRRT therapy loaded: 403,322 rows\n",
      "   Unique CRRT therapy hospitalizations: 3152\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading crrt_therapy table...\")\n",
    "try:\n",
    "    \n",
    "    \n",
    "    print(f\"   CRRT therapy loaded: {len(clif.crrt_therapy.df):,} rows\")\n",
    "    print(f\"   Unique CRRT therapy hospitalizations: {clif.crrt_therapy.df['hospitalization_id'].nunique()}\")\n",
    "except Exception as e:\n",
    "    print(f\"   CRRT therapy not available or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e24aeae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CRRT therapy DataFrame:\n",
      "   Total CRRT records: 403,322\n",
      "   Records with encounter blocks: 403,322\n",
      "   Unique encounter blocks in CRRT data: 3152\n",
      "   Unique hospitalizations  in CRRT data: 3152\n"
     ]
    }
   ],
   "source": [
    "# Update CRRT therapy DataFrame with encounter blocks\n",
    "clif.crrt_therapy.df = clif.crrt_therapy.df.merge(\n",
    "    clif.encounter_mapping[['hospitalization_id', 'encounter_block']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "n_crrt_hosp = clif.crrt_therapy.df['hospitalization_id'].nunique()\n",
    "n_crrt_blocks = clif.crrt_therapy.df['encounter_block'].nunique()\n",
    "crrt_hosp_ids = set(clif.crrt_therapy.df['hospitalization_id'].unique())\n",
    "\n",
    "print(f\"Updated CRRT therapy DataFrame:\")\n",
    "print(f\"   Total CRRT records: {len(clif.crrt_therapy.df):,}\")\n",
    "print(f\"   Records with encounter blocks: {clif.crrt_therapy.df['encounter_block'].notna().sum():,}\")\n",
    "print(f\"   Unique encounter blocks in CRRT data: {n_crrt_blocks}\")\n",
    "print(f\"   Unique hospitalizations  in CRRT data: {n_crrt_hosp}\")\n",
    "\n",
    "strobe_counts[\"2_crrt_hospitalizations\"] = n_crrt_hosp\n",
    "strobe_counts[\"2_crrt_blocks\"] = n_crrt_blocks\n",
    "\n",
    "# Filter cohort_df to only hospitalizations present in CRRT data\n",
    "cohort_df = cohort_df[cohort_df['hospitalization_id'].isin(crrt_hosp_ids)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe7975",
   "metadata": {},
   "source": [
    "## Step3: Exclude ESRD encounters\n",
    "\n",
    "Prior to admission ICD codes for ESRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31b132e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Hospital dx table...\n",
      "   Hospital dx loaded: 108,880 rows\n",
      "   Unique Hospital dx hospitalizations: 3124\n",
      "Merge encounter blocks with diagnosis\n",
      "   Total Hospital dx records: 108,880\n",
      "   Records with encounter blocks: 108,880\n",
      "   Unique encounter blocks in Hospital dx data: 3124\n",
      "   Unique hospitalizations  in Hospital dx data: 3124\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading Hospital dx table...\")\n",
    "try:\n",
    "    clif.load_table(\n",
    "        'hospital_diagnosis',\n",
    "        filters={'hospitalization_id': list(crrt_hosp_ids)}\n",
    "    )\n",
    "    print(f\"   Hospital dx loaded: {len(clif.hospital_diagnosis.df):,} rows\")\n",
    "    print(f\"   Unique Hospital dx hospitalizations: {clif.hospital_diagnosis.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "    print(\"Merge encounter blocks with diagnosis\")\n",
    "    clif.hospital_diagnosis.df = clif.hospital_diagnosis.df.merge(\n",
    "                    clif.encounter_mapping[['hospitalization_id', 'encounter_block']],\n",
    "                    on='hospitalization_id',\n",
    "                    how='left')\n",
    "\n",
    "    n_dx_hosp = clif.hospital_diagnosis.df['hospitalization_id'].nunique()\n",
    "    n_dx_blocks = clif.hospital_diagnosis.df['encounter_block'].nunique()\n",
    "    cohort_hosp_ids = set(clif.hospital_diagnosis.df['hospitalization_id'].unique())\n",
    "    cohort_blocks = set(clif.hospital_diagnosis.df['encounter_block'].unique())\n",
    "    print(f\"   Total Hospital dx records: {len(clif.hospital_diagnosis.df):,}\")\n",
    "    print(f\"   Records with encounter blocks: {clif.hospital_diagnosis.df['encounter_block'].notna().sum():,}\")\n",
    "    print(f\"   Unique encounter blocks in Hospital dx data: {n_dx_blocks}\")\n",
    "    print(f\"   Unique hospitalizations  in Hospital dx data: {n_dx_hosp}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Hospital dx not available or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27acd845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hospital dx column names : Index(['hospitalization_id', 'diagnosis_code', 'diagnosis_code_format',\n",
      "       'diagnosis_primary', 'poa_present', 'encounter_block'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "hospital_diagnosis_df = clif.hospital_diagnosis.df.copy()\n",
    "\n",
    "print(\"Hospital dx column names :\", hospital_diagnosis_df.columns)\n",
    "# Clean and standardize diagnosis codes\n",
    "hospital_diagnosis_df['diagnosis_code'] = hospital_diagnosis_df['diagnosis_code'].str.replace('.', '').str.lower()\n",
    "\n",
    "if 'present_on_admission' in hospital_diagnosis_df.columns:\n",
    "    hospital_diagnosis_df = hospital_diagnosis_df.rename(columns={'present_on_admission': 'poa_present'})\n",
    "\n",
    "# Check present_on_admission column type and standardize to int8\n",
    "if 'poa_present' in hospital_diagnosis_df.columns:\n",
    "    # Only allow 1 (present on admission) or 0 (not present on admission)\n",
    "    # Any other value (including Exempt, Unknown, Unspecified, NA) is set to 0\n",
    "    hospital_diagnosis_df['poa_present'] = hospital_diagnosis_df['poa_present'].astype(str).str.lower()\n",
    "    hospital_diagnosis_df['poa_present'] = hospital_diagnosis_df['poa_present'].map(\n",
    "        {'yes': 1, 'y': 1, 'true': 1, '1': 1, 'no': 0, 'n': 0, 'false': 0, '0': 0}\n",
    "    ).fillna(0).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd9ae99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows matching ESRD codes: 1680\n",
      "Present_on_admission values for ESRD codes:\n",
      "poa_present\n",
      "1    1588\n",
      "0      92\n",
      "Name: count, dtype: int64\n",
      "Hospitalizations with ESRD (including NA present_on_admission): 905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0_total_hospitalizations': 166781,\n",
       " '1_adult_hospitalizations': 166781,\n",
       " '1b_before_stitching': 166781,\n",
       " '1b_after_stitching': 166644,\n",
       " '1b_stitched_hosp_ids': 137,\n",
       " '2_crrt_hospitalizations': 3152,\n",
       " '2_crrt_blocks': 3152,\n",
       " '3_hospitalizations_with_esrd': 905,\n",
       " '3_encounter_blocks_with_esrd': 905,\n",
       " '3_encounter_blocks_without_esrd': 2247,\n",
       " '3_hospitalizations_without_esrd': 2247}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define ESRD diagnosis codes\n",
    "# Let's debug why we're not finding ESRD codes\n",
    "esrd_codes = [\n",
    "    'z992',    # Dependence on renal dialysis\n",
    "    'z9115',   # Patient's noncompliance with renal dialysis\n",
    "    'i120',    # Hypertensive chronic kidney disease with stage 5 CKD or ESRD\n",
    "    'n186',    # End stage renal disease\n",
    "    'i132',    # Hypertensive heart and chronic kidney disease with heart failure and ESRD\n",
    "    'z992',    # Dependence on renal dialysis (alternate code)\n",
    "    'i120',    # Hypertensive chronic kidney disease with stage 5 CKD or ESRD (alternate code)\n",
    "    'z91158',  # Patient's noncompliance with renal dialysis (alternate code)\n",
    "    'i1311',   # Hypertensive heart and chronic kidney disease with heart failure and stage 5 CKD\n",
    "    'i132',    # Hypertensive heart and chronic kidney disease with ESRD (alternate code)\n",
    "    '5856',     #ICD9 :End stage renal disease\n",
    "    '40391',    #ICD9: Hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage V or end stage renal disease\n",
    "    '40311',     #ICD9: Hypertensive chronic kidney disease, benign, with chronic kidney disease stage V or end stage renal disease\n",
    "    'v4511',     #ICD9: Renal dialysis status\n",
    "    'v4512'     #ICD9: Noncompliance with renal dialysis\n",
    "]\n",
    "\n",
    "# Get hospitalization IDs with ESRD diagnoses and print debug info\n",
    "print(\"\\nNumber of rows matching ESRD codes:\", hospital_diagnosis_df['diagnosis_code'].isin(esrd_codes).sum())\n",
    "\n",
    "\n",
    "# Count how many ESRD codes have present_on_admission = 1, 0, or NA\n",
    "esrd_poa_counts = hospital_diagnosis_df[\n",
    "    hospital_diagnosis_df['diagnosis_code'].isin(esrd_codes)\n",
    "]['poa_present'].value_counts(dropna=False)\n",
    "print(\"Present_on_admission values for ESRD codes:\")\n",
    "print(esrd_poa_counts)\n",
    "\n",
    "# Use a more inclusive approach for ESRD identification\n",
    "# Include cases where present_on_admission is 1 OR NA (assuming NA means unknown/possible)\n",
    "esrd_mask = (\n",
    "    hospital_diagnosis_df['diagnosis_code'].isin(esrd_codes) & \n",
    "    ((hospital_diagnosis_df['poa_present'] == 1) | \n",
    "        (hospital_diagnosis_df['poa_present'].isna()))\n",
    ")\n",
    "hosp_ids_with_esrd = hospital_diagnosis_df[esrd_mask]['hospitalization_id'].unique()\n",
    "blocks_with_esrd = hospital_diagnosis_df[esrd_mask]['encounter_block'].unique()\n",
    "\n",
    "print(f\"Hospitalizations with ESRD (including NA present_on_admission): {len(hosp_ids_with_esrd)}\")\n",
    "\n",
    "\n",
    "strobe_counts['3_hospitalizations_with_esrd'] = len(hosp_ids_with_esrd)\n",
    "strobe_counts['3_encounter_blocks_with_esrd'] = len(blocks_with_esrd)\n",
    "\n",
    "\n",
    "# Filter out hospitalizations with ESRD\n",
    "cohort_df = cohort_df[~cohort_df['hospitalization_id'].isin(hosp_ids_with_esrd)].copy()\n",
    "cohort_hosp_ids = set(cohort_df['hospitalization_id'].unique())\n",
    "cohort_blocks = set(cohort_df['encounter_block'].unique())\n",
    "# Create cohort subset excluding hospitalizations with ESRD\n",
    "strobe_counts['3_encounter_blocks_without_esrd'] = len(cohort_blocks)  # Count blocks without ESRD\n",
    "strobe_counts['3_hospitalizations_without_esrd'] = len(cohort_hosp_ids)  # Count hospitalizations without ESRD\n",
    "\n",
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a78a5c",
   "metadata": {},
   "source": [
    "## Cohort Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bad0094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crrt_df = clif.crrt_therapy.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e9ec0",
   "metadata": {},
   "source": [
    "### 1.  AKI Codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6be6bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of non-ESRD encounter blocks with AKI codes: 97.9%\n",
      "(2199 out of 2247 blocks)\n",
      "\n",
      "Sample of AKI-related diagnoses found: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "diagnosis_code\n",
       "n170    1385\n",
       "n179     897\n",
       "r34      286\n",
       "n990       9\n",
       "n178       5\n",
       "n171       1\n",
       "n172       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AKI Codes Sanity check\n",
    "\n",
    "# Define AKI ICD-10 codes\n",
    "aki_codes = [\n",
    "    # ICD-10 codes for acute kidney injury\n",
    "    'n170', 'n171', 'n172', 'n178', 'n179',  # Acute kidney failure codes\n",
    "    'r34',   # Anuria and oliguria\n",
    "    'n990', # Post-procedural kidney failure\n",
    "    't795',  # Traumatic anuria\n",
    "    '5845',  # ICD9 Acute kidney failure with lesion of tubular necrosis\n",
    "    '5849',  # ICD9- Acute kidney failure, unspecified\n",
    "    \"5848\"    # ICD9 - Acute kidney failure with other specified pathological lesion in kidney\n",
    "]\n",
    "\n",
    "# Filter to non-ESRD encounters first\n",
    "non_esrd_encounters = hospital_diagnosis_df[hospital_diagnosis_df['encounter_block'].isin(cohort_df['encounter_block'])]\n",
    "\n",
    "# Create mask for AKI diagnoses on the filtered data\n",
    "aki_mask = non_esrd_encounters['diagnosis_code'].isin(aki_codes)\n",
    "\n",
    "# Get encounter blocks with AKI diagnoses\n",
    "blocks_with_aki = non_esrd_encounters[aki_mask]['encounter_block'].unique()\n",
    "total_non_esrd_blocks = cohort_df['encounter_block'].nunique()\n",
    "strobe_counts['3_encounter_blocks_with_AKI_no_esrd'] = len(blocks_with_aki) \n",
    "\n",
    "# Calculate percentage\n",
    "aki_percentage = (len(blocks_with_aki) / total_non_esrd_blocks) * 100\n",
    "\n",
    "print(f\"\\nPercentage of non-ESRD encounter blocks with AKI codes: {aki_percentage:.1f}%\")\n",
    "print(f\"({len(blocks_with_aki)} out of {total_non_esrd_blocks} blocks)\")\n",
    "strobe_counts['3_Percentage_non_ESRD_encounter_blocks_with_AKI_codes'] = aki_percentage\n",
    "# Show sample of AKI diagnoses\n",
    "aki_diagnoses = non_esrd_encounters[aki_mask][['hospitalization_id', 'diagnosis_code','poa_present']].drop_duplicates()\n",
    "print(\"\\nSample of AKI-related diagnoses found: \")\n",
    "aki_diagnoses['diagnosis_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f500f",
   "metadata": {},
   "source": [
    "### 2. ICU Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb06cc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validating ICU Administration ===\n",
      "\n",
      "Number of CRRT hospitalizations without any ICU stay: 10\n",
      "WARNING: Found CRRT hospitalizations without ICU stays\n",
      "Number of hospitalization IDs without ICU stays: 10 check crrt_non_icu_df df\n"
     ]
    }
   ],
   "source": [
    "# Filter ADT data to only include hospitalizations in all_ids\n",
    "adt_final_stitched = adt_stitched[adt_stitched['hospitalization_id'].isin(cohort_df['hospitalization_id'])].copy()\n",
    "adt_final_stitched = adt_final_stitched.sort_values(by=['encounter_block', 'in_dttm'])\n",
    "desired_order = ['hospitalization_id', 'encounter_block', 'hospital_id', 'in_dttm', 'out_dttm']\n",
    "remaining_cols = [col for col in adt_final_stitched.columns if col not in desired_order]\n",
    "adt_final_stitched = adt_final_stitched[desired_order + remaining_cols]\n",
    "\n",
    "print(\"\\n=== Validating ICU Administration ===\")\n",
    "\n",
    "adt_final_stitched['is_icu'] = adt_final_stitched['location_category'] == 'icu'\n",
    "\n",
    "# Check if each hospitalization had at least one ICU stay\n",
    "hosp_icu_status = adt_final_stitched.groupby('encounter_block')['is_icu'].any()\n",
    "non_icu_hosps = hosp_icu_status[~hosp_icu_status].index.tolist()\n",
    "strobe_counts[\"3_number_hosp_without_ICU_stay\"] = len(non_icu_hosps)\n",
    "print(f\"\\nNumber of CRRT hospitalizations without any ICU stay: {len(non_icu_hosps)}\")\n",
    "if len(non_icu_hosps) > 0:\n",
    "    print(\"WARNING: Found CRRT hospitalizations without ICU stays\")\n",
    "    print(\"Number of hospitalization IDs without ICU stays:\", len(non_icu_hosps), \"check crrt_non_icu_df df\")\n",
    "else:\n",
    "    print(\"All CRRT hospitalizations had at least one ICU stay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29014906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospitalization_id</th>\n",
       "      <th>encounter_block</th>\n",
       "      <th>recorded_dttm</th>\n",
       "      <th>crrt_mode_category</th>\n",
       "      <th>crrt_mode_name</th>\n",
       "      <th>blood_flow_rate</th>\n",
       "      <th>pre_filter_replacement_fluid_rate</th>\n",
       "      <th>post_filter_replacement_fluid_rate</th>\n",
       "      <th>dialysate_flow_rate</th>\n",
       "      <th>ultrafiltration_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46968</th>\n",
       "      <td>633645555</td>\n",
       "      <td>155665</td>\n",
       "      <td>2021-04-23 16:00:00-05:00</td>\n",
       "      <td>cvvhd</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>250.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166638</th>\n",
       "      <td>633645555</td>\n",
       "      <td>155665</td>\n",
       "      <td>2021-04-23 17:00:00-05:00</td>\n",
       "      <td>cvvhd</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>250.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108606</th>\n",
       "      <td>633645555</td>\n",
       "      <td>155665</td>\n",
       "      <td>2021-04-23 18:00:00-05:00</td>\n",
       "      <td>cvvhd</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>250.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128019</th>\n",
       "      <td>633645555</td>\n",
       "      <td>155665</td>\n",
       "      <td>2021-04-23 19:00:00-05:00</td>\n",
       "      <td>cvvhd</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>250.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173627</th>\n",
       "      <td>633645555</td>\n",
       "      <td>155665</td>\n",
       "      <td>2021-04-23 20:00:00-05:00</td>\n",
       "      <td>cvvhd</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>250.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130637</th>\n",
       "      <td>645586474</td>\n",
       "      <td>3314</td>\n",
       "      <td>2020-07-09 08:30:00-05:00</td>\n",
       "      <td>cvvhd</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>260.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119797</th>\n",
       "      <td>645586474</td>\n",
       "      <td>3314</td>\n",
       "      <td>2020-07-09 09:00:00-05:00</td>\n",
       "      <td>cvvhd</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>260.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75044</th>\n",
       "      <td>645586474</td>\n",
       "      <td>3314</td>\n",
       "      <td>2020-07-09 09:30:00-05:00</td>\n",
       "      <td>cvvhd</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>260.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9418</th>\n",
       "      <td>645586474</td>\n",
       "      <td>3314</td>\n",
       "      <td>2020-07-09 10:30:00-05:00</td>\n",
       "      <td>cvvhd</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>260.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226576</th>\n",
       "      <td>645884094</td>\n",
       "      <td>13987</td>\n",
       "      <td>2019-01-10 12:50:00-06:00</td>\n",
       "      <td>cvvhd</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>850.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1121 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hospitalization_id  encounter_block             recorded_dttm  \\\n",
       "46968           633645555           155665 2021-04-23 16:00:00-05:00   \n",
       "166638          633645555           155665 2021-04-23 17:00:00-05:00   \n",
       "108606          633645555           155665 2021-04-23 18:00:00-05:00   \n",
       "128019          633645555           155665 2021-04-23 19:00:00-05:00   \n",
       "173627          633645555           155665 2021-04-23 20:00:00-05:00   \n",
       "...                   ...              ...                       ...   \n",
       "130637          645586474             3314 2020-07-09 08:30:00-05:00   \n",
       "119797          645586474             3314 2020-07-09 09:00:00-05:00   \n",
       "75044           645586474             3314 2020-07-09 09:30:00-05:00   \n",
       "9418            645586474             3314 2020-07-09 10:30:00-05:00   \n",
       "226576          645884094            13987 2019-01-10 12:50:00-06:00   \n",
       "\n",
       "       crrt_mode_category  crrt_mode_name  blood_flow_rate  \\\n",
       "46968               cvvhd            <NA>            250.0   \n",
       "166638              cvvhd            <NA>            250.0   \n",
       "108606              cvvhd            <NA>            250.0   \n",
       "128019              cvvhd            <NA>            250.0   \n",
       "173627              cvvhd            <NA>            250.0   \n",
       "...                   ...             ...              ...   \n",
       "130637              cvvhd            <NA>            260.0   \n",
       "119797              cvvhd            <NA>            260.0   \n",
       "75044               cvvhd            <NA>            260.0   \n",
       "9418                cvvhd            <NA>            260.0   \n",
       "226576              cvvhd            <NA>              NaN   \n",
       "\n",
       "        pre_filter_replacement_fluid_rate  post_filter_replacement_fluid_rate  \\\n",
       "46968                                <NA>                                <NA>   \n",
       "166638                               <NA>                                <NA>   \n",
       "108606                               <NA>                                <NA>   \n",
       "128019                               <NA>                                <NA>   \n",
       "173627                               <NA>                                <NA>   \n",
       "...                                   ...                                 ...   \n",
       "130637                               <NA>                                <NA>   \n",
       "119797                               <NA>                                <NA>   \n",
       "75044                                <NA>                                <NA>   \n",
       "9418                                 <NA>                                <NA>   \n",
       "226576                               <NA>                                <NA>   \n",
       "\n",
       "        dialysate_flow_rate  ultrafiltration_out  \n",
       "46968                   NaN                  NaN  \n",
       "166638               1280.0                114.0  \n",
       "108606               1920.0                142.0  \n",
       "128019               1950.0                145.0  \n",
       "173627               1910.0                141.0  \n",
       "...                     ...                  ...  \n",
       "130637                  NaN                  NaN  \n",
       "119797                  NaN                  NaN  \n",
       "75044                   NaN                  NaN  \n",
       "9418                    NaN                  NaN  \n",
       "226576                  NaN                850.0  \n",
       "\n",
       "[1121 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crrt_non_icu_df = crrt_df[crrt_df['encounter_block'].isin(non_icu_hosps)]\n",
    "crrt_non_icu_df = crrt_non_icu_df.sort_values(by=['hospitalization_id', 'encounter_block', 'recorded_dttm'])\n",
    "desired_order = ['hospitalization_id', 'encounter_block', 'recorded_dttm', 'crrt_mode_category']\n",
    "remaining_cols = [col for col in crrt_non_icu_df.columns if col not in desired_order]\n",
    "crrt_non_icu_df = crrt_non_icu_df[desired_order + remaining_cols]\n",
    "crrt_non_icu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24ba07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_subset = adt_df[adt_df['hospitalization_id'].isin(crrt_non_icu_df['hospitalization_id'].unique())]\n",
    "hosp_subset = hosp_df[hosp_df['hospitalization_id'].isin(crrt_non_icu_df['hospitalization_id'].unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907be98",
   "metadata": {},
   "source": [
    "## Labs, Vitals, Meds for the cohort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bd9a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_ids_list = list(cohort_df['hospitalization_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac7bc4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading labs table...\n",
      "   Labs loaded: 2,035,934 rows\n",
      "   Unique lab categories: 16\n",
      "   Unique lab hospitalizations: 2247\n",
      "\n",
      "Loading medication_admin_continuous table...\n",
      "   Medications loaded: 972,727 rows\n",
      "   Unique medication categories: 15\n",
      "   Unique medication_admin_continuous hospitalizations: 2198\n",
      "\n",
      "================================================================================\n",
      "Table Loading Summary\n",
      "================================================================================\n",
      "   patient: 97,254 rows\n",
      "   hospitalization: 166,781 rows\n",
      "   adt: 425,067 rows\n",
      "   labs: 2,035,934 rows\n",
      "   medication_admin_continuous: 972,727 rows\n",
      "   hospital_diagnosis: 108,880 rows\n",
      "   crrt_therapy: 403,322 rows\n",
      "\n",
      "Cohort size: 2,247 hospitalizations\n",
      "\n",
      "Freed 0 objects from memory\n"
     ]
    }
   ],
   "source": [
    "# Vitals\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "#Labs\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "labs_of_interest = ['po2_arterial','pco2_arterial', 'ph_arterial','ph_venous', 'bicarbonate','so2_arterial',\n",
    "                    'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', \n",
    "                    'bun', 'glucose_serum', 'lactate', 'hemoglobin' ]\n",
    "\n",
    "# Continuous administered meds\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol',\n",
    "    'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', \n",
    "    'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium'\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Load Labs\n",
    "# ----------------------------------------------------------------------------\n",
    "print(f\"\\nLoading labs table...\")\n",
    "clif.load_table(\n",
    "    'labs',\n",
    "    columns=labs_required_columns,\n",
    "    filters={\n",
    "        'hospitalization_id': cohort_ids_list,\n",
    "        'lab_category': labs_of_interest\n",
    "    }\n",
    ")\n",
    "print(f\"   Labs loaded: {len(clif.labs.df):,} rows\")\n",
    "print(f\"   Unique lab categories: {clif.labs.df['lab_category'].nunique()}\")\n",
    "print(f\"   Unique lab hospitalizations: {clif.labs.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Load Medication Administration (Continuous)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(f\"\\nLoading medication_admin_continuous table...\")\n",
    "clif.load_table(\n",
    "    'medication_admin_continuous',\n",
    "    columns=meds_required_columns,\n",
    "    filters={\n",
    "        'hospitalization_id': cohort_ids_list,\n",
    "        'med_category': meds_of_interest\n",
    "    }\n",
    ")\n",
    "print(f\"   Medications loaded: {len(clif.medication_admin_continuous.df):,} rows\")\n",
    "print(f\"   Unique medication categories: {clif.medication_admin_continuous.df['med_category'].nunique()}\")\n",
    "print(f\"   Unique medication_admin_continuous hospitalizations: {clif.medication_admin_continuous.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Summary\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Table Loading Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "loaded_tables = clif.get_loaded_tables()\n",
    "for table_name in loaded_tables:\n",
    "    table = getattr(clif, table_name)\n",
    "    print(f\"   {table_name}: {len(table.df):,} rows\")\n",
    "\n",
    "print(f\"\\nCohort size: {len(cohort_hosp_ids):,} hospitalizations\")\n",
    "\n",
    "# Free memory\n",
    "freed = gc.collect()\n",
    "print(f\"\\nFreed {freed} objects from memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be0ebea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating wide dataset\n",
      "==================================================\n",
      "=== WIDE DATASET CREATION STARTED ===\n",
      "==================================================\n",
      "\n",
      "Phase 1: Initialization\n",
      "  1.1: Validating parameters\n",
      "  1.2: Configuring encounter stitching (enabled)\n",
      "\n",
      "Phase 2: Encounter Processing\n",
      "  2.1: === SPECIAL: ENCOUNTER STITCHING ===\n",
      "\n",
      "Phase 3: Table Loading\n",
      "  3.1: Auto-loading base tables\n",
      "  3.2: Loading optional tables: ['vitals', 'labs', 'medication_admin_continuous']\n",
      "       - Loading vitals table...\n",
      "       - labs table already loaded\n",
      "       - medication_admin_continuous table already loaded\n",
      "\n",
      "Phase 4: Calling Wide Dataset Utility\n",
      "  4.1: Passing to wide_dataset.create_wide_dataset()\n",
      "       - Tables: ['vitals', 'labs', 'medication_admin_continuous']\n",
      "       - Category filters: ['vitals', 'labs', 'medication_admin_continuous']\n",
      "       - Batch size: 1000\n",
      "       - Memory limit: None\n",
      "       - Show progress: True\n",
      "\n",
      "Phase 4: Wide Dataset Processing (utility function)\n",
      "  4.1: Starting wide dataset creation...\n",
      "Filtering to specific hospitalization IDs: 2247 encounters\n",
      "\n",
      "Loading and filtering base tables...\n",
      "       - Base tables filtered - Hospitalization: 2247, Patient: 97254, ADT: 10260\n",
      "\n",
      "  4.2: Determining processing mode\n",
      "       - Batch mode: 2247 hospitalizations in 3 batches of 1000\n",
      "  4.B: === BATCH PROCESSING MODE ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4.B.1: Processing batch 1/3\n",
      "             - 1000 hospitalizations in batch\n",
      "\n",
      "    4.S.1: Loading and filtering base tables\n",
      "           - Base cohort created with 1000 records\n",
      "    4.S.3: Processing tables\n",
      "           - Processing vitals...\n",
      "Loaded 3476830 records from vitals\n",
      "           === PIVOTING VITALS ===\n",
      "           - Categories to pivot: ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
      "Filtering vitals categories to: ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
      "Pivoted vitals: 609233 combo_ids with 8 category columns\n",
      "           - Processing labs...\n",
      "Loaded 901976 records from labs\n",
      "           === PIVOTING LABS ===\n",
      "           - Categories to pivot: ['po2_arterial', 'pco2_arterial', 'ph_arterial', 'ph_venous', 'bicarbonate', 'so2_arterial', 'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', 'bun', 'glucose_serum', 'lactate', 'hemoglobin']\n",
      "Filtering labs categories to: ['po2_arterial', 'pco2_arterial', 'ph_arterial', 'ph_venous', 'bicarbonate', 'so2_arterial', 'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', 'bun', 'glucose_serum', 'lactate', 'hemoglobin']\n",
      "Pivoted labs: 163316 combo_ids with 16 category columns\n",
      "           - Processing medication_admin_continuous...\n",
      "           - No converted data found, using original medication data\n",
      "Loaded 431041 records from medication_admin_continuous\n",
      "           === PIVOTING MEDICATION_ADMIN_CONTINUOUS ===\n",
      "           - Categories to pivot: ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin', 'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', 'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium']\n",
      "           - Using original medication column: med_dose\n",
      "Filtering medication_admin_continuous categories to: ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin', 'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', 'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium']\n",
      "Pivoted medication_admin_continuous: 270831 combo_ids with 15 category columns\n",
      "    4.S.4: Creating wide dataset\n",
      "           - Building event time union from 4 tables\n",
      "           - Creating combo_id keys\n",
      "           - Executing main join query\n",
      "Executing join query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 1/3 [00:07<00:14,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    === SPECIAL: MISSING COLUMNS ===\n",
      "           - Added missing column: vecuronium\n",
      "           - Added missing column: pancuronium\n",
      "\n",
      "    4.S.6: Final cleanup\n",
      "           - Removing duplicate columns\n",
      "           - Dropping temporary columns (combo_id, date)\n",
      "           - Wide dataset created: 878397 records with 54 columns\n",
      "             - Batch 1 completed: 878397 records\n",
      "    4.B.2: Processing batch 2/3\n",
      "             - 1000 hospitalizations in batch\n",
      "\n",
      "    4.S.1: Loading and filtering base tables\n",
      "           - Base cohort created with 1000 records\n",
      "    4.S.3: Processing tables\n",
      "           - Processing vitals...\n",
      "Loaded 3738375 records from vitals\n",
      "           === PIVOTING VITALS ===\n",
      "           - Categories to pivot: ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
      "Filtering vitals categories to: ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
      "Pivoted vitals: 670547 combo_ids with 8 category columns\n",
      "           - Processing labs...\n",
      "Loaded 920837 records from labs\n",
      "           === PIVOTING LABS ===\n",
      "           - Categories to pivot: ['po2_arterial', 'pco2_arterial', 'ph_arterial', 'ph_venous', 'bicarbonate', 'so2_arterial', 'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', 'bun', 'glucose_serum', 'lactate', 'hemoglobin']\n",
      "Filtering labs categories to: ['po2_arterial', 'pco2_arterial', 'ph_arterial', 'ph_venous', 'bicarbonate', 'so2_arterial', 'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', 'bun', 'glucose_serum', 'lactate', 'hemoglobin']\n",
      "Pivoted labs: 171501 combo_ids with 16 category columns\n",
      "           - Processing medication_admin_continuous...\n",
      "           - No converted data found, using original medication data\n",
      "Loaded 440690 records from medication_admin_continuous\n",
      "           === PIVOTING MEDICATION_ADMIN_CONTINUOUS ===\n",
      "           - Categories to pivot: ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin', 'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', 'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium']\n",
      "           - Using original medication column: med_dose\n",
      "Filtering medication_admin_continuous categories to: ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin', 'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', 'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium']\n",
      "Pivoted medication_admin_continuous: 271712 combo_ids with 15 category columns\n",
      "    4.S.4: Creating wide dataset\n",
      "           - Building event time union from 4 tables\n",
      "           - Creating combo_id keys\n",
      "           - Executing main join query\n",
      "Executing join query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  67%|██████▋   | 2/3 [00:12<00:06,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    === SPECIAL: MISSING COLUMNS ===\n",
      "           - Added missing column: vecuronium\n",
      "           - Added missing column: pancuronium\n",
      "\n",
      "    4.S.6: Final cleanup\n",
      "           - Removing duplicate columns\n",
      "           - Dropping temporary columns (combo_id, date)\n",
      "           - Wide dataset created: 944391 records with 54 columns\n",
      "             - Batch 2 completed: 944391 records\n",
      "    4.B.3: Processing batch 3/3\n",
      "             - 247 hospitalizations in batch\n",
      "\n",
      "    4.S.1: Loading and filtering base tables\n",
      "           - Base cohort created with 247 records\n",
      "    4.S.3: Processing tables\n",
      "           - Processing vitals...\n",
      "Loaded 884569 records from vitals\n",
      "           === PIVOTING VITALS ===\n",
      "           - Categories to pivot: ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
      "Filtering vitals categories to: ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
      "Pivoted vitals: 157157 combo_ids with 8 category columns\n",
      "           - Processing labs...\n",
      "Loaded 213121 records from labs\n",
      "           === PIVOTING LABS ===\n",
      "           - Categories to pivot: ['po2_arterial', 'pco2_arterial', 'ph_arterial', 'ph_venous', 'bicarbonate', 'so2_arterial', 'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', 'bun', 'glucose_serum', 'lactate', 'hemoglobin']\n",
      "Filtering labs categories to: ['po2_arterial', 'pco2_arterial', 'ph_arterial', 'ph_venous', 'bicarbonate', 'so2_arterial', 'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', 'bun', 'glucose_serum', 'lactate', 'hemoglobin']\n",
      "Pivoted labs: 40248 combo_ids with 16 category columns\n",
      "           - Processing medication_admin_continuous...\n",
      "           - No converted data found, using original medication data\n",
      "Loaded 100996 records from medication_admin_continuous\n",
      "           === PIVOTING MEDICATION_ADMIN_CONTINUOUS ===\n",
      "           - Categories to pivot: ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin', 'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', 'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium']\n",
      "           - Using original medication column: med_dose\n",
      "Filtering medication_admin_continuous categories to: ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin', 'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', 'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium']\n",
      "Pivoted medication_admin_continuous: 63245 combo_ids with 15 category columns\n",
      "    4.S.4: Creating wide dataset\n",
      "           - Building event time union from 4 tables\n",
      "           - Creating combo_id keys\n",
      "           - Executing main join query\n",
      "Executing join query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 3/3 [00:14<00:00,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    === SPECIAL: MISSING COLUMNS ===\n",
      "           - Added missing column: vecuronium\n",
      "           - Added missing column: pancuronium\n",
      "\n",
      "    4.S.6: Final cleanup\n",
      "           - Removing duplicate columns\n",
      "           - Dropping temporary columns (combo_id, date)\n",
      "           - Wide dataset created: 221933 records with 54 columns\n",
      "             - Batch 3 completed: 221933 records\n",
      "             - Combining 3 batch results...\n",
      "             - Final dataset: 2044721 records with 54 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 5: Post-Processing\n",
      "  5.1: === SPECIAL: ADDING ENCOUNTER BLOCKS ===\n",
      "       - Encounter_block column already present - 2247 unique encounter blocks\n",
      "  5.2: No assessment type optimization needed\n",
      "\n",
      "Phase 6: Completion\n",
      "  6.1: Wide dataset stored in self.wide_df\n",
      "  6.2: Dataset shape: 2044721 rows x 54 columns\n",
      "\n",
      "==================================================\n",
      "=== WIDE DATASET CREATION COMPLETED ===\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating wide dataset\")\n",
    "clif.create_wide_dataset(\n",
    "    hospitalization_ids=list(cohort_df['hospitalization_id']),\n",
    "    tables_to_load=['vitals', 'labs', 'medication_admin_continuous' ],\n",
    "    category_filters={\n",
    "        'vitals': vitals_of_interest,\n",
    "        'labs': labs_of_interest,\n",
    "        'medication_admin_continuous': meds_of_interest\n",
    "    },\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa553876",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_keep = [\n",
    "       'patient_id','hospitalization_id', 'event_time',\n",
    "       'angiotensin', 'cisatracurium', 'dexmedetomidine', 'dobutamine',\n",
    "       'dopamine', 'epinephrine', 'isoproterenol', 'lorazepam', 'midazolam',\n",
    "       'milrinone', 'norepinephrine', 'phenylephrine', 'propofol',\n",
    "       'rocuronium', 'vasopressin', 'bicarbonate', 'bun', 'calcium_total',\n",
    "       'chloride', 'creatinine', 'glucose_serum', 'hemoglobin', 'lactate',\n",
    "       'magnesium', 'pco2_arterial', 'ph_arterial', 'ph_venous',\n",
    "       'po2_arterial', 'potassium', 'so2_arterial', 'sodium', 'dbp',\n",
    "       'heart_rate', 'height_cm', 'map', 'respiratory_rate', 'sbp', 'spo2',\n",
    "       'weight_kg','vecuronium', 'pancuronium'\n",
    "]\n",
    "clif.wide_df = clif.wide_df.loc[:, vars_to_keep]\n",
    "clif.wide_df.rename(columns={'event_time': 'recorded_dttm'}, inplace=True)\n",
    "wide_df = clif.wide_df\n",
    "wide_df = wide_df.merge(\n",
    "    encounter_mapping,\n",
    "    on = 'hospitalization_id',\n",
    "     how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13c9349c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id                                object\n",
       "hospitalization_id                        object\n",
       "recorded_dttm         datetime64[us, US/Central]\n",
       "angiotensin                              float64\n",
       "cisatracurium                            float64\n",
       "dexmedetomidine                          float64\n",
       "dobutamine                               float64\n",
       "dopamine                                 float64\n",
       "epinephrine                              float64\n",
       "isoproterenol                            float64\n",
       "lorazepam                                float64\n",
       "midazolam                                float64\n",
       "milrinone                                float64\n",
       "norepinephrine                           float64\n",
       "phenylephrine                            float64\n",
       "propofol                                 float64\n",
       "rocuronium                               float64\n",
       "vasopressin                              float64\n",
       "bicarbonate                              float64\n",
       "bun                                      float64\n",
       "calcium_total                            float64\n",
       "chloride                                 float64\n",
       "creatinine                               float64\n",
       "glucose_serum                            float64\n",
       "hemoglobin                               float64\n",
       "lactate                                  float64\n",
       "magnesium                                float64\n",
       "pco2_arterial                            float64\n",
       "ph_arterial                              float64\n",
       "ph_venous                                float64\n",
       "po2_arterial                             float64\n",
       "potassium                                float64\n",
       "so2_arterial                             float64\n",
       "sodium                                   float64\n",
       "dbp                                      float64\n",
       "heart_rate                               float64\n",
       "height_cm                                float64\n",
       "map                                      float64\n",
       "respiratory_rate                         float64\n",
       "sbp                                      float64\n",
       "spo2                                     float64\n",
       "weight_kg                                float64\n",
       "vecuronium                               float64\n",
       "pancuronium                              float64\n",
       "encounter_block                            int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48218806",
   "metadata": {},
   "source": [
    "## Check PF ratio calc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48a57b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clifpy.tables.respiratory_support.RespiratorySupport at 0xb3f89e350>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clif.load_table('respiratory_support',\n",
    "                        columns=rst_required_columns,\n",
    "                        filters={'hospitalization_id': cohort_ids_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e327c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_support = clif.respiratory_support.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2fd27a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id                                           object\n",
       "hospitalization_id                                   object\n",
       "recorded_dttm                    datetime64[us, US/Central]\n",
       "angiotensin                                         float64\n",
       "cisatracurium                                       float64\n",
       "                                            ...            \n",
       "pressure_support_set                                float64\n",
       "peak_inspiratory_pressure_set                       float64\n",
       "peak_inspiratory_pressure_obs                       float64\n",
       "plateau_pressure_obs                                float64\n",
       "minute_vent_obs                                     float64\n",
       "Length: 62, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = wide_df.merge(\n",
    "    resp_support,\n",
    "    on=['hospitalization_id', 'recorded_dttm'],\n",
    "    how='outer'\n",
    ")\n",
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a297df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "q = f\"\"\"\n",
    "        FROM merged_df\n",
    "        SELECT *\n",
    "        WHERE fio2_set IS NOT NULL OR po2_arterial IS NOT NULL OR device_category IS NOT NULL\n",
    "        \"\"\"\n",
    "df = duckdb.sql(q).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91807efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[['hospitalization_id', \n",
    "                'recorded_dttm','po2_arterial','fio2_set', 'device_category',\n",
    "                'mode_category'\n",
    "]].sort_values(['hospitalization_id', 'recorded_dttm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce9451",
   "metadata": {},
   "source": [
    "## Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb0d6c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Processing ICU segments...\n",
      "   ICU segments identified: 4,696\n",
      "\n",
      "2. Calculating ICU Length of Stay...\n",
      "   Hospitalizations with ICU LOS calculated: 2,237\n",
      "   Median ICU LOS: 7.28 days\n",
      "\n",
      "3. Calculating vitals datetime range...\n",
      "   Hospitalizations with vitals range: 2,247\n",
      "\n",
      "4. Calculating Hospital Length of Stay...\n",
      "   Hospital LOS calculated for: 2,247 hospitalizations\n",
      "   Median Hospital LOS: 14.60 days\n",
      "\n",
      "5. Processing final outcome times...\n",
      "   Final outcome times processed for: 2,247 hospitalizations\n",
      "\n",
      "6. Calculating mortality outcomes...\n",
      "   Mortality calculated for: 2,247 hospitalizations\n",
      "   In-hospital deaths: 1,296 (57.7%)\n",
      "   30-day deaths: 1,338 (59.5%)\n",
      "\n",
      "7. Combining all outcomes...\n",
      "\n",
      "Final outcomes dataset:\n",
      "   Total records: 2,247\n",
      "   Records with ICU LOS: 2,237\n",
      "   Records with Hospital LOS: 2,247\n",
      "   In-hospital mortality rate: 57.7%\n",
      "   30-day mortality rate: 59.5%\n"
     ]
    }
   ],
   "source": [
    "# ICU SEGMENTS - equivalent to icu_segs in R\n",
    "print(\"\\n1. Processing ICU segments...\")\n",
    "icu_segs = adt_final_stitched.copy()\n",
    "icu_segs['loccat'] = icu_segs['location_category'].str.lower()\n",
    "icu_segs = icu_segs[\n",
    "    (icu_segs['loccat'] == 'icu') &\n",
    "    (icu_segs['in_dttm'].notna()) &\n",
    "    (icu_segs['out_dttm'].notna()) &\n",
    "    (icu_segs['out_dttm'] > icu_segs['in_dttm'])\n",
    "]\n",
    "\n",
    "print(f\"   ICU segments identified: {len(icu_segs):,}\")\n",
    "\n",
    "# 2. ICU LENGTH OF STAY \n",
    "print(\"\\n2. Calculating ICU Length of Stay...\")\n",
    "icu_los = icu_segs[icu_segs['hospitalization_id'].isin(cohort_df['hospitalization_id'])].copy()\n",
    "icu_los['seg_days'] = (icu_los['out_dttm'] - icu_los['in_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "icu_los_summary = icu_los.groupby('hospitalization_id').agg({\n",
    "    'seg_days': 'sum'\n",
    "}).reset_index()\n",
    "icu_los_summary.rename(columns={'seg_days': 'icu_los_days'}, inplace=True)\n",
    "\n",
    "print(f\"   Hospitalizations with ICU LOS calculated: {len(icu_los_summary):,}\")\n",
    "print(f\"   Median ICU LOS: {icu_los_summary['icu_los_days'].median():.2f} days\")\n",
    "\n",
    "# VITALS DATETIME RANGE -\n",
    "print(\"\\n3. Calculating vitals datetime range...\")\n",
    "# Use the wide_df which contains vitals data\n",
    "vitals_dttm = wide_df[wide_df['hospitalization_id'].isin(cohort_df['hospitalization_id'])].copy()\n",
    "vitals_dttm = vitals_dttm[vitals_dttm['recorded_dttm'].notna()]\n",
    "\n",
    "vitals_range = vitals_dttm.groupby('hospitalization_id').agg({\n",
    "    'recorded_dttm': ['min', 'max']\n",
    "}).reset_index()\n",
    "vitals_range.columns = ['hospitalization_id', 'first_vital_dttm', 'last_vital_dttm']\n",
    "\n",
    "print(f\"   Hospitalizations with vitals range: {len(vitals_range):,}\")\n",
    "\n",
    "# HOSPITAL LENGTH OF STAY - equivalent to hosp_los in R\n",
    "print(\"\\n4. Calculating Hospital Length of Stay...\")\n",
    "hosp_los = cohort_df[['hospitalization_id']].merge(\n",
    "    vitals_range, \n",
    "    on='hospitalization_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate hospital LOS in days\n",
    "hosp_los['hosp_los_days'] = (\n",
    "    hosp_los['last_vital_dttm'] - hosp_los['first_vital_dttm']\n",
    ").dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# Ensure non-negative values and handle infinities\n",
    "hosp_los['hosp_los_days'] = hosp_los['hosp_los_days'].apply(\n",
    "    lambda x: max(x, 0) if pd.notna(x) and np.isfinite(x) else np.nan\n",
    ")\n",
    "\n",
    "hospitalization_df = clif.hospitalization.df\n",
    "# Merge with hospitalization data\n",
    "hosp_los = hosp_los.merge(\n",
    "    hospitalization_df[['hospitalization_id', 'discharge_category']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"   Hospital LOS calculated for: {hosp_los['hosp_los_days'].notna().sum():,} hospitalizations\")\n",
    "print(f\"   Median Hospital LOS: {hosp_los['hosp_los_days'].median():.2f} days\")\n",
    "\n",
    "# 5. FINAL OUTCOME TIMES - equivalent to final_outcome_times in R\n",
    "print(\"\\n5. Processing final outcome times...\")\n",
    "final_outcome_times = hospitalization_df[\n",
    "    ['patient_id', 'hospitalization_id', 'discharge_category', 'discharge_dttm']\n",
    "].copy()\n",
    "\n",
    "final_outcome_times = final_outcome_times[\n",
    "    final_outcome_times['hospitalization_id'].isin(cohort_df['hospitalization_id'])\n",
    "]\n",
    "\n",
    "final_outcome_times['discharge_cat_low'] = final_outcome_times['discharge_category'].str.lower()\n",
    "\n",
    "# Merge with patient death data\n",
    "patient_df = clif.patient.df[['patient_id', 'death_dttm']]\n",
    "final_outcome_times = final_outcome_times.merge(\n",
    "    patient_df, \n",
    "    on='patient_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with vitals range\n",
    "final_outcome_times = final_outcome_times.merge(\n",
    "    vitals_range, \n",
    "    on='hospitalization_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create final death datetime - use last vital time if discharged as expired/hospice but no death_dttm\n",
    "final_outcome_times['death_dttm_final'] = final_outcome_times.apply(\n",
    "    lambda row: row['last_vital_dttm'] \n",
    "    if (row['discharge_cat_low'] in ['expired', 'hospice'] and pd.isna(row['death_dttm']))\n",
    "    else row['death_dttm'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"   Final outcome times processed for: {len(final_outcome_times):,} hospitalizations\")\n",
    "\n",
    "# 6. MORTALITY CALCULATIONS - equivalent to mortality_instay in R\n",
    "print(\"\\n6. Calculating mortality outcomes...\")\n",
    "\n",
    "# Need to get admission and discharge times - merge with hospitalization data\n",
    "mortality_base = cohort_df.merge(\n",
    "    hospitalization_df[['hospitalization_id', 'admission_dttm', 'discharge_dttm']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "mortality_instay = mortality_base.merge(\n",
    "    final_outcome_times[['hospitalization_id', 'death_dttm_final']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate mortality flags\n",
    "mortality_instay['death_ts'] = mortality_instay['death_dttm_final']\n",
    "\n",
    "# In-hospital death: death occurred between admission and discharge\n",
    "mortality_instay['in_hosp_death'] = (\n",
    "    (mortality_instay['death_ts'].notna()) &\n",
    "    (mortality_instay['death_ts'] >= mortality_instay['admission_dttm']) &\n",
    "    (mortality_instay['death_ts'] <= mortality_instay['discharge_dttm'])\n",
    ").astype(int)\n",
    "\n",
    "# 30-day mortality: death within 30 days of admission\n",
    "mortality_instay['death_30d'] = (\n",
    "    (mortality_instay['death_ts'].notna()) &\n",
    "    (mortality_instay['death_ts'] <= (mortality_instay['admission_dttm'] + pd.Timedelta(days=30)))\n",
    ").astype(int)\n",
    "\n",
    "# Final mortality dataset\n",
    "mortality_final = mortality_instay[['hospitalization_id', 'in_hosp_death', 'death_30d']].copy()\n",
    "\n",
    "print(f\"   Mortality calculated for: {len(mortality_final):,} hospitalizations\")\n",
    "print(f\"   In-hospital deaths: {mortality_final['in_hosp_death'].sum():,} ({mortality_final['in_hosp_death'].mean()*100:.1f}%)\")\n",
    "print(f\"   30-day deaths: {mortality_final['death_30d'].sum():,} ({mortality_final['death_30d'].mean()*100:.1f}%)\")\n",
    "\n",
    "# 7. COMBINE ALL OUTCOMES\n",
    "print(\"\\n7. Combining all outcomes...\")\n",
    "outcomes_df = cohort_df[['hospitalization_id', 'encounter_block']].merge(\n",
    "    icu_los_summary, on='hospitalization_id', how='left'\n",
    ").merge(\n",
    "    hosp_los[['hospitalization_id', 'hosp_los_days']], on='hospitalization_id', how='left'\n",
    ").merge(\n",
    "    mortality_final, on='hospitalization_id', how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal outcomes dataset:\")\n",
    "print(f\"   Total records: {len(outcomes_df):,}\")\n",
    "print(f\"   Records with ICU LOS: {outcomes_df['icu_los_days'].notna().sum():,}\")\n",
    "print(f\"   Records with Hospital LOS: {outcomes_df['hosp_los_days'].notna().sum():,}\")\n",
    "print(f\"   In-hospital mortality rate: {outcomes_df['in_hosp_death'].mean()*100:.1f}%\")\n",
    "print(f\"   30-day mortality rate: {outcomes_df['death_30d'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba8796e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hospitalization_id     object\n",
       "encounter_block         int32\n",
       "icu_los_days          float64\n",
       "hosp_los_days         float64\n",
       "in_hosp_death           int64\n",
       "death_30d               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734b385",
   "metadata": {},
   "source": [
    "## Process CRRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ff948cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter crrt_df to only include hospitalization_ids present in the cohort\n",
    "crrt_cohort = crrt_df[crrt_df['hospitalization_id'].isin(cohort_df['hospitalization_id'])].copy()\n",
    "# del crrt_df\n",
    "\n",
    "#assume on_crrt for all rows in the crrt table \n",
    "crrt_cohort.loc[:, \"on_crrt\"] = 1\n",
    "\n",
    "# Forward fill blood_flow_rate within each encounter_block\n",
    "crrt_cohort = crrt_cohort.sort_values(['hospitalization_id', 'encounter_block', 'recorded_dttm'])\n",
    "crrt_cohort['blood_flow_rate'] = crrt_cohort.groupby('encounter_block')['blood_flow_rate'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0b26c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hospitalization_id                                string[python]\n",
       "recorded_dttm                         datetime64[us, US/Central]\n",
       "crrt_mode_name                                             Int32\n",
       "crrt_mode_category                                        object\n",
       "blood_flow_rate                                          float64\n",
       "pre_filter_replacement_fluid_rate                          Int32\n",
       "post_filter_replacement_fluid_rate                         Int32\n",
       "dialysate_flow_rate                                      float64\n",
       "ultrafiltration_out                                      float64\n",
       "encounter_block                                            int32\n",
       "on_crrt                                                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crrt_cohort.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "437d19f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing CRRT Data for ATS 2026 Study\n",
      "================================================================================\n",
      "\n",
      "3. Calculating Index Time...\n",
      "   Index times calculated for: 2,247 encounter_blocks\n",
      "\n",
      "4. Finding closest weights to index time...\n",
      "   Weight records available: 45,006\n",
      "   Weights found for: 2,190 encounter_blocks\n",
      "\n",
      "5. Getting CRRT parameters at index time...\n",
      "   CRRT records at index time: 2,247\n",
      "\n",
      "6. Combining results...\n",
      "   Final dataset: 2,247 records\n",
      "   Records with weights: 2,190\n",
      "   Records with CRRT mode: 2,247\n",
      "\n",
      "7. Data validation...\n",
      "   Unique encounter blocks: 2,247\n",
      "   Date range: 2018-01-06 15:15:00-06:00 to 2024-12-31 01:00:00-06:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Processing CRRT Data for ATS 2026 Study\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 3: Calculate Index Time (First CRRT per encounter_block)\n",
    "print(\"\\n3. Calculating Index Time...\")\n",
    "index_times = crrt_cohort.groupby('encounter_block').agg({\n",
    "    'recorded_dttm': 'min'\n",
    "}).reset_index()\n",
    "index_times.rename(columns={'recorded_dttm': 'index_time'}, inplace=True)\n",
    "\n",
    "print(f\"   Index times calculated for: {len(index_times):,} encounter_blocks\")\n",
    "\n",
    "# Step 4: Get Weight Data - SIMPLE APPROACH\n",
    "print(\"\\n4. Finding closest weights to index time...\")\n",
    "\n",
    "# Extract weight data\n",
    "weight_data = wide_df[['encounter_block', 'recorded_dttm', 'weight_kg']].copy()\n",
    "weight_data = weight_data[weight_data['weight_kg'].notna()]\n",
    "\n",
    "print(f\"   Weight records available: {len(weight_data):,}\")\n",
    "\n",
    "# Simple merge and filter approach\n",
    "combined = index_times.merge(weight_data, on='encounter_block', how='inner')\n",
    "\n",
    "# Keep only weights recorded before or at index time\n",
    "combined = combined[combined['recorded_dttm'] <= combined['index_time']]\n",
    "\n",
    "# For each encounter_block, get the weight closest to (but not after) index_time\n",
    "closest_weights = (combined\n",
    "                  .sort_values(['encounter_block', 'recorded_dttm'])\n",
    "                  .groupby('encounter_block')\n",
    "                  .last()\n",
    "                  .reset_index())\n",
    "\n",
    "# Keep only the columns we need\n",
    "closest_weights = closest_weights[['encounter_block', 'weight_kg']]\n",
    "\n",
    "print(f\"   Weights found for: {len(closest_weights):,} encounter_blocks\")\n",
    "\n",
    "# Step 5: Get CRRT parameters at index time\n",
    "print(\"\\n5. Getting CRRT parameters at index time...\")\n",
    "\n",
    "# Merge CRRT data with index times\n",
    "crrt_with_index = crrt_cohort.merge(\n",
    "    index_times, \n",
    "    on='encounter_block',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "\n",
    "# Filter to records at exactly the index time\n",
    "crrt_at_index = crrt_with_index[crrt_with_index['recorded_dttm'] == crrt_with_index['index_time']].copy()\n",
    "\n",
    "print(f\"   CRRT records at index time: {len(crrt_at_index):,}\")\n",
    "\n",
    "# Step 6: Combine CRRT data with weights\n",
    "print(\"\\n6. Combining results...\")\n",
    "\n",
    "index_crrt_df = crrt_at_index.merge(\n",
    "    closest_weights, \n",
    "    on='encounter_block', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"   Final dataset: {len(index_crrt_df):,} records\")\n",
    "print(f\"   Records with weights: {index_crrt_df['weight_kg'].notna().sum():,}\")\n",
    "print(f\"   Records with CRRT mode: {index_crrt_df['crrt_mode_category'].notna().sum():,}\")\n",
    "\n",
    "# Verify what we have\n",
    "print(\"\\n7. Data validation...\")\n",
    "print(f\"   Unique encounter blocks: {index_crrt_df['encounter_block'].nunique():,}\")\n",
    "print(f\"   Date range: {index_crrt_df['index_time'].min()} to {index_crrt_df['index_time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d038ce37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hospitalization_id                                string[python]\n",
       "recorded_dttm                         datetime64[us, US/Central]\n",
       "crrt_mode_name                                             Int32\n",
       "crrt_mode_category                                        object\n",
       "blood_flow_rate                                          float64\n",
       "pre_filter_replacement_fluid_rate                          Int32\n",
       "post_filter_replacement_fluid_rate                         Int32\n",
       "dialysate_flow_rate                                      float64\n",
       "ultrafiltration_out                                      float64\n",
       "encounter_block                                            int32\n",
       "on_crrt                                                    int64\n",
       "index_time                            datetime64[us, US/Central]\n",
       "weight_kg                                                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_crrt_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27ec7b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Step 7: Calculating CRRT Dose at Index Time\n",
      "================================================================================\n",
      "\n",
      "   Mode distribution:\n",
      "crrt_mode_category\n",
      "cvvhd    2207\n",
      "scuf       40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   Calculating mode-specific flow rates...\n",
      "   Total flow rates calculated: 2,207\n",
      "\n",
      "   Calculating CRRT dose (mL/kg/hr)...\n",
      "\n",
      "================================================================================\n",
      "Step 8: CRRT Dose Summary Statistics\n",
      "================================================================================\n",
      "\n",
      "   CRRT doses calculated: 776/2,247 (34.5%)\n",
      "\n",
      "   Dose statistics (mL/kg/hr):\n",
      "     Mean ± SD: 25.2 ± 53.1\n",
      "     Median [IQR]: 19.1 [9.0-30.3]\n",
      "     Range: 0.1 - 1012.6\n",
      "\n",
      "   Clinical Target Analysis:\n",
      "     Below target (<20 mL/kg/hr): 409 (52.7%)\n",
      "     Within target (20-25 mL/kg/hr): 76 (9.8%)\n",
      "     Above target (>25 mL/kg/hr): 291 (37.5%)\n",
      "\n",
      "   Edge Case Detection:\n",
      "     ⚠️  Extreme low doses (<10 mL/kg/hr): 221\n",
      "     ⚠️  Extreme high doses (>50 mL/kg/hr): 57\n",
      "\n",
      "   Dose by CRRT Mode:\n",
      "     CVVHD:\n",
      "       Count: 2,207 (98.2%)\n",
      "       Doses available: 776\n",
      "       Mean dose: 25.2 mL/kg/hr\n",
      "       Median dose: 19.1 mL/kg/hr\n",
      "\n",
      "================================================================================\n",
      "Step 9: Creating Final Analysis Dataset\n",
      "================================================================================\n",
      "\n",
      "   Final analysis dataset created:\n",
      "     Total records: 2,247\n",
      "     Unique encounter blocks: 2,247\n",
      "     Records with valid dose: 776\n",
      "     Records with weight: 2,190\n",
      "\n",
      "   Quality Checks:\n",
      "     Missing weights: 57\n",
      "     Missing doses: 1,471\n",
      "     Missing mode: 0\n",
      "\n",
      "✅ CRRT dose calculation completed successfully!\n",
      "   Dataset 'crrt_analysis_df' ready for propensity score analysis\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: Calculate CRRT Dose\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 7: Calculating CRRT Dose at Index Time\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Fill NaN values with 0 for flow rate calculations\n",
    "flow_cols = ['dialysate_flow_rate', 'pre_filter_replacement_fluid_rate', 'post_filter_replacement_fluid_rate']\n",
    "index_crrt_df[flow_cols] = index_crrt_df[flow_cols].fillna(0)\n",
    "\n",
    "# Standardize mode category to lowercase\n",
    "index_crrt_df['crrt_mode_category'] = index_crrt_df['crrt_mode_category'].str.lower()\n",
    "\n",
    "print(\"\\n   Mode distribution:\")\n",
    "print(index_crrt_df['crrt_mode_category'].value_counts())\n",
    "\n",
    "# Calculate total flow based on mode (vectorized approach)\n",
    "print(\"\\n   Calculating mode-specific flow rates...\")\n",
    "\n",
    "conditions = [\n",
    "    index_crrt_df['crrt_mode_category'] == 'cvvhd',\n",
    "    index_crrt_df['crrt_mode_category'] == 'cvvh', \n",
    "    index_crrt_df['crrt_mode_category'] == 'cvvhdf'\n",
    "]\n",
    "\n",
    "choices = [\n",
    "    # CVVHD: Dialysate flow rate alone\n",
    "    index_crrt_df['dialysate_flow_rate'],\n",
    "    # CVVH: Replacement fluid rate (pre + post)\n",
    "    index_crrt_df['pre_filter_replacement_fluid_rate'] + index_crrt_df['post_filter_replacement_fluid_rate'],\n",
    "    # CVVHDF: All flows combined\n",
    "    index_crrt_df['dialysate_flow_rate'] + index_crrt_df['pre_filter_replacement_fluid_rate'] + index_crrt_df['post_filter_replacement_fluid_rate']\n",
    "]\n",
    "\n",
    "# Apply vectorized conditions\n",
    "index_crrt_df['total_flow_rate'] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "print(f\"   Total flow rates calculated: {index_crrt_df['total_flow_rate'].notna().sum():,}\")\n",
    "\n",
    "# Calculate dose: total_flow_rate / weight_kg (vectorized)\n",
    "print(\"\\n   Calculating CRRT dose (mL/kg/hr)...\")\n",
    "\n",
    "index_crrt_df['crrt_dose_ml_kg_hr'] = np.where(\n",
    "    (index_crrt_df['weight_kg'] > 0) & (index_crrt_df['total_flow_rate'] > 0),\n",
    "    index_crrt_df['total_flow_rate'] / index_crrt_df['weight_kg'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: Summary Statistics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 8: CRRT Dose Summary Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dose_available = index_crrt_df['crrt_dose_ml_kg_hr'].notna()\n",
    "print(f\"\\n   CRRT doses calculated: {dose_available.sum():,}/{len(index_crrt_df):,} ({dose_available.mean()*100:.1f}%)\")\n",
    "\n",
    "if dose_available.sum() > 0:\n",
    "    dose_stats = index_crrt_df['crrt_dose_ml_kg_hr'].describe()\n",
    "    print(f\"\\n   Dose statistics (mL/kg/hr):\")\n",
    "    print(f\"     Mean ± SD: {dose_stats['mean']:.1f} ± {dose_stats['std']:.1f}\")\n",
    "    print(f\"     Median [IQR]: {dose_stats['50%']:.1f} [{dose_stats['25%']:.1f}-{dose_stats['75%']:.1f}]\")\n",
    "    print(f\"     Range: {dose_stats['min']:.1f} - {dose_stats['max']:.1f}\")\n",
    "    \n",
    "    # Clinical targets analysis\n",
    "    print(f\"\\n   Clinical Target Analysis:\")\n",
    "    target_range = ((index_crrt_df['crrt_dose_ml_kg_hr'] >= 20) & \n",
    "                   (index_crrt_df['crrt_dose_ml_kg_hr'] <= 25)).sum()\n",
    "    below_target = (index_crrt_df['crrt_dose_ml_kg_hr'] < 20).sum()\n",
    "    above_target = (index_crrt_df['crrt_dose_ml_kg_hr'] > 25).sum()\n",
    "    \n",
    "    print(f\"     Below target (<20 mL/kg/hr): {below_target:,} ({below_target/dose_available.sum()*100:.1f}%)\")\n",
    "    print(f\"     Within target (20-25 mL/kg/hr): {target_range:,} ({target_range/dose_available.sum()*100:.1f}%)\")\n",
    "    print(f\"     Above target (>25 mL/kg/hr): {above_target:,} ({above_target/dose_available.sum()*100:.1f}%)\")\n",
    "    \n",
    "    # Edge case detection\n",
    "    print(f\"\\n   Edge Case Detection:\")\n",
    "    extreme_low = (index_crrt_df['crrt_dose_ml_kg_hr'] < 10).sum()\n",
    "    extreme_high = (index_crrt_df['crrt_dose_ml_kg_hr'] > 50).sum()\n",
    "    \n",
    "    if extreme_low > 0:\n",
    "        print(f\"     ⚠️  Extreme low doses (<10 mL/kg/hr): {extreme_low}\")\n",
    "    if extreme_high > 0:\n",
    "        print(f\"     ⚠️  Extreme high doses (>50 mL/kg/hr): {extreme_high}\")\n",
    "\n",
    "# Mode breakdown with dose statistics\n",
    "print(f\"\\n   Dose by CRRT Mode:\")\n",
    "for mode in index_crrt_df['crrt_mode_category'].unique():\n",
    "    if pd.notna(mode):\n",
    "        mode_data = index_crrt_df[index_crrt_df['crrt_mode_category'] == mode]\n",
    "        mode_doses = mode_data['crrt_dose_ml_kg_hr'].dropna()\n",
    "        \n",
    "        if len(mode_doses) > 0:\n",
    "            print(f\"     {mode.upper()}:\")\n",
    "            print(f\"       Count: {len(mode_data):,} ({len(mode_data)/len(index_crrt_df)*100:.1f}%)\")\n",
    "            print(f\"       Doses available: {len(mode_doses):,}\")\n",
    "            print(f\"       Mean dose: {mode_doses.mean():.1f} mL/kg/hr\")\n",
    "            print(f\"       Median dose: {mode_doses.median():.1f} mL/kg/hr\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: Create Final Analysis Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 9: Creating Final Analysis Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select key columns for analysis\n",
    "analysis_columns = [\n",
    "    'encounter_block', \n",
    "    'hospitalization_id',\n",
    "    'index_time',\n",
    "    'crrt_mode_category',\n",
    "    'weight_kg',\n",
    "    'total_flow_rate',\n",
    "    'crrt_dose_ml_kg_hr',\n",
    "    'dialysate_flow_rate',\n",
    "    'pre_filter_replacement_fluid_rate',\n",
    "    'post_filter_replacement_fluid_rate',\n",
    "    'blood_flow_rate',\n",
    "    'ultrafiltration_out'\n",
    "]\n",
    "\n",
    "# Create final analysis dataset\n",
    "crrt_analysis_df = index_crrt_df[analysis_columns].copy()\n",
    "\n",
    "print(f\"\\n   Final analysis dataset created:\")\n",
    "print(f\"     Total records: {len(crrt_analysis_df):,}\")\n",
    "print(f\"     Unique encounter blocks: {crrt_analysis_df['encounter_block'].nunique():,}\")\n",
    "print(f\"     Records with valid dose: {crrt_analysis_df['crrt_dose_ml_kg_hr'].notna().sum():,}\")\n",
    "print(f\"     Records with weight: {crrt_analysis_df['weight_kg'].notna().sum():,}\")\n",
    "\n",
    "# Quality checks\n",
    "print(f\"\\n   Quality Checks:\")\n",
    "print(f\"     Missing weights: {crrt_analysis_df['weight_kg'].isna().sum():,}\")\n",
    "print(f\"     Missing doses: {crrt_analysis_df['crrt_dose_ml_kg_hr'].isna().sum():,}\")\n",
    "print(f\"     Missing mode: {crrt_analysis_df['crrt_mode_category'].isna().sum():,}\")\n",
    "\n",
    "print(\"\\n✅ CRRT dose calculation completed successfully!\")\n",
    "print(f\"   Dataset 'crrt_analysis_df' ready for propensity score analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1db36691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Step 10: Time-Weighted Average CRRT Dose Analysis\n",
      "================================================================================\n",
      "\n",
      "   CRRT records by time window:\n",
      "     At initiation (t=0): 2,247 records\n",
      "     Within 6 hours: 13,909 records\n",
      "     Within 24 hours: 44,231 records\n",
      "\n",
      "   Calculating doses for time windows...\n",
      "\n",
      "   Calculating time-weighted averages...\n",
      "\n",
      "================================================================================\n",
      "CRRT Dose Comparison: Initiation vs 6h vs 24h\n",
      "================================================================================\n",
      "\n",
      "📍 At Initiation (t=0):\n",
      "   Doses available: 776/2,247 (34.5%)\n",
      "   Mean ± SD: 25.2 ± 53.1 mL/kg/hr\n",
      "   Median [IQR]: 19.1 [9.0-30.3]\n",
      "\n",
      "⏰ First 6 Hours:\n",
      "   Doses available: 1,993/2,247 (88.7%)\n",
      "   Mean ± SD: 53.8 ± 342.8 mL/kg/hr\n",
      "   Median [IQR]: 32.1 [26.5-43.4]\n",
      "   Avg measurements per encounter: 5.0\n",
      "\n",
      "📅 First 24 Hours:\n",
      "   Doses available: 2,074/2,247 (92.3%)\n",
      "   Mean ± SD: 52.8 ± 166.4 mL/kg/hr\n",
      "   Median [IQR]: 33.9 [27.3-47.4]\n",
      "   Avg measurements per encounter: 17.2\n",
      "\n",
      "================================================================================\n",
      "Clinical Target Analysis (20-25 mL/kg/hr)\n",
      "================================================================================\n",
      "\n",
      "At Initiation:\n",
      "   Below target (<20): 409 (52.7%)\n",
      "   Within target (20-25): 76 (9.8%)\n",
      "   Above target (>25): 291 (37.5%)\n",
      "\n",
      "First 6 Hours (Time-Weighted):\n",
      "   Below target (<20): 172 (8.6%)\n",
      "   Within target (20-25): 233 (11.7%)\n",
      "   Above target (>25): 1,588 (79.7%)\n",
      "\n",
      "First 24 Hours (Time-Weighted):\n",
      "   Below target (<20): 132 (6.4%)\n",
      "   Within target (20-25): 219 (10.6%)\n",
      "   Above target (>25): 1,723 (83.1%)\n",
      "\n",
      "================================================================================\n",
      "Creating Combined Dose Dataset\n",
      "================================================================================\n",
      "\n",
      "   Combined dataset created:\n",
      "     Total encounters: 2,247\n",
      "     Has initiation dose: 776\n",
      "     Has 6h avg dose: 1,993\n",
      "     Has 24h avg dose: 2,074\n",
      "\n",
      "   Correlation between dose measurements:\n",
      "                    dose_at_initiation  dose_6h_avg  dose_24h_avg\n",
      "dose_at_initiation            1.000000     0.674688      0.271298\n",
      "dose_6h_avg                   0.674688     1.000000      0.646001\n",
      "dose_24h_avg                  0.271298     0.646001      1.000000\n",
      "\n",
      "✅ Time-weighted dose analysis completed!\n",
      "   Datasets created: 'dose_6h_summary', 'dose_24h_summary', 'dose_comparison_df'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: Time-Weighted Average Dose Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 10: Time-Weighted Average CRRT Dose Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all CRRT records with index time and encounter blocks\n",
    "crrt_with_index = crrt_cohort.merge(\n",
    "    index_times,\n",
    "    on='encounter_block',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Calculate time from index\n",
    "crrt_with_index['hours_from_index'] = (\n",
    "    crrt_with_index['recorded_dttm'] - crrt_with_index['index_time']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Filter for records within time windows\n",
    "crrt_6h = crrt_with_index[\n",
    "    (crrt_with_index['hours_from_index'] >= 0) & \n",
    "    (crrt_with_index['hours_from_index'] <= 6)\n",
    "].copy()\n",
    "\n",
    "crrt_24h = crrt_with_index[\n",
    "    (crrt_with_index['hours_from_index'] >= 0) & \n",
    "    (crrt_with_index['hours_from_index'] <= 24)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\n   CRRT records by time window:\")\n",
    "print(f\"     At initiation (t=0): {len(crrt_at_index):,} records\")\n",
    "print(f\"     Within 6 hours: {len(crrt_6h):,} records\")\n",
    "print(f\"     Within 24 hours: {len(crrt_24h):,} records\")\n",
    "\n",
    "# ============================================================================\n",
    "# Calculate instantaneous doses for each time window\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_crrt_dose(df, closest_weights):\n",
    "    \"\"\"Calculate CRRT dose for a dataframe of CRRT records\"\"\"\n",
    "    # Merge with weights\n",
    "    df_with_weight = df.merge(\n",
    "        closest_weights[['encounter_block', 'weight_kg']],\n",
    "        on='encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill NaN flow rates with 0\n",
    "    flow_cols = ['dialysate_flow_rate', 'pre_filter_replacement_fluid_rate', \n",
    "                 'post_filter_replacement_fluid_rate']\n",
    "    df_with_weight[flow_cols] = df_with_weight[flow_cols].fillna(0)\n",
    "    \n",
    "    # Standardize mode\n",
    "    df_with_weight['crrt_mode_category'] = df_with_weight['crrt_mode_category'].str.lower()\n",
    "    \n",
    "    # Calculate total flow based on mode\n",
    "    conditions = [\n",
    "        df_with_weight['crrt_mode_category'] == 'cvvhd',\n",
    "        df_with_weight['crrt_mode_category'] == 'cvvh',\n",
    "        df_with_weight['crrt_mode_category'] == 'cvvhdf'\n",
    "    ]\n",
    "    \n",
    "    choices = [\n",
    "        df_with_weight['dialysate_flow_rate'],\n",
    "        df_with_weight['pre_filter_replacement_fluid_rate'] + df_with_weight['post_filter_replacement_fluid_rate'],\n",
    "        df_with_weight['dialysate_flow_rate'] + df_with_weight['pre_filter_replacement_fluid_rate'] + df_with_weight['post_filter_replacement_fluid_rate']\n",
    "    ]\n",
    "    \n",
    "    df_with_weight['total_flow_rate'] = np.select(conditions, choices, default=np.nan)\n",
    "    \n",
    "    # Calculate dose\n",
    "    df_with_weight['crrt_dose_ml_kg_hr'] = np.where(\n",
    "        (df_with_weight['weight_kg'] > 0) & (df_with_weight['total_flow_rate'] > 0),\n",
    "        df_with_weight['total_flow_rate'] / df_with_weight['weight_kg'],\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    return df_with_weight\n",
    "\n",
    "# Calculate doses for each window\n",
    "print(\"\\n   Calculating doses for time windows...\")\n",
    "crrt_6h_doses = calculate_crrt_dose(crrt_6h, closest_weights)\n",
    "crrt_24h_doses = calculate_crrt_dose(crrt_24h, closest_weights)\n",
    "\n",
    "# ============================================================================\n",
    "# Calculate time-weighted averages\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_time_weighted_dose(df):\n",
    "    \"\"\"\n",
    "    Calculate time-weighted average dose per encounter block.\n",
    "    Uses trapezoidal rule for integration.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for block, group in df.groupby('encounter_block'):\n",
    "        # Sort by time\n",
    "        group = group.sort_values('recorded_dttm')\n",
    "        \n",
    "        # Filter out records without valid doses\n",
    "        valid_doses = group[group['crrt_dose_ml_kg_hr'].notna()].copy()\n",
    "        \n",
    "        if len(valid_doses) == 0:\n",
    "            results.append({\n",
    "                'encounter_block': block,\n",
    "                'time_weighted_dose': np.nan,\n",
    "                'n_measurements': 0,\n",
    "                'duration_hours': 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        if len(valid_doses) == 1:\n",
    "            # Only one measurement - use it as the average\n",
    "            results.append({\n",
    "                'encounter_block': block,\n",
    "                'time_weighted_dose': valid_doses['crrt_dose_ml_kg_hr'].iloc[0],\n",
    "                'n_measurements': 1,\n",
    "                'duration_hours': 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Calculate time-weighted average using trapezoidal rule\n",
    "        times = valid_doses['hours_from_index'].values\n",
    "        doses = valid_doses['crrt_dose_ml_kg_hr'].values\n",
    "        \n",
    "        # Calculate weighted sum\n",
    "        weighted_sum = 0\n",
    "        for i in range(len(times) - 1):\n",
    "            dt = times[i+1] - times[i]\n",
    "            avg_dose = (doses[i] + doses[i+1]) / 2\n",
    "            weighted_sum += avg_dose * dt\n",
    "        \n",
    "        total_duration = times[-1] - times[0]\n",
    "        time_weighted_dose = weighted_sum / total_duration if total_duration > 0 else doses.mean()\n",
    "        \n",
    "        results.append({\n",
    "            'encounter_block': block,\n",
    "            'time_weighted_dose': time_weighted_dose,\n",
    "            'n_measurements': len(valid_doses),\n",
    "            'duration_hours': total_duration\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n   Calculating time-weighted averages...\")\n",
    "dose_6h_summary = calculate_time_weighted_dose(crrt_6h_doses)\n",
    "dose_24h_summary = calculate_time_weighted_dose(crrt_24h_doses)\n",
    "\n",
    "# ============================================================================\n",
    "# Summary Statistics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CRRT Dose Comparison: Initiation vs 6h vs 24h\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# At initiation\n",
    "init_available = crrt_analysis_df['crrt_dose_ml_kg_hr'].notna().sum()\n",
    "print(f\"\\n📍 At Initiation (t=0):\")\n",
    "print(f\"   Doses available: {init_available:,}/{len(crrt_analysis_df):,} ({init_available/len(crrt_analysis_df)*100:.1f}%)\")\n",
    "if init_available > 0:\n",
    "    init_stats = crrt_analysis_df['crrt_dose_ml_kg_hr'].describe()\n",
    "    print(f\"   Mean ± SD: {init_stats['mean']:.1f} ± {init_stats['std']:.1f} mL/kg/hr\")\n",
    "    print(f\"   Median [IQR]: {init_stats['50%']:.1f} [{init_stats['25%']:.1f}-{init_stats['75%']:.1f}]\")\n",
    "\n",
    "# 6 hours\n",
    "dose_6h_available = dose_6h_summary['time_weighted_dose'].notna().sum()\n",
    "print(f\"\\n⏰ First 6 Hours:\")\n",
    "print(f\"   Doses available: {dose_6h_available:,}/{len(dose_6h_summary):,} ({dose_6h_available/len(dose_6h_summary)*100:.1f}%)\")\n",
    "if dose_6h_available > 0:\n",
    "    stats_6h = dose_6h_summary['time_weighted_dose'].describe()\n",
    "    print(f\"   Mean ± SD: {stats_6h['mean']:.1f} ± {stats_6h['std']:.1f} mL/kg/hr\")\n",
    "    print(f\"   Median [IQR]: {stats_6h['50%']:.1f} [{stats_6h['25%']:.1f}-{stats_6h['75%']:.1f}]\")\n",
    "    print(f\"   Avg measurements per encounter: {dose_6h_summary['n_measurements'].mean():.1f}\")\n",
    "\n",
    "# 24 hours\n",
    "dose_24h_available = dose_24h_summary['time_weighted_dose'].notna().sum()\n",
    "print(f\"\\n📅 First 24 Hours:\")\n",
    "print(f\"   Doses available: {dose_24h_available:,}/{len(dose_24h_summary):,} ({dose_24h_available/len(dose_24h_summary)*100:.1f}%)\")\n",
    "if dose_24h_available > 0:\n",
    "    stats_24h = dose_24h_summary['time_weighted_dose'].describe()\n",
    "    print(f\"   Mean ± SD: {stats_24h['mean']:.1f} ± {stats_24h['std']:.1f} mL/kg/hr\")\n",
    "    print(f\"   Median [IQR]: {stats_24h['50%']:.1f} [{stats_24h['25%']:.1f}-{stats_24h['75%']:.1f}]\")\n",
    "    print(f\"   Avg measurements per encounter: {dose_24h_summary['n_measurements'].mean():.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Clinical Target Analysis by Time Window\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Clinical Target Analysis (20-25 mL/kg/hr)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def analyze_targets(doses, label):\n",
    "    \"\"\"Analyze how many doses meet clinical targets\"\"\"\n",
    "    valid_doses = doses[doses.notna()]\n",
    "    if len(valid_doses) == 0:\n",
    "        return\n",
    "    \n",
    "    below = (valid_doses < 20).sum()\n",
    "    within = ((valid_doses >= 20) & (valid_doses <= 25)).sum()\n",
    "    above = (valid_doses > 25).sum()\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"   Below target (<20): {below:,} ({below/len(valid_doses)*100:.1f}%)\")\n",
    "    print(f\"   Within target (20-25): {within:,} ({within/len(valid_doses)*100:.1f}%)\")\n",
    "    print(f\"   Above target (>25): {above:,} ({above/len(valid_doses)*100:.1f}%)\")\n",
    "\n",
    "analyze_targets(crrt_analysis_df['crrt_dose_ml_kg_hr'], \"At Initiation\")\n",
    "analyze_targets(dose_6h_summary['time_weighted_dose'], \"First 6 Hours (Time-Weighted)\")\n",
    "analyze_targets(dose_24h_summary['time_weighted_dose'], \"First 24 Hours (Time-Weighted)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create Combined Analysis Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Creating Combined Dose Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Merge all dose measurements\n",
    "dose_comparison_df = crrt_analysis_df[['encounter_block', 'crrt_dose_ml_kg_hr']].copy()\n",
    "dose_comparison_df = dose_comparison_df.rename(columns={'crrt_dose_ml_kg_hr': 'dose_at_initiation'})\n",
    "\n",
    "dose_comparison_df = dose_comparison_df.merge(\n",
    "    dose_6h_summary[['encounter_block', 'time_weighted_dose', 'n_measurements']],\n",
    "    on='encounter_block',\n",
    "    how='left',\n",
    "    suffixes=('', '_6h')\n",
    ").rename(columns={'time_weighted_dose': 'dose_6h_avg', 'n_measurements': 'n_measurements_6h'})\n",
    "\n",
    "dose_comparison_df = dose_comparison_df.merge(\n",
    "    dose_24h_summary[['encounter_block', 'time_weighted_dose', 'n_measurements']],\n",
    "    on='encounter_block',\n",
    "    how='left',\n",
    "    suffixes=('', '_24h')\n",
    ").rename(columns={'time_weighted_dose': 'dose_24h_avg', 'n_measurements': 'n_measurements_24h'})\n",
    "\n",
    "print(f\"\\n   Combined dataset created:\")\n",
    "print(f\"     Total encounters: {len(dose_comparison_df):,}\")\n",
    "print(f\"     Has initiation dose: {dose_comparison_df['dose_at_initiation'].notna().sum():,}\")\n",
    "print(f\"     Has 6h avg dose: {dose_comparison_df['dose_6h_avg'].notna().sum():,}\")\n",
    "print(f\"     Has 24h avg dose: {dose_comparison_df['dose_24h_avg'].notna().sum():,}\")\n",
    "\n",
    "# Check correlation between time windows\n",
    "print(f\"\\n   Correlation between dose measurements:\")\n",
    "print(dose_comparison_df[['dose_at_initiation', 'dose_6h_avg', 'dose_24h_avg']].corr())\n",
    "\n",
    "print(\"\\n✅ Time-weighted dose analysis completed!\")\n",
    "print(f\"   Datasets created: 'dose_6h_summary', 'dose_24h_summary', 'dose_comparison_df'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d8eb7",
   "metadata": {},
   "source": [
    "# Weight and DFR window availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec8cdc",
   "metadata": {},
   "source": [
    "## Weight data availability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1a15d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WEIGHT AVAILABILITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total encounters in cohort: 2,247\n",
      "\n",
      "1. WEIGHT AVAILABILITY ACROSS ENTIRE HOSPITALIZATION\n",
      "--------------------------------------------------------------------------------\n",
      "Encounters with ANY weight recorded: 2,242 (99.8%)\n",
      "Encounters with NO weight at all: 5 (0.2%)\n",
      "\n",
      "Weight records per encounter:\n",
      "  Mean: 20.1\n",
      "  Median: 8\n",
      "  Min-Max: 1 - 1459\n",
      "\n",
      "Distribution:\n",
      "  1 weight: 120\n",
      "  2-5 weights: 744\n",
      "  6-10 weights: 406\n",
      "  >10 weights: 972\n",
      "\n",
      "2. WEIGHT AVAILABILITY BEFORE CRRT INITIATION\n",
      "--------------------------------------------------------------------------------\n",
      "Encounters with weight BEFORE CRRT: 2,190 (97.5%)\n",
      "Encounters WITHOUT weight before CRRT: 57 (2.5%)\n",
      "\n",
      "3. DETAILED BREAKDOWN\n",
      "--------------------------------------------------------------------------------\n",
      "Category 1 - NO weight recorded (entire hospitalization): 5\n",
      "Category 2 - Has weight but ONLY AFTER CRRT start: 52\n",
      "Category 3 - Has weight BEFORE CRRT start: 2,190\n",
      "\n",
      "Verification: 5 + 52 + 2190 = 2247 (should equal 2247)\n",
      "\n",
      "4. TIMING ANALYSIS - Weights recorded AFTER CRRT start\n",
      "--------------------------------------------------------------------------------\n",
      "First weight appears (hours after CRRT start):\n",
      "  Mean: 15.2 hours\n",
      "  Median: 9.8 hours\n",
      "  Range: 0.3 - 54.0 hours\n",
      "\n",
      "Distribution:\n",
      "  Within 6 hours: 18\n",
      "  6-24 hours: 20\n",
      "  >24 hours: 14\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"WEIGHT AVAILABILITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all encounters from cohort\n",
    "cohort_encounters = set(cohort_df['encounter_block'].unique())\n",
    "print(f\"\\nTotal encounters in cohort: {len(cohort_encounters):,}\")\n",
    "\n",
    "# Check weight availability in wide_df (entire hospitalization)\n",
    "print(\"\\n1. WEIGHT AVAILABILITY ACROSS ENTIRE HOSPITALIZATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get all weight records\n",
    "all_weight_data = wide_df[['encounter_block', 'recorded_dttm', 'weight_kg']].copy()\n",
    "all_weight_data = all_weight_data[all_weight_data['weight_kg'].notna()]\n",
    "\n",
    "# Count encounters with ANY weight recorded\n",
    "encounters_with_any_weight = set(all_weight_data['encounter_block'].unique())\n",
    "encounters_without_any_weight = cohort_encounters - encounters_with_any_weight\n",
    "\n",
    "print(f\"Encounters with ANY weight recorded: {len(encounters_with_any_weight):,} ({len(encounters_with_any_weight)/len(cohort_encounters)*100:.1f}%)\")\n",
    "print(f\"Encounters with NO weight at all: {len(encounters_without_any_weight):,} ({len(encounters_without_any_weight)/len(cohort_encounters)*100:.1f}%)\")\n",
    "\n",
    "# Check weight counts per encounter\n",
    "weight_counts = all_weight_data.groupby('encounter_block').size()\n",
    "print(f\"\\nWeight records per encounter:\")\n",
    "print(f\"  Mean: {weight_counts.mean():.1f}\")\n",
    "print(f\"  Median: {weight_counts.median():.0f}\")\n",
    "print(f\"  Min-Max: {weight_counts.min():.0f} - {weight_counts.max():.0f}\")\n",
    "\n",
    "# Distribution of weight record counts\n",
    "print(f\"\\nDistribution:\")\n",
    "print(f\"  1 weight: {(weight_counts == 1).sum():,}\")\n",
    "print(f\"  2-5 weights: {((weight_counts >= 2) & (weight_counts <= 5)).sum():,}\")\n",
    "print(f\"  6-10 weights: {((weight_counts >= 6) & (weight_counts <= 10)).sum():,}\")\n",
    "print(f\"  >10 weights: {(weight_counts > 10).sum():,}\")\n",
    "\n",
    "# 2. WEIGHT BEFORE CRRT INITIATION\n",
    "print(\"\\n2. WEIGHT AVAILABILITY BEFORE CRRT INITIATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Merge weight data with index times\n",
    "weight_with_index = all_weight_data.merge(\n",
    "    index_times[['encounter_block', 'index_time']],\n",
    "    on='encounter_block',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Filter to weights BEFORE or AT index time\n",
    "weight_before_crrt = weight_with_index[weight_with_index['recorded_dttm'] <= weight_with_index['index_time']]\n",
    "\n",
    "encounters_with_weight_before = set(weight_before_crrt['encounter_block'].unique())\n",
    "encounters_without_weight_before = cohort_encounters - encounters_with_weight_before\n",
    "\n",
    "print(f\"Encounters with weight BEFORE CRRT: {len(encounters_with_weight_before):,} ({len(encounters_with_weight_before)/len(cohort_encounters)*100:.1f}%)\")\n",
    "print(f\"Encounters WITHOUT weight before CRRT: {len(encounters_without_weight_before):,} ({len(encounters_without_weight_before)/len(cohort_encounters)*100:.1f}%)\")\n",
    "\n",
    "# 3. DETAILED BREAKDOWN\n",
    "print(\"\\n3. DETAILED BREAKDOWN\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Category 1: No weight at all\n",
    "cat1 = encounters_without_any_weight\n",
    "print(f\"Category 1 - NO weight recorded (entire hospitalization): {len(cat1):,}\")\n",
    "\n",
    "# Category 2: Has weight but ONLY after CRRT initiation\n",
    "cat2 = encounters_with_any_weight - encounters_with_weight_before\n",
    "print(f\"Category 2 - Has weight but ONLY AFTER CRRT start: {len(cat2):,}\")\n",
    "\n",
    "# Category 3: Has weight before CRRT\n",
    "cat3 = encounters_with_weight_before\n",
    "print(f\"Category 3 - Has weight BEFORE CRRT start: {len(cat3):,}\")\n",
    "\n",
    "print(f\"\\nVerification: {len(cat1)} + {len(cat2)} + {len(cat3)} = {len(cat1) + len(cat2) + len(cat3)} (should equal {len(cohort_encounters)})\")\n",
    "\n",
    "# 4. TIMING ANALYSIS FOR CATEGORY 2\n",
    "if len(cat2) > 0:\n",
    "    print(\"\\n4. TIMING ANALYSIS - Weights recorded AFTER CRRT start\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # For category 2 encounters, find when first weight appears\n",
    "    cat2_weights = all_weight_data[all_weight_data['encounter_block'].isin(cat2)].merge(\n",
    "        index_times[['encounter_block', 'index_time']],\n",
    "        on='encounter_block'\n",
    "    )\n",
    "    \n",
    "    cat2_weights['time_after_crrt_hours'] = (cat2_weights['recorded_dttm'] - cat2_weights['index_time']).dt.total_seconds() / 3600\n",
    "    \n",
    "    first_weight_after = cat2_weights.groupby('encounter_block')['time_after_crrt_hours'].min()\n",
    "    \n",
    "    print(f\"First weight appears (hours after CRRT start):\")\n",
    "    print(f\"  Mean: {first_weight_after.mean():.1f} hours\")\n",
    "    print(f\"  Median: {first_weight_after.median():.1f} hours\")\n",
    "    print(f\"  Range: {first_weight_after.min():.1f} - {first_weight_after.max():.1f} hours\")\n",
    "    \n",
    "    print(f\"\\nDistribution:\")\n",
    "    print(f\"  Within 6 hours: {(first_weight_after <= 6).sum():,}\")\n",
    "    print(f\"  6-24 hours: {((first_weight_after > 6) & (first_weight_after <= 24)).sum():,}\")\n",
    "    print(f\"  >24 hours: {(first_weight_after > 24).sum():,}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8135c",
   "metadata": {},
   "source": [
    "## DFR availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ffee62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOSE CALCULATION AVAILABILITY ANALYSIS\n",
      "Examining when CRRT dose can be calculated: At initiation, within 6h, or within 24h\n",
      "================================================================================\n",
      "\n",
      "Total encounters in cohort: 2,247\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS BY TIME WINDOW\n",
      "================================================================================\n",
      "\n",
      "At Initiation (exact match):\n",
      "--------------------------------------------------------------------------------\n",
      "  Total CRRT records in window: 2,247\n",
      "  Unique encounters with CRRT data: 2,247\n",
      "  Encounters with ANY flow rate: 799 (35.6%)\n",
      "  Encounters with BOTH weight AND flow: 776 (34.5%)\n",
      "\n",
      "Within 1 hour:\n",
      "--------------------------------------------------------------------------------\n",
      "  Total CRRT records in window: 4,276\n",
      "  Unique encounters with CRRT data: 2,247\n",
      "  Encounters with ANY flow rate: 1,855 (82.6%)\n",
      "  Encounters with BOTH weight AND flow: 1,806 (80.4%)\n",
      "\n",
      "Within 6 hours:\n",
      "--------------------------------------------------------------------------------\n",
      "  Total CRRT records in window: 13,909\n",
      "  Unique encounters with CRRT data: 2,247\n",
      "  Encounters with ANY flow rate: 2,048 (91.1%)\n",
      "  Encounters with BOTH weight AND flow: 1,996 (88.8%)\n",
      "\n",
      "Within 24 hours:\n",
      "--------------------------------------------------------------------------------\n",
      "  Total CRRT records in window: 44,231\n",
      "  Unique encounters with CRRT data: 2,247\n",
      "  Encounters with ANY flow rate: 2,129 (94.7%)\n",
      "  Encounters with BOTH weight AND flow: 2,077 (92.4%)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Time Window                    Has CRRT     Has Flows    Dose Ready   %       \n",
      "--------------------------------------------------------------------------------\n",
      "At Initiation (exact match)    2,247        799          776          34.5   %\n",
      "Within 1 hour                  2,247        1,855        1,806        80.4   %\n",
      "Within 6 hours                 2,247        2,048        1,996        88.8   %\n",
      "Within 24 hours                2,247        2,129        2,077        92.4   %\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DOSE CALCULATION AVAILABILITY ANALYSIS\")\n",
    "print(\"Examining when CRRT dose can be calculated: At initiation, within 6h, or within 24h\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all CRRT records for cohort encounters\n",
    "cohort_encounters = set(cohort_df['encounter_block'].unique())\n",
    "print(f\"\\nTotal encounters in cohort: {len(cohort_encounters):,}\")\n",
    "\n",
    "# Merge CRRT data with index times\n",
    "crrt_with_timing = crrt_cohort.merge(\n",
    "    index_times[['encounter_block', 'index_time']],\n",
    "    on='encounter_block',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Calculate time from CRRT initiation\n",
    "crrt_with_timing['hours_from_initiation'] = (\n",
    "    crrt_with_timing['recorded_dttm'] - crrt_with_timing['index_time']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Define time windows\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS BY TIME WINDOW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "time_windows = [\n",
    "    ('At Initiation (exact match)', 0, 0),\n",
    "    ('Within 1 hour', 0, 1),\n",
    "    ('Within 6 hours', 0, 6),\n",
    "    ('Within 24 hours', 0, 24),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for window_name, start_hr, end_hr in time_windows:\n",
    "    print(f\"\\n{window_name}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Filter CRRT records to time window\n",
    "    if start_hr == end_hr == 0:\n",
    "        # Exact match at initiation\n",
    "        window_crrt = crrt_with_timing[crrt_with_timing['hours_from_initiation'] == 0].copy()\n",
    "    else:\n",
    "        window_crrt = crrt_with_timing[\n",
    "            (crrt_with_timing['hours_from_initiation'] >= start_hr) & \n",
    "            (crrt_with_timing['hours_from_initiation'] <= end_hr)\n",
    "        ].copy()\n",
    "    \n",
    "    print(f\"  Total CRRT records in window: {len(window_crrt):,}\")\n",
    "    print(f\"  Unique encounters with CRRT data: {window_crrt['encounter_block'].nunique():,}\")\n",
    "    \n",
    "    # Check for flow rates\n",
    "    window_crrt['has_dfr'] = window_crrt['dialysate_flow_rate'] > 0\n",
    "    window_crrt['has_pre_rfr'] = window_crrt['pre_filter_replacement_fluid_rate'] > 0\n",
    "    window_crrt['has_post_rfr'] = window_crrt['post_filter_replacement_fluid_rate'] > 0\n",
    "    window_crrt['has_any_flow'] = (\n",
    "        window_crrt['has_dfr'] | \n",
    "        window_crrt['has_pre_rfr'] | \n",
    "        window_crrt['has_post_rfr']\n",
    "    )\n",
    "    \n",
    "    # Count encounters with flow rates\n",
    "    encounters_with_flows = window_crrt[window_crrt['has_any_flow']]['encounter_block'].unique()\n",
    "    print(f\"  Encounters with ANY flow rate: {len(encounters_with_flows):,} ({len(encounters_with_flows)/len(cohort_encounters)*100:.1f}%)\")\n",
    "    \n",
    "    # Get first record with flows for each encounter\n",
    "    first_with_flows = (window_crrt[window_crrt['has_any_flow']]\n",
    "                       .sort_values(['encounter_block', 'recorded_dttm'])\n",
    "                       .groupby('encounter_block')\n",
    "                       .first()\n",
    "                       .reset_index())\n",
    "    \n",
    "    # Merge with weights (using closest weight before or at that time)\n",
    "    combined_temp = first_with_flows.merge(\n",
    "        index_times[['encounter_block', 'index_time']], \n",
    "        on='encounter_block'\n",
    "    )\n",
    "    \n",
    "    # Get weights available at that time\n",
    "    weight_at_time = all_weight_data.merge(\n",
    "        combined_temp[['encounter_block', 'recorded_dttm']], \n",
    "        on='encounter_block'\n",
    "    )\n",
    "    weight_at_time = weight_at_time[\n",
    "        weight_at_time['recorded_dttm_x'] <= weight_at_time['recorded_dttm_y']\n",
    "    ]\n",
    "    \n",
    "    # Get most recent weight for each encounter\n",
    "    weights_available = (weight_at_time\n",
    "                        .sort_values(['encounter_block', 'recorded_dttm_x'])\n",
    "                        .groupby('encounter_block')\n",
    "                        .last()\n",
    "                        .reset_index())\n",
    "    \n",
    "    # Merge flows with weights\n",
    "    dose_ready = first_with_flows.merge(\n",
    "        weights_available[['encounter_block', 'weight_kg']],\n",
    "        on='encounter_block',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Calculate dose\n",
    "    dose_ready = dose_ready[dose_ready['crrt_mode_category'].notna()].copy()\n",
    "    dose_ready['crrt_mode_category'] = dose_ready['crrt_mode_category'].str.lower()\n",
    "    \n",
    "    # Calculate total flow\n",
    "    dose_ready[flow_cols] = dose_ready[flow_cols].fillna(0)\n",
    "    \n",
    "    conditions = [\n",
    "        dose_ready['crrt_mode_category'] == 'cvvhd',\n",
    "        dose_ready['crrt_mode_category'] == 'cvvh',\n",
    "        dose_ready['crrt_mode_category'] == 'cvvhdf'\n",
    "    ]\n",
    "    \n",
    "    choices = [\n",
    "        dose_ready['dialysate_flow_rate'],\n",
    "        dose_ready['pre_filter_replacement_fluid_rate'] + dose_ready['post_filter_replacement_fluid_rate'],\n",
    "        dose_ready['dialysate_flow_rate'] + dose_ready['pre_filter_replacement_fluid_rate'] + dose_ready['post_filter_replacement_fluid_rate']\n",
    "    ]\n",
    "    \n",
    "    dose_ready['total_flow'] = np.select(conditions, choices, default=np.nan)\n",
    "    \n",
    "    dose_ready['dose_calculable'] = (\n",
    "        (dose_ready['weight_kg'] > 0) & \n",
    "        (dose_ready['total_flow'] > 0)\n",
    "    )\n",
    "    \n",
    "    n_dose_calculable = dose_ready['dose_calculable'].sum()\n",
    "    \n",
    "    print(f\"  Encounters with BOTH weight AND flow: {n_dose_calculable:,} ({n_dose_calculable/len(cohort_encounters)*100:.1f}%)\")\n",
    "    \n",
    "    # Store results\n",
    "    results[window_name] = {\n",
    "        'encounters_with_crrt': window_crrt['encounter_block'].nunique(),\n",
    "        'encounters_with_flows': len(encounters_with_flows),\n",
    "        'encounters_dose_calculable': n_dose_calculable,\n",
    "        'percentage': n_dose_calculable/len(cohort_encounters)*100\n",
    "    }\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n{'Time Window':<30} {'Has CRRT':<12} {'Has Flows':<12} {'Dose Ready':<12} {'%':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for window_name, stats in results.items():\n",
    "    print(f\"{window_name:<30} {stats['encounters_with_crrt']:<12,} {stats['encounters_with_flows']:<12,} {stats['encounters_dose_calculable']:<12,} {stats['percentage']:<7.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31f16dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "crrt_without_flows = crrt_df[~crrt_df['hospitalization_id'].isin(first_with_flows['hospitalization_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "699f6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Analysis: Missing CRRT Dose Within First 6 Hours\n",
      "================================================================================\n",
      "\n",
      "📊 Overall Summary:\n",
      "   Total CRRT encounters: 2,247\n",
      "   Have dose within 6h: 1,993 (88.7%)\n",
      "   Missing dose within 6h: 254 (11.3%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Breakdown: Why is Dose Missing Within First 6 Hours?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total CRRT records within 6h for missing encounters: 1,001\n",
      "\n",
      "1️⃣ No CRRT records within 6h: 0 encounters\n",
      "\n",
      "2️⃣ Missing weight: 57 encounters\n",
      "\n",
      "3️⃣ Have weight but missing/zero flow rates: 197 encounters\n",
      "   Total records with weight: 633\n",
      "\n",
      "   Flow Rate Availability (among records with weight):\n",
      "     dialysate_flow_rate: 0/633 records (0.0%)\n",
      "     pre_filter_replacement_fluid_rate: 0/633 records (0.0%)\n",
      "     post_filter_replacement_fluid_rate: 0/633 records (0.0%)\n",
      "\n",
      "   Records with NO flow rates at all: 0 (0.0%)\n",
      "   Encounters affected: 0\n",
      "\n",
      "4️⃣ Unknown/missing CRRT mode: 148 records\n",
      "   Encounters affected: 30\n",
      "   Modes found: {'scuf': 148}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Creating Dataset of Missing Encounters\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Primary Reasons for Missing Dose:\n",
      "   No flow rates recorded: 197 (77.6%)\n",
      "   Missing weight: 57 (22.4%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Comparing Encounters: Missing vs Available Dose Within 6h\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "✅ Analysis Complete\n",
      "   Dataset created: 'missing_encounters_detailed' (254 encounters)\n",
      "================================================================================\n",
      "\n",
      "Sample of missing encounters:\n",
      "   encounter_block hospitalization_id          missing_reason  \\\n",
      "0           104962          633041921  No flow rates recorded   \n",
      "1           114691          633429395  No flow rates recorded   \n",
      "2           111620          633125137  No flow rates recorded   \n",
      "3           144387          632649695          Missing weight   \n",
      "4            48646          643000722          Missing weight   \n",
      "5           148487          633539797          Missing weight   \n",
      "6            84488          632823365          Missing weight   \n",
      "7           138759          632596413  No flow rates recorded   \n",
      "8           111626          633819139  No flow rates recorded   \n",
      "9           130573          633095417  No flow rates recorded   \n",
      "\n",
      "   has_crrt_records_6h  has_weight  has_any_flow_6h  \n",
      "0                 True        True            False  \n",
      "1                 True        True            False  \n",
      "2                 True        True            False  \n",
      "3                 True       False            False  \n",
      "4                 True       False            False  \n",
      "5                 True       False            False  \n",
      "6                 True       False            False  \n",
      "7                 True        True            False  \n",
      "8                 True        True            False  \n",
      "9                 True        True            False  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Identify Encounters Missing Dose Within First 6 Hours\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Analysis: Missing CRRT Dose Within First 6 Hours\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all CRRT records within 6 hours of initiation with index time\n",
    "crrt_6h = crrt_with_index[\n",
    "    (crrt_with_index['hours_from_index'] >= 0) & \n",
    "    (crrt_with_index['hours_from_index'] <= 6)\n",
    "].copy()\n",
    "\n",
    "# Calculate dose for 6h window (reusing the calculation function from earlier)\n",
    "crrt_6h_doses = calculate_crrt_dose(crrt_6h, closest_weights)\n",
    "\n",
    "# Identify which encounters have at least one valid dose within 6h\n",
    "encounters_with_dose_6h = crrt_6h_doses[\n",
    "    crrt_6h_doses['crrt_dose_ml_kg_hr'].notna()\n",
    "]['encounter_block'].unique()\n",
    "\n",
    "# All encounters in cohort\n",
    "all_encounters = cohort_df['encounter_block'].unique()\n",
    "\n",
    "# Encounters WITHOUT any dose within 6h\n",
    "missing_dose_6h = set(all_encounters) - set(encounters_with_dose_6h)\n",
    "\n",
    "print(f\"\\n📊 Overall Summary:\")\n",
    "print(f\"   Total CRRT encounters: {len(all_encounters):,}\")\n",
    "print(f\"   Have dose within 6h: {len(encounters_with_dose_6h):,} ({len(encounters_with_dose_6h)/len(all_encounters)*100:.1f}%)\")\n",
    "print(f\"   Missing dose within 6h: {len(missing_dose_6h):,} ({len(missing_dose_6h)/len(all_encounters)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Detailed Breakdown of Why Dose is Missing\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Breakdown: Why is Dose Missing Within First 6 Hours?\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create dataframe of missing encounters for analysis\n",
    "missing_encounters_df = pd.DataFrame({'encounter_block': list(missing_dose_6h)})\n",
    "\n",
    "# Get their CRRT records within 6h\n",
    "missing_crrt_6h = crrt_6h[crrt_6h['encounter_block'].isin(missing_dose_6h)].copy()\n",
    "\n",
    "print(f\"\\nTotal CRRT records within 6h for missing encounters: {len(missing_crrt_6h):,}\")\n",
    "\n",
    "# Reason 1: No CRRT records at all within 6h\n",
    "no_records_6h = missing_encounters_df[\n",
    "    ~missing_encounters_df['encounter_block'].isin(missing_crrt_6h['encounter_block'])\n",
    "]\n",
    "print(f\"\\n1️⃣ No CRRT records within 6h: {len(no_records_6h):,} encounters\")\n",
    "\n",
    "if len(missing_crrt_6h) > 0:\n",
    "    # Merge with weights\n",
    "    missing_crrt_6h = missing_crrt_6h.merge(\n",
    "        closest_weights[['encounter_block', 'weight_kg']],\n",
    "        on='encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Reason 2: Missing weight\n",
    "    encounters_no_weight = missing_crrt_6h[\n",
    "        missing_crrt_6h['weight_kg'].isna()\n",
    "    ]['encounter_block'].unique()\n",
    "    print(f\"\\n2️⃣ Missing weight: {len(encounters_no_weight):,} encounters\")\n",
    "    \n",
    "    # For those WITH weight, check flow rates\n",
    "    with_weight = missing_crrt_6h[missing_crrt_6h['weight_kg'].notna()].copy()\n",
    "    \n",
    "    if len(with_weight) > 0:\n",
    "        # Check individual flow components\n",
    "        flow_cols = ['dialysate_flow_rate', 'pre_filter_replacement_fluid_rate', \n",
    "                     'post_filter_replacement_fluid_rate']\n",
    "        \n",
    "        print(f\"\\n3️⃣ Have weight but missing/zero flow rates: {len(with_weight['encounter_block'].unique()):,} encounters\")\n",
    "        print(f\"   Total records with weight: {len(with_weight):,}\")\n",
    "        \n",
    "        # Detailed flow rate analysis\n",
    "        print(f\"\\n   Flow Rate Availability (among records with weight):\")\n",
    "        for col in flow_cols:\n",
    "            available = (with_weight[col] > 0).sum()\n",
    "            print(f\"     {col}: {available:,}/{len(with_weight):,} records ({available/len(with_weight)*100:.1f}%)\")\n",
    "        \n",
    "        # Check if ANY flow rate is present\n",
    "        with_weight['has_any_flow'] = (\n",
    "            (with_weight['dialysate_flow_rate'] > 0) |\n",
    "            (with_weight['pre_filter_replacement_fluid_rate'] > 0) |\n",
    "            (with_weight['post_filter_replacement_fluid_rate'] > 0)\n",
    "        )\n",
    "        \n",
    "        no_flow = with_weight[~with_weight['has_any_flow']]\n",
    "        print(f\"\\n   Records with NO flow rates at all: {len(no_flow):,} ({len(no_flow)/len(with_weight)*100:.1f}%)\")\n",
    "        print(f\"   Encounters affected: {no_flow['encounter_block'].nunique():,}\")\n",
    "        \n",
    "        # Reason 4: Wrong/missing CRRT mode\n",
    "        with_weight['crrt_mode_category'] = with_weight['crrt_mode_category'].str.lower()\n",
    "        unknown_mode = with_weight[\n",
    "            ~with_weight['crrt_mode_category'].isin(['cvvhd', 'cvvh', 'cvvhdf'])\n",
    "        ]\n",
    "        \n",
    "        if len(unknown_mode) > 0:\n",
    "            print(f\"\\n4️⃣ Unknown/missing CRRT mode: {len(unknown_mode):,} records\")\n",
    "            print(f\"   Encounters affected: {unknown_mode['encounter_block'].nunique():,}\")\n",
    "            print(f\"   Modes found: {unknown_mode['crrt_mode_category'].value_counts().to_dict()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create Dataset of Missing Encounters\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Creating Dataset of Missing Encounters\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Merge with cohort data to get patient characteristics\n",
    "missing_encounters_detailed = missing_encounters_df.merge(\n",
    "    cohort_df,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add reason codes\n",
    "missing_encounters_detailed['has_crrt_records_6h'] = missing_encounters_detailed['encounter_block'].isin(\n",
    "    missing_crrt_6h['encounter_block']\n",
    ")\n",
    "\n",
    "missing_encounters_detailed['has_weight'] = missing_encounters_detailed['encounter_block'].isin(\n",
    "    missing_crrt_6h[missing_crrt_6h['weight_kg'].notna()]['encounter_block']\n",
    ")\n",
    "\n",
    "if len(with_weight) > 0:\n",
    "    missing_encounters_detailed['has_any_flow_6h'] = missing_encounters_detailed['encounter_block'].isin(\n",
    "        with_weight[with_weight['has_any_flow']]['encounter_block']\n",
    "    )\n",
    "else:\n",
    "    missing_encounters_detailed['has_any_flow_6h'] = False\n",
    "\n",
    "# Classify primary reason for missing dose\n",
    "def classify_missing_reason(row):\n",
    "    if not row['has_crrt_records_6h']:\n",
    "        return 'No CRRT records in 6h'\n",
    "    elif not row['has_weight']:\n",
    "        return 'Missing weight'\n",
    "    elif not row['has_any_flow_6h']:\n",
    "        return 'No flow rates recorded'\n",
    "    else:\n",
    "        return 'Other (mode/calculation issue)'\n",
    "\n",
    "missing_encounters_detailed['missing_reason'] = missing_encounters_detailed.apply(\n",
    "    classify_missing_reason, axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nPrimary Reasons for Missing Dose:\")\n",
    "reason_counts = missing_encounters_detailed['missing_reason'].value_counts()\n",
    "for reason, count in reason_counts.items():\n",
    "    print(f\"   {reason}: {count:,} ({count/len(missing_encounters_detailed)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Compare Characteristics: Missing vs Available Dose\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Comparing Encounters: Missing vs Available Dose Within 6h\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create comparison groups\n",
    "encounters_available_df = cohort_df[\n",
    "    cohort_df['encounter_block'].isin(encounters_with_dose_6h)\n",
    "].copy()\n",
    "encounters_available_df['dose_available_6h'] = True\n",
    "\n",
    "missing_encounters_detailed['dose_available_6h'] = False\n",
    "\n",
    "comparison_df = pd.concat([\n",
    "    encounters_available_df[['encounter_block', 'dose_available_6h']],\n",
    "    missing_encounters_detailed[['encounter_block', 'dose_available_6h']]\n",
    "])\n",
    "\n",
    "# Merge with any other characteristics you have\n",
    "# For example, if you have SOFA scores:\n",
    "if 'sofa_df' in dir() or 'sofa_df' in locals():\n",
    "    comparison_df = comparison_df.merge(\n",
    "        sofa_df[['encounter_block', 'sofa_total']],\n",
    "        on='encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSOFA Score Comparison:\")\n",
    "    for has_dose in [True, False]:\n",
    "        subset = comparison_df[comparison_df['dose_available_6h'] == has_dose]\n",
    "        sofa_valid = subset['sofa_total'].dropna()\n",
    "        label = \"Dose Available\" if has_dose else \"Dose Missing\"\n",
    "        \n",
    "        if len(sofa_valid) > 0:\n",
    "            print(f\"   {label}:\")\n",
    "            print(f\"     N = {len(sofa_valid):,}\")\n",
    "            print(f\"     Mean SOFA: {sofa_valid.mean():.1f} ± {sofa_valid.std():.1f}\")\n",
    "            print(f\"     Median SOFA: {sofa_valid.median():.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Analysis Complete\")\n",
    "print(f\"   Dataset created: 'missing_encounters_detailed' ({len(missing_encounters_detailed):,} encounters)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export for review\n",
    "print(\"\\nSample of missing encounters:\")\n",
    "print(missing_encounters_detailed[['encounter_block', 'hospitalization_id', \n",
    "                                   'missing_reason', 'has_crrt_records_6h', \n",
    "                                   'has_weight', 'has_any_flow_6h']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4fff580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Detailed CRRT Data Examination: Missing Dose Encounters\n",
      "================================================================================\n",
      "\n",
      "Total encounters to examine: 254\n",
      "\n",
      "📊 CRRT Records Overview:\n",
      "   Total CRRT records: 26,376\n",
      "   Unique encounters: 254\n",
      "   Unique hospitalizations: 254\n",
      "   Date range: 2018-01-16 16:00:00-06:00 to 2024-12-27 04:00:00-06:00\n",
      "\n",
      "   Records per encounter:\n",
      "     Mean: 103.8\n",
      "     Median: 36\n",
      "     Range: 1 - 1944\n",
      "\n",
      "📍 First 6 Hours Only:\n",
      "   CRRT records: 1,001\n",
      "   Encounters with any records: 254\n",
      "   Encounters with NO records in 6h: 0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Data Completeness Analysis (First 6 Hours)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Column                                        Records      % Complete  \n",
      "---------------------------------------------------------------------\n",
      "crrt_mode_category                             1,001 (100.0%)   Non-zero:  1,001 (100.0%)\n",
      "dialysate_flow_rate                              469 ( 46.9%)   Non-zero:    318 ( 31.8%)\n",
      "pre_filter_replacement_fluid_rate                  0 (  0.0%)   Non-zero:      0 (  0.0%)\n",
      "post_filter_replacement_fluid_rate                 0 (  0.0%)   Non-zero:      0 (  0.0%)\n",
      "ultrafiltration_out                              686 ( 68.5%)   Non-zero:    472 ( 47.2%)\n",
      "blood_flow_rate                                  505 ( 50.4%)   Non-zero:    504 ( 50.3%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CRRT Mode Distribution\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode                           Count      %\n",
      "---------------------------------------------\n",
      "cvvhd                          850         84.9%\n",
      "scuf                           151         15.1%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Flow Rate Statistics (First 6 Hours)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "dialysate_flow_rate:\n",
      "   Non-null: 469/1,001 (46.9%)\n",
      "   Non-zero: 318 (31.8%)\n",
      "   Mean: 4597.3 mL/hr\n",
      "   Median: 3425.0 mL/hr\n",
      "   Range: 300.0 - 238000.0 mL/hr\n",
      "\n",
      "pre_filter_replacement_fluid_rate:\n",
      "   Non-null: 0/1,001 (0.0%)\n",
      "   Non-zero: 0 (0.0%)\n",
      "\n",
      "post_filter_replacement_fluid_rate:\n",
      "   Non-null: 0/1,001 (0.0%)\n",
      "   Non-zero: 0 (0.0%)\n",
      "\n",
      "ultrafiltration_out:\n",
      "   Non-null: 686/1,001 (68.5%)\n",
      "   Non-zero: 472 (47.2%)\n",
      "   Mean: 113.8 mL/hr\n",
      "   Median: 99.5 mL/hr\n",
      "   Range: 0.1 - 1300.0 mL/hr\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Sample Records for Manual Review\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Showing records from first 10 encounters:\n",
      "       encounter_block hospitalization_id             recorded_dttm  hours_from_index crrt_mode_category  dialysate_flow_rate  pre_filter_replacement_fluid_rate  post_filter_replacement_fluid_rate  ultrafiltration_out  blood_flow_rate\n",
      "15254             2008          632484115 2021-06-21 05:35:00-05:00          0.000000              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "22953             2008          632484115 2021-06-21 06:00:00-05:00          0.416667              cvvhd                  NaN                               <NA>                                <NA>                 44.0            250.0\n",
      "17107             2008          632484115 2021-06-21 07:00:00-05:00          1.416667              cvvhd                  NaN                               <NA>                                <NA>                100.0            250.0\n",
      "124               2008          632484115 2021-06-21 08:00:00-05:00          2.416667              cvvhd                  NaN                               <NA>                                <NA>                100.0            250.0\n",
      "5108              6461          632759835 2020-06-19 20:35:00-05:00          0.000000              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "23964             6461          632759835 2020-06-19 21:00:00-05:00          0.416667               scuf                  0.0                               <NA>                                <NA>                 22.0            250.0\n",
      "24346             6461          632759835 2020-06-19 22:00:00-05:00          1.416667               scuf                  0.0                               <NA>                                <NA>                 57.0              NaN\n",
      "1889              6461          632759835 2020-06-19 23:00:00-05:00          2.416667               scuf                  0.0                               <NA>                                <NA>                111.0              NaN\n",
      "23978             6461          632759835 2020-06-20 00:00:00-05:00          3.416667               scuf                  0.0                               <NA>                                <NA>                116.0              NaN\n",
      "175               6461          632759835 2020-06-20 01:00:00-05:00          4.416667               scuf                  0.0                               <NA>                                <NA>                181.0              NaN\n",
      "12430             6461          632759835 2020-06-20 02:00:00-05:00          5.416667               scuf                  0.0                               <NA>                                <NA>                236.0              NaN\n",
      "190              24720          646700760 2022-06-10 21:15:00-05:00          0.000000              cvvhd                  NaN                               <NA>                                <NA>                  NaN            270.0\n",
      "188              54998          642465172 2024-11-19 23:00:00-06:00          0.000000              cvvhd               1080.0                               <NA>                                <NA>                  0.0              NaN\n",
      "2918             54998          642465172 2024-11-20 00:00:00-06:00          1.000000              cvvhd               3930.0                               <NA>                                <NA>                  9.0              NaN\n",
      "6281             54998          642465172 2024-11-20 01:00:00-06:00          2.000000              cvvhd               3890.0                               <NA>                                <NA>                111.0              NaN\n",
      "909              54998          642465172 2024-11-20 02:00:00-06:00          3.000000              cvvhd               3820.0                               <NA>                                <NA>                110.0              NaN\n",
      "18286            54998          642465172 2024-11-20 03:00:00-06:00          4.000000              cvvhd               3880.0                               <NA>                                <NA>                111.0              NaN\n",
      "6805             54998          642465172 2024-11-20 04:00:00-06:00          5.000000              cvvhd               3850.0                               <NA>                                <NA>                111.0              NaN\n",
      "23823            54998          642465172 2024-11-20 05:00:00-06:00          6.000000              cvvhd               3490.0                               <NA>                                <NA>                106.0              NaN\n",
      "66               80925          633085881 2020-03-19 19:00:00-05:00          0.000000              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "148             118753          632815053 2022-09-12 20:45:00-05:00          0.000000              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "13208           118753          632815053 2022-09-12 21:00:00-05:00          0.250000              cvvhd                900.0                               <NA>                                <NA>                 18.0            250.0\n",
      "8683            118753          632815053 2022-09-12 22:00:00-05:00          1.250000              cvvhd               3480.0                               <NA>                                <NA>                 74.0              NaN\n",
      "13994           118753          632815053 2022-09-12 23:00:00-05:00          2.250000              cvvhd               3050.0                               <NA>                                <NA>                 65.0              NaN\n",
      "902             118753          632815053 2022-09-12 23:45:00-05:00          3.000000              cvvhd                  NaN                               <NA>                                <NA>                  NaN            280.0\n",
      "5523            118753          632815053 2022-09-13 00:00:00-05:00          3.250000              cvvhd               1270.0                               <NA>                                <NA>                 27.0              NaN\n",
      "21552           118753          632815053 2022-09-13 01:00:00-05:00          4.250000              cvvhd               3260.0                               <NA>                                <NA>                 69.0              NaN\n",
      "1185            118753          632815053 2022-09-13 02:00:00-05:00          5.250000              cvvhd               3410.0                               <NA>                                <NA>                 73.0              NaN\n",
      "60              124118          632600281 2018-12-10 09:05:00-06:00          0.000000              cvvhd                  NaN                               <NA>                                <NA>                  NaN              NaN\n",
      "2706            124118          632600281 2018-12-10 10:40:00-06:00          1.583333              cvvhd                  NaN                               <NA>                                <NA>                  NaN              NaN\n",
      "5140            144585          632924727 2020-08-26 20:20:00-05:00          0.000000              cvvhd                  NaN                               <NA>                                <NA>                  NaN            220.0\n",
      "3020            144585          632924727 2020-08-26 21:00:00-05:00          0.666667              cvvhd               1970.0                               <NA>                                <NA>                  0.0              NaN\n",
      "3182            144585          632924727 2020-08-26 22:00:00-05:00          1.666667              cvvhd               1950.0                               <NA>                                <NA>                 14.0              NaN\n",
      "19898           144585          632924727 2020-08-26 23:00:00-05:00          2.666667              cvvhd               1950.0                               <NA>                                <NA>                 66.0              NaN\n",
      "12662           144585          632924727 2020-08-27 00:00:00-05:00          3.666667              cvvhd               1950.0                               <NA>                                <NA>                 96.0              NaN\n",
      "2760            144585          632924727 2020-08-27 01:00:00-05:00          4.666667              cvvhd               1920.0                               <NA>                                <NA>                 95.0              NaN\n",
      "157             144585          632924727 2020-08-27 02:00:00-05:00          5.666667              cvvhd               1920.0                               <NA>                                <NA>                 95.0              NaN\n",
      "100             156690          632750969 2022-09-14 07:05:00-05:00          0.000000              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "209             156690          632750969 2022-09-14 08:00:00-05:00          0.916667              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "19056           162169          633474305 2022-10-17 14:20:00-05:00          0.000000              cvvhd                  NaN                               <NA>                                <NA>                  NaN            260.0\n",
      "3157            162169          633474305 2022-10-17 15:00:00-05:00          0.666667               scuf                  0.0                               <NA>                                <NA>                  0.0            260.0\n",
      "13661           162169          633474305 2022-10-17 16:00:00-05:00          1.666667               scuf                  0.0                               <NA>                                <NA>                 10.0            260.0\n",
      "24              162169          633474305 2022-10-17 17:00:00-05:00          2.666667               scuf                  0.0                               <NA>                                <NA>                 10.0            260.0\n",
      "12545           162169          633474305 2022-10-17 18:00:00-05:00          3.666667               scuf                  0.0                               <NA>                                <NA>                 23.0            260.0\n",
      "7935            162169          633474305 2022-10-17 19:00:00-05:00          4.666667               scuf                  0.0                               <NA>                                <NA>                 23.0            260.0\n",
      "10159           162169          633474305 2022-10-17 19:28:00-05:00          5.133333              cvvhd                  NaN                               <NA>                                <NA>                  NaN            260.0\n",
      "11500           162169          633474305 2022-10-17 20:00:00-05:00          5.666667               scuf                  0.0                               <NA>                                <NA>                 27.0            260.0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Problematic Patterns Identified\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1️⃣ Encounters with records but NO flow rates: 231\n",
      "   Total records affected: 683\n",
      "\n",
      "2️⃣ Encounters with missing CRRT mode: 0\n",
      "   Total records affected: 0\n",
      "\n",
      "================================================================================\n",
      "✅ Analysis Complete\n",
      "================================================================================\n",
      "\n",
      "Datasets created:\n",
      "   'missing_crrt_full' - All CRRT records for missing encounters (26,376 records)\n",
      "   'missing_crrt_6h_only' - First 6h only (1,001 records)\n",
      "   'sample_records' - Sample for review (47 records)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Examine CRRT Data for Encounters Missing Dose Within 6 Hours\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Detailed CRRT Data Examination: Missing Dose Encounters\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get list of encounters missing dose\n",
    "missing_encounter_list = missing_encounters_detailed['encounter_block'].unique()\n",
    "\n",
    "print(f\"\\nTotal encounters to examine: {len(missing_encounter_list):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Filter CRRT data for these encounters\n",
    "# ============================================================================\n",
    "\n",
    "# All CRRT records for missing encounters\n",
    "missing_crrt_full = clif.crrt_therapy.df[\n",
    "    clif.crrt_therapy.df['encounter_block'].isin(missing_encounter_list)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\n📊 CRRT Records Overview:\")\n",
    "print(f\"   Total CRRT records: {len(missing_crrt_full):,}\")\n",
    "print(f\"   Unique encounters: {missing_crrt_full['encounter_block'].nunique():,}\")\n",
    "print(f\"   Unique hospitalizations: {missing_crrt_full['hospitalization_id'].nunique():,}\")\n",
    "print(f\"   Date range: {missing_crrt_full['recorded_dttm'].min()} to {missing_crrt_full['recorded_dttm'].max()}\")\n",
    "\n",
    "# Records per encounter\n",
    "records_per_encounter = missing_crrt_full.groupby('encounter_block').size()\n",
    "print(f\"\\n   Records per encounter:\")\n",
    "print(f\"     Mean: {records_per_encounter.mean():.1f}\")\n",
    "print(f\"     Median: {records_per_encounter.median():.0f}\")\n",
    "print(f\"     Range: {records_per_encounter.min()} - {records_per_encounter.max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Focus on first 6 hours only\n",
    "# ============================================================================\n",
    "\n",
    "# Add index time to filter for first 6 hours\n",
    "missing_crrt_full = missing_crrt_full.merge(\n",
    "    index_times,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "missing_crrt_full['hours_from_index'] = (\n",
    "    missing_crrt_full['recorded_dttm'] - missing_crrt_full['index_time']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Filter for first 6 hours\n",
    "missing_crrt_6h_only = missing_crrt_full[\n",
    "    (missing_crrt_full['hours_from_index'] >= 0) & \n",
    "    (missing_crrt_full['hours_from_index'] <= 6)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\n📍 First 6 Hours Only:\")\n",
    "print(f\"   CRRT records: {len(missing_crrt_6h_only):,}\")\n",
    "print(f\"   Encounters with any records: {missing_crrt_6h_only['encounter_block'].nunique():,}\")\n",
    "print(f\"   Encounters with NO records in 6h: {len(missing_encounter_list) - missing_crrt_6h_only['encounter_block'].nunique():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Analyze Data Completeness\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Data Completeness Analysis (First 6 Hours)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "key_columns = [\n",
    "    'crrt_mode_category',\n",
    "    'dialysate_flow_rate',\n",
    "    'pre_filter_replacement_fluid_rate',\n",
    "    'post_filter_replacement_fluid_rate',\n",
    "    'ultrafiltration_out',\n",
    "    'blood_flow_rate'\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Column':<45} {'Records':<12} {'% Complete':<12}\")\n",
    "print(\"-\" * 69)\n",
    "\n",
    "for col in key_columns:\n",
    "    if col in missing_crrt_6h_only.columns:\n",
    "        available = missing_crrt_6h_only[col].notna().sum()\n",
    "        non_zero = (missing_crrt_6h_only[col] > 0).sum() if missing_crrt_6h_only[col].dtype in ['float64', 'int64', 'Float64', 'Int64'] else available\n",
    "        total = len(missing_crrt_6h_only)\n",
    "        \n",
    "        print(f\"{col:<45} {available:>6,} ({available/total*100:>5.1f}%)   Non-zero: {non_zero:>6,} ({non_zero/total*100:>5.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{col:<45} Column not found\")\n",
    "\n",
    "# ============================================================================\n",
    "# CRRT Mode Distribution\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CRRT Mode Distribution\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(missing_crrt_6h_only) > 0:\n",
    "    mode_counts = missing_crrt_6h_only['crrt_mode_category'].value_counts(dropna=False)\n",
    "    print(f\"\\n{'Mode':<30} {'Count':<10} {'%'}\")\n",
    "    print(\"-\" * 45)\n",
    "    for mode, count in mode_counts.items():\n",
    "        mode_str = str(mode) if pd.notna(mode) else 'Missing/NaN'\n",
    "        print(f\"{mode_str:<30} {count:<10,} {count/len(missing_crrt_6h_only)*100:>5.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# Flow Rate Statistics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Flow Rate Statistics (First 6 Hours)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "flow_cols = ['dialysate_flow_rate', 'pre_filter_replacement_fluid_rate', \n",
    "             'post_filter_replacement_fluid_rate', 'ultrafiltration_out']\n",
    "\n",
    "for col in flow_cols:\n",
    "    if col in missing_crrt_6h_only.columns:\n",
    "        valid = missing_crrt_6h_only[col].dropna()\n",
    "        non_zero = valid[valid > 0]\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"   Non-null: {len(valid):,}/{len(missing_crrt_6h_only):,} ({len(valid)/len(missing_crrt_6h_only)*100:.1f}%)\")\n",
    "        print(f\"   Non-zero: {len(non_zero):,} ({len(non_zero)/len(missing_crrt_6h_only)*100:.1f}%)\")\n",
    "        \n",
    "        if len(non_zero) > 0:\n",
    "            print(f\"   Mean: {non_zero.mean():.1f} mL/hr\")\n",
    "            print(f\"   Median: {non_zero.median():.1f} mL/hr\")\n",
    "            print(f\"   Range: {non_zero.min():.1f} - {non_zero.max():.1f} mL/hr\")\n",
    "\n",
    "# ============================================================================\n",
    "# Sample Records for Manual Review\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Sample Records for Manual Review\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Select columns for review\n",
    "review_cols = [\n",
    "    'encounter_block', 'hospitalization_id', 'recorded_dttm', 'hours_from_index',\n",
    "    'crrt_mode_category', 'dialysate_flow_rate', \n",
    "    'pre_filter_replacement_fluid_rate', 'post_filter_replacement_fluid_rate',\n",
    "    'ultrafiltration_out', 'blood_flow_rate'\n",
    "]\n",
    "\n",
    "# Filter to columns that exist\n",
    "review_cols_exist = [col for col in review_cols if col in missing_crrt_6h_only.columns]\n",
    "\n",
    "# Get sample from first 10 encounters\n",
    "sample_encounters = missing_crrt_6h_only['encounter_block'].unique()[:10]\n",
    "sample_records = missing_crrt_6h_only[\n",
    "    missing_crrt_6h_only['encounter_block'].isin(sample_encounters)\n",
    "][review_cols_exist].sort_values(['encounter_block', 'hours_from_index'])\n",
    "\n",
    "print(f\"\\nShowing records from first 10 encounters:\")\n",
    "print(sample_records.to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# Identify Specific Problematic Patterns\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Problematic Patterns Identified\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(missing_crrt_6h_only) > 0:\n",
    "    # Pattern 1: Records with no flow rates at all\n",
    "    no_flow_mask = (\n",
    "        (missing_crrt_6h_only['dialysate_flow_rate'].fillna(0) == 0) &\n",
    "        (missing_crrt_6h_only['pre_filter_replacement_fluid_rate'].fillna(0) == 0) &\n",
    "        (missing_crrt_6h_only['post_filter_replacement_fluid_rate'].fillna(0) == 0)\n",
    "    )\n",
    "    no_flow_encounters = missing_crrt_6h_only[no_flow_mask]['encounter_block'].nunique()\n",
    "    print(f\"\\n1️⃣ Encounters with records but NO flow rates: {no_flow_encounters:,}\")\n",
    "    print(f\"   Total records affected: {no_flow_mask.sum():,}\")\n",
    "    \n",
    "    # Pattern 2: Missing CRRT mode\n",
    "    missing_mode = missing_crrt_6h_only['crrt_mode_category'].isna()\n",
    "    missing_mode_encounters = missing_crrt_6h_only[missing_mode]['encounter_block'].nunique()\n",
    "    print(f\"\\n2️⃣ Encounters with missing CRRT mode: {missing_mode_encounters:,}\")\n",
    "    print(f\"   Total records affected: {missing_mode.sum():,}\")\n",
    "    \n",
    "    # Pattern 3: Unrecognized modes\n",
    "    valid_modes = ['cvvhd', 'cvvh', 'cvvhdf', 'scuf']\n",
    "    if 'crrt_mode_category' in missing_crrt_6h_only.columns:\n",
    "        invalid_mode_mask = (\n",
    "            missing_crrt_6h_only['crrt_mode_category'].notna() &\n",
    "            ~missing_crrt_6h_only['crrt_mode_category'].str.lower().isin(valid_modes)\n",
    "        )\n",
    "        if invalid_mode_mask.sum() > 0:\n",
    "            invalid_modes = missing_crrt_6h_only[invalid_mode_mask]['crrt_mode_category'].unique()\n",
    "            print(f\"\\n3️⃣ Unrecognized CRRT modes found: {invalid_modes}\")\n",
    "            print(f\"   Total records affected: {invalid_mode_mask.sum():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Export for Detailed Review\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Analysis Complete\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDatasets created:\")\n",
    "print(f\"   'missing_crrt_full' - All CRRT records for missing encounters ({len(missing_crrt_full):,} records)\")\n",
    "print(f\"   'missing_crrt_6h_only' - First 6h only ({len(missing_crrt_6h_only):,} records)\")\n",
    "print(f\"   'sample_records' - Sample for review ({len(sample_records):,} records)\")\n",
    "\n",
    "# Optionally save to CSV for external review\n",
    "# missing_crrt_6h_only.to_csv('output/missing_dose_crrt_records_6h.csv', index=False)\n",
    "# print(\"\\n💾 Exported to: output/missing_dose_crrt_records_6h.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0723ed06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08995433789954338"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "197/2190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27c1a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Adding Weight Data to Missing CRRT Records\n",
      "================================================================================\n",
      "\n",
      "✅ Weight column added to 'missing_crrt_full'\n",
      "\n",
      "   Weight Availability:\n",
      "     Total records: 26,376\n",
      "     Records with weight: 16,717 (63.4%)\n",
      "     Records missing weight: 9,659 (36.6%)\n",
      "\n",
      "   By Unique Encounters:\n",
      "     Total encounters: 254\n",
      "     Encounters with weight: 197 (77.6%)\n",
      "     Encounters missing weight: 57 (22.4%)\n",
      "\n",
      "   Weight Statistics (kg):\n",
      "     Mean ± SD: 96.8 ± 27.7\n",
      "     Median [IQR]: 89.3 [79.6-108.1]\n",
      "     Range: 45.0 - 210.0\n",
      "\n",
      "✅ Weight column also added to 'missing_crrt_6h_only'\n",
      "     Records with weight: 633 (63.2%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Sample Records with Weight (First 6 Hours)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Showing records from first 10 encounters (with weight):\n",
      "     encounter_block hospitalization_id             recorded_dttm  hours_from_index   weight_kg crrt_mode_category  dialysate_flow_rate  pre_filter_replacement_fluid_rate  post_filter_replacement_fluid_rate  ultrafiltration_out  blood_flow_rate\n",
      "590             2008          632484115 2021-06-21 05:35:00-05:00          0.000000   94.298750              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "862             2008          632484115 2021-06-21 06:00:00-05:00          0.416667   94.298750              cvvhd                  NaN                               <NA>                                <NA>                 44.0            250.0\n",
      "649             2008          632484115 2021-06-21 07:00:00-05:00          1.416667   94.298750              cvvhd                  NaN                               <NA>                                <NA>                100.0            250.0\n",
      "4               2008          632484115 2021-06-21 08:00:00-05:00          2.416667   94.298750              cvvhd                  NaN                               <NA>                                <NA>                100.0            250.0\n",
      "214             6461          632759835 2020-06-19 20:35:00-05:00          0.000000  116.098750              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "914             6461          632759835 2020-06-19 21:00:00-05:00          0.416667  116.098750               scuf                  0.0                               <NA>                                <NA>                 22.0            250.0\n",
      "923             6461          632759835 2020-06-19 22:00:00-05:00          1.416667  116.098750               scuf                  0.0                               <NA>                                <NA>                 57.0              NaN\n",
      "86              6461          632759835 2020-06-19 23:00:00-05:00          2.416667  116.098750               scuf                  0.0                               <NA>                                <NA>                111.0              NaN\n",
      "915             6461          632759835 2020-06-20 00:00:00-05:00          3.416667  116.098750               scuf                  0.0                               <NA>                                <NA>                116.0              NaN\n",
      "7               6461          632759835 2020-06-20 01:00:00-05:00          4.416667  116.098750               scuf                  0.0                               <NA>                                <NA>                181.0              NaN\n",
      "472             6461          632759835 2020-06-20 02:00:00-05:00          5.416667  116.098750               scuf                  0.0                               <NA>                                <NA>                236.0              NaN\n",
      "9              24720          646700760 2022-06-10 21:15:00-05:00          0.000000   81.399375              cvvhd                  NaN                               <NA>                                <NA>                  NaN            270.0\n",
      "8              54998          642465172 2024-11-19 23:00:00-06:00          0.000000         NaN              cvvhd               1080.0                               <NA>                                <NA>                  0.0              NaN\n",
      "123            54998          642465172 2024-11-20 00:00:00-06:00          1.000000         NaN              cvvhd               3930.0                               <NA>                                <NA>                  9.0              NaN\n",
      "259            54998          642465172 2024-11-20 01:00:00-06:00          2.000000         NaN              cvvhd               3890.0                               <NA>                                <NA>                111.0              NaN\n",
      "43             54998          642465172 2024-11-20 02:00:00-06:00          3.000000         NaN              cvvhd               3820.0                               <NA>                                <NA>                110.0              NaN\n",
      "695            54998          642465172 2024-11-20 03:00:00-06:00          4.000000         NaN              cvvhd               3880.0                               <NA>                                <NA>                111.0              NaN\n",
      "277            54998          642465172 2024-11-20 04:00:00-06:00          5.000000         NaN              cvvhd               3850.0                               <NA>                                <NA>                111.0              NaN\n",
      "910            54998          642465172 2024-11-20 05:00:00-06:00          6.000000         NaN              cvvhd               3490.0                               <NA>                                <NA>                106.0              NaN\n",
      "2              80925          633085881 2020-03-19 19:00:00-05:00          0.000000   77.999375              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "5             118753          632815053 2022-09-12 20:45:00-05:00          0.000000         NaN              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "507           118753          632815053 2022-09-12 21:00:00-05:00          0.250000         NaN              cvvhd                900.0                               <NA>                                <NA>                 18.0            250.0\n",
      "347           118753          632815053 2022-09-12 22:00:00-05:00          1.250000         NaN              cvvhd               3480.0                               <NA>                                <NA>                 74.0              NaN\n",
      "542           118753          632815053 2022-09-12 23:00:00-05:00          2.250000         NaN              cvvhd               3050.0                               <NA>                                <NA>                 65.0              NaN\n",
      "42            118753          632815053 2022-09-12 23:45:00-05:00          3.000000         NaN              cvvhd                  NaN                               <NA>                                <NA>                  NaN            280.0\n",
      "233           118753          632815053 2022-09-13 00:00:00-05:00          3.250000         NaN              cvvhd               1270.0                               <NA>                                <NA>                 27.0              NaN\n",
      "819           118753          632815053 2022-09-13 01:00:00-05:00          4.250000         NaN              cvvhd               3260.0                               <NA>                                <NA>                 69.0              NaN\n",
      "57            118753          632815053 2022-09-13 02:00:00-05:00          5.250000         NaN              cvvhd               3410.0                               <NA>                                <NA>                 73.0              NaN\n",
      "1             124118          632600281 2018-12-10 09:05:00-06:00          0.000000   92.898750              cvvhd                  NaN                               <NA>                                <NA>                  NaN              NaN\n",
      "114           124118          632600281 2018-12-10 10:40:00-06:00          1.583333   92.898750              cvvhd                  NaN                               <NA>                                <NA>                  NaN              NaN\n",
      "217           144585          632924727 2020-08-26 20:20:00-05:00          0.000000         NaN              cvvhd                  NaN                               <NA>                                <NA>                  NaN            220.0\n",
      "126           144585          632924727 2020-08-26 21:00:00-05:00          0.666667         NaN              cvvhd               1970.0                               <NA>                                <NA>                  0.0              NaN\n",
      "134           144585          632924727 2020-08-26 22:00:00-05:00          1.666667         NaN              cvvhd               1950.0                               <NA>                                <NA>                 14.0              NaN\n",
      "756           144585          632924727 2020-08-26 23:00:00-05:00          2.666667         NaN              cvvhd               1950.0                               <NA>                                <NA>                 66.0              NaN\n",
      "479           144585          632924727 2020-08-27 00:00:00-05:00          3.666667         NaN              cvvhd               1950.0                               <NA>                                <NA>                 96.0              NaN\n",
      "116           144585          632924727 2020-08-27 01:00:00-05:00          4.666667         NaN              cvvhd               1920.0                               <NA>                                <NA>                 95.0              NaN\n",
      "6             144585          632924727 2020-08-27 02:00:00-05:00          5.666667         NaN              cvvhd               1920.0                               <NA>                                <NA>                 95.0              NaN\n",
      "3             156690          632750969 2022-09-14 07:05:00-05:00          0.000000   68.099375              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "11            156690          632750969 2022-09-14 08:00:00-05:00          0.916667   68.099375              cvvhd                  NaN                               <NA>                                <NA>                  NaN            250.0\n",
      "723           162169          633474305 2022-10-17 14:20:00-05:00          0.000000   85.399375              cvvhd                  NaN                               <NA>                                <NA>                  NaN            260.0\n",
      "132           162169          633474305 2022-10-17 15:00:00-05:00          0.666667   85.399375               scuf                  0.0                               <NA>                                <NA>                  0.0            260.0\n",
      "526           162169          633474305 2022-10-17 16:00:00-05:00          1.666667   85.399375               scuf                  0.0                               <NA>                                <NA>                 10.0            260.0\n",
      "0             162169          633474305 2022-10-17 17:00:00-05:00          2.666667   85.399375               scuf                  0.0                               <NA>                                <NA>                 10.0            260.0\n",
      "477           162169          633474305 2022-10-17 18:00:00-05:00          3.666667   85.399375               scuf                  0.0                               <NA>                                <NA>                 23.0            260.0\n",
      "314           162169          633474305 2022-10-17 19:00:00-05:00          4.666667   85.399375               scuf                  0.0                               <NA>                                <NA>                 23.0            260.0\n",
      "391           162169          633474305 2022-10-17 19:28:00-05:00          5.133333   85.399375              cvvhd                  NaN                               <NA>                                <NA>                  NaN            260.0\n",
      "437           162169          633474305 2022-10-17 20:00:00-05:00          5.666667   85.399375               scuf                  0.0                               <NA>                                <NA>                 27.0            260.0\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Add Weight Column to Missing CRRT Data\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Adding Weight Data to Missing CRRT Records\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Merge the pre-CRRT weights (closest_weights) with missing_crrt_full\n",
    "missing_crrt_full = missing_crrt_full.merge(\n",
    "    closest_weights[['encounter_block', 'weight_kg']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Weight column added to 'missing_crrt_full'\")\n",
    "print(f\"\\n   Weight Availability:\")\n",
    "print(f\"     Total records: {len(missing_crrt_full):,}\")\n",
    "print(f\"     Records with weight: {missing_crrt_full['weight_kg'].notna().sum():,} ({missing_crrt_full['weight_kg'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"     Records missing weight: {missing_crrt_full['weight_kg'].isna().sum():,} ({missing_crrt_full['weight_kg'].isna().mean()*100:.1f}%)\")\n",
    "\n",
    "# By unique encounters\n",
    "print(f\"\\n   By Unique Encounters:\")\n",
    "weight_by_encounter = missing_crrt_full.groupby('encounter_block')['weight_kg'].first()\n",
    "print(f\"     Total encounters: {len(weight_by_encounter):,}\")\n",
    "print(f\"     Encounters with weight: {weight_by_encounter.notna().sum():,} ({weight_by_encounter.notna().mean()*100:.1f}%)\")\n",
    "print(f\"     Encounters missing weight: {weight_by_encounter.isna().sum():,} ({weight_by_encounter.isna().mean()*100:.1f}%)\")\n",
    "\n",
    "# Weight statistics for those that have it\n",
    "if missing_crrt_full['weight_kg'].notna().sum() > 0:\n",
    "    weight_stats = missing_crrt_full['weight_kg'].describe()\n",
    "    print(f\"\\n   Weight Statistics (kg):\")\n",
    "    print(f\"     Mean ± SD: {weight_stats['mean']:.1f} ± {weight_stats['std']:.1f}\")\n",
    "    print(f\"     Median [IQR]: {weight_stats['50%']:.1f} [{weight_stats['25%']:.1f}-{weight_stats['75%']:.1f}]\")\n",
    "    print(f\"     Range: {weight_stats['min']:.1f} - {weight_stats['max']:.1f}\")\n",
    "\n",
    "# Also update missing_crrt_6h_only with weight\n",
    "missing_crrt_6h_only = missing_crrt_6h_only.merge(\n",
    "    closest_weights[['encounter_block', 'weight_kg']],\n",
    "    on='encounter_block',\n",
    "    how='left',\n",
    "    suffixes=('_old', '')  # In case weight was already there\n",
    ")\n",
    "\n",
    "# Remove old weight column if it exists\n",
    "if 'weight_kg_old' in missing_crrt_6h_only.columns:\n",
    "    missing_crrt_6h_only = missing_crrt_6h_only.drop(columns=['weight_kg_old'])\n",
    "\n",
    "print(f\"\\n✅ Weight column also added to 'missing_crrt_6h_only'\")\n",
    "print(f\"     Records with weight: {missing_crrt_6h_only['weight_kg'].notna().sum():,} ({missing_crrt_6h_only['weight_kg'].notna().mean()*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Updated Sample with Weight Column\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Sample Records with Weight (First 6 Hours)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Updated review columns including weight\n",
    "review_cols = [\n",
    "    'encounter_block', 'hospitalization_id', 'recorded_dttm', 'hours_from_index',\n",
    "    'weight_kg',  # <-- Added weight\n",
    "    'crrt_mode_category', 'dialysate_flow_rate', \n",
    "    'pre_filter_replacement_fluid_rate', 'post_filter_replacement_fluid_rate',\n",
    "    'ultrafiltration_out', 'blood_flow_rate'\n",
    "]\n",
    "\n",
    "# Filter to columns that exist\n",
    "review_cols_exist = [col for col in review_cols if col in missing_crrt_6h_only.columns]\n",
    "\n",
    "# Get sample from first 10 encounters\n",
    "sample_encounters = missing_crrt_6h_only['encounter_block'].unique()[:10]\n",
    "sample_records_with_weight = missing_crrt_6h_only[\n",
    "    missing_crrt_6h_only['encounter_block'].isin(sample_encounters)\n",
    "][review_cols_exist].sort_values(['encounter_block', 'hours_from_index'])\n",
    "\n",
    "print(f\"\\nShowing records from first 10 encounters (with weight):\")\n",
    "print(sample_records_with_weight.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb5b7a1",
   "metadata": {},
   "source": [
    "## Labs availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f20f611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Extracting Labs at CRRT Initiation\n",
      "================================================================================\n",
      "\n",
      "Total lab records in wide_df: 2,044,721\n",
      "Lab records for cohort encounters: 2,044,721\n",
      "\n",
      "Labs at exact index time: 1,270\n",
      "\n",
      "Lab availability at index time:\n",
      "  ph_arterial: 7 (0.3%)\n",
      "  lactate: 13 (0.6%)\n",
      "  bicarbonate: 13 (0.6%)\n",
      "  potassium: 10 (0.4%)\n",
      "\n",
      "✅ Labs at initiation extracted: 1,270 encounters\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Total encounters: 2,247\n",
      "Encounters with lab data: 1,270\n",
      "\n",
      "Lab Statistics (at/near CRRT initiation):\n",
      "\n",
      "ph_arterial:\n",
      "  Count: 7\n",
      "  Mean ± SD: 7.21 ± 0.14\n",
      "  Median [IQR]: 7.24 [7.11-7.32]\n",
      "  Range: 7.00 - 7.35\n",
      "\n",
      "lactate:\n",
      "  Count: 13\n",
      "  Mean ± SD: 9.22 ± 4.83\n",
      "  Median [IQR]: 9.30 [5.60-13.00]\n",
      "  Range: 2.20 - 16.50\n",
      "\n",
      "bicarbonate:\n",
      "  Count: 13\n",
      "  Mean ± SD: 21.30 ± 9.03\n",
      "  Median [IQR]: 21.00 [18.00-24.00]\n",
      "  Range: 5.90 - 44.00\n",
      "\n",
      "potassium:\n",
      "  Count: 10\n",
      "  Mean ± SD: 4.93 ± 1.08\n",
      "  Median [IQR]: 4.70 [4.22-5.42]\n",
      "  Range: 3.50 - 7.30\n",
      "\n",
      "✅ Labs at initiation dataset created: 'labs_at_initiation'\n",
      "\n",
      "Sample:\n",
      "      encounter_block                index_time  ph_arterial  lactate  \\\n",
      "3259             8407 2021-06-24 21:30:00-05:00          NaN      NaN   \n",
      "5113            13338 2021-06-29 05:30:00-05:00          NaN      NaN   \n",
      "5274             8927 2022-04-06 16:00:00-05:00          NaN      NaN   \n",
      "6393            13947 2022-01-01 20:00:00-06:00          NaN      NaN   \n",
      "7184            13419 2020-10-09 06:00:00-05:00          NaN      NaN   \n",
      "\n",
      "      bicarbonate  potassium  \n",
      "3259          NaN        NaN  \n",
      "5113          NaN        NaN  \n",
      "5274          NaN        NaN  \n",
      "6393          NaN        NaN  \n",
      "7184          NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Extracting Labs at CRRT Initiation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define labs we want to extract\n",
    "labs_to_extract = ['ph_arterial', 'lactate', 'bicarbonate', 'potassium']\n",
    "\n",
    "# Extract lab data with encounter_block\n",
    "lab_data = wide_df[['encounter_block', 'recorded_dttm'] + labs_to_extract].copy()\n",
    "\n",
    "print(f\"\\nTotal lab records in wide_df: {len(lab_data):,}\")\n",
    "\n",
    "# Merge with index times\n",
    "labs_with_index = lab_data.merge(\n",
    "    index_times,\n",
    "    on='encounter_block',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Lab records for cohort encounters: {len(labs_with_index):,}\")\n",
    "\n",
    "# Get labs at exact index time\n",
    "labs_at_index = labs_with_index[\n",
    "    labs_with_index['recorded_dttm'] == labs_with_index['index_time']\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nLabs at exact index time: {len(labs_at_index):,}\")\n",
    "\n",
    "# Check availability\n",
    "print(f\"\\nLab availability at index time:\")\n",
    "for lab in labs_to_extract:\n",
    "    available = labs_at_index[lab].notna().sum()\n",
    "    print(f\"  {lab}: {available:,} ({available/len(index_times)*100:.1f}%)\")\n",
    "\n",
    "# Create final labs dataset\n",
    "labs_at_initiation = labs_at_index[['encounter_block', 'index_time'] + labs_to_extract].copy()\n",
    "\n",
    "print(f\"\\n✅ Labs at initiation extracted: {len(labs_at_initiation):,} encounters\")\n",
    "\n",
    "# If exact match gives too few results, use closest within window\n",
    "if len(labs_at_initiation) < len(index_times) * 0.5:  # Less than 50%\n",
    "    print(\"\\n⚠️  Low coverage at exact time. Extracting closest labs within 6-hour window...\")\n",
    "    \n",
    "    # Get labs within 6 hours before or after index time\n",
    "    labs_with_index['time_diff_hours'] = abs(\n",
    "        (labs_with_index['recorded_dttm'] - labs_with_index['index_time']).dt.total_seconds() / 3600\n",
    "    )\n",
    "    \n",
    "    labs_near_index = labs_with_index[labs_with_index['time_diff_hours'] <= 6].copy()\n",
    "    \n",
    "    print(f\"Lab records within 6-hour window: {len(labs_near_index):,}\")\n",
    "    \n",
    "    # For each encounter and each lab, get the closest value\n",
    "    closest_labs = []\n",
    "    \n",
    "    for encounter in index_times['encounter_block']:\n",
    "        encounter_labs = labs_near_index[labs_near_index['encounter_block'] == encounter]\n",
    "        \n",
    "        if len(encounter_labs) == 0:\n",
    "            continue\n",
    "        \n",
    "        lab_values = {'encounter_block': encounter}\n",
    "        \n",
    "        for lab in labs_to_extract:\n",
    "            # Get records where this lab is not null\n",
    "            lab_records = encounter_labs[encounter_labs[lab].notna()]\n",
    "            \n",
    "            if len(lab_records) > 0:\n",
    "                # Get the closest one\n",
    "                closest_idx = lab_records['time_diff_hours'].idxmin()\n",
    "                lab_values[lab] = lab_records.loc[closest_idx, lab]\n",
    "            else:\n",
    "                lab_values[lab] = np.nan\n",
    "        \n",
    "        closest_labs.append(lab_values)\n",
    "    \n",
    "    labs_at_initiation = pd.DataFrame(closest_labs)\n",
    "    \n",
    "    # Merge back with index_time\n",
    "    labs_at_initiation = labs_at_initiation.merge(\n",
    "        index_times[['encounter_block', 'index_time']],\n",
    "        on='encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nUpdated lab availability (within 6-hour window):\")\n",
    "    for lab in labs_to_extract:\n",
    "        available = labs_at_initiation[lab].notna().sum()\n",
    "        print(f\"  {lab}: {available:,} ({available/len(index_times)*100:.1f}%)\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total encounters: {len(index_times):,}\")\n",
    "print(f\"Encounters with lab data: {len(labs_at_initiation):,}\")\n",
    "\n",
    "print(f\"\\nLab Statistics (at/near CRRT initiation):\")\n",
    "for lab in labs_to_extract:\n",
    "    lab_values = labs_at_initiation[lab].dropna()\n",
    "    if len(lab_values) > 0:\n",
    "        print(f\"\\n{lab}:\")\n",
    "        print(f\"  Count: {len(lab_values):,}\")\n",
    "        print(f\"  Mean ± SD: {lab_values.mean():.2f} ± {lab_values.std():.2f}\")\n",
    "        print(f\"  Median [IQR]: {lab_values.median():.2f} [{lab_values.quantile(0.25):.2f}-{lab_values.quantile(0.75):.2f}]\")\n",
    "        print(f\"  Range: {lab_values.min():.2f} - {lab_values.max():.2f}\")\n",
    "\n",
    "print(\"\\n✅ Labs at initiation dataset created: 'labs_at_initiation'\")\n",
    "print(f\"\\nSample:\")\n",
    "print(labs_at_initiation.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f055d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85163f19",
   "metadata": {},
   "source": [
    "# SOFA Score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44ac3ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ClifOrchestrator for SOFA computation...\n",
      "ClifOrchestrator initialized.\n",
      "✅ ClifOrchestrator initialized\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing ClifOrchestrator for SOFA computation...\")\n",
    "co = ClifOrchestrator(\n",
    "    data_directory=config['tables_path'],\n",
    "    filetype=config['file_type'],\n",
    "    timezone=config['timezone']\n",
    ")\n",
    "print(\"✅ ClifOrchestrator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c910bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_cohort_df = index_times.copy()\n",
    "sofa_cohort_df['start_time'] = sofa_cohort_df['index_time']\n",
    "sofa_cohort_df['end_time'] = sofa_cohort_df['index_time'] + pd.Timedelta(hours=6)\n",
    "\n",
    "# Join with cohort_df to get hospitalization_id\n",
    "sofa_cohort_df = sofa_cohort_df.merge(\n",
    "    cohort_df[['encounter_block', 'hospitalization_id']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Keep only required columns\n",
    "sofa_cohort_df = sofa_cohort_df[['hospitalization_id','encounter_block', 'start_time', 'end_time']]\n",
    "sofa_cohort_ids = cohort_df['hospitalization_id'].astype(str).unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01db5455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required tables for SOFA computation...\n",
      "SOFA requires: Labs (creatinine, platelet_count, po2_arterial, bilirubin_total)\n",
      "               Vitals (map, spo2)\n",
      "               Assessments (gcs_total)\n",
      "               Medications (norepinephrine, epinephrine, dopamine, dobutamine)\n",
      "               Respiratory (device_category, fio2_set)\n",
      "Loading labs with 5 columns and 2247 hospitalization filters...\n",
      "Loading vitals with 4 columns and 2247 hospitalization filters...\n",
      "Loading patient_assessments with 4 columns and 2247 hospitalization filters...\n",
      "Loading medication_admin_continuous with all columns and 2247 hospitalization filters...\n",
      "Loading respiratory_support with all columns and 2247 hospitalization filters...\n",
      "✅ All required tables loaded for SOFA computation\n"
     ]
    }
   ],
   "source": [
    "# Load required tables for SOFA computation with cohort filtering\n",
    "print(\"Loading required tables for SOFA computation...\")\n",
    "print(\"SOFA requires: Labs (creatinine, platelet_count, po2_arterial, bilirubin_total)\")\n",
    "print(\"               Vitals (map, spo2)\")\n",
    "print(\"               Assessments (gcs_total)\")\n",
    "print(\"               Medications (norepinephrine, epinephrine, dopamine, dobutamine)\")\n",
    "print(\"               Respiratory (device_category, fio2_set)\")\n",
    "\n",
    "# Define columns to load for each table (optimize memory usage)\n",
    "sofa_columns = {\n",
    "    'labs': ['hospitalization_id', 'lab_result_dttm', 'lab_category', 'lab_value', 'lab_value_numeric'],\n",
    "    'vitals': ['hospitalization_id', 'recorded_dttm', 'vital_category', 'vital_value'],\n",
    "    'patient_assessments': ['hospitalization_id', 'recorded_dttm', 'assessment_category', 'numerical_value'],\n",
    "    'medication_admin_continuous': None,  # Load all columns\n",
    "    'respiratory_support': None  # Load all columns\n",
    "}\n",
    "\n",
    "sofa_tables = ['labs', 'vitals', 'patient_assessments', 'medication_admin_continuous', 'respiratory_support']\n",
    "\n",
    "for table_name in sofa_tables:\n",
    "    table_cols = sofa_columns.get(table_name)\n",
    "    print(f\"Loading {table_name} with {len(table_cols) if table_cols else 'all'} columns and {len(sofa_cohort_ids)} hospitalization filters...\")\n",
    "    co.load_table(\n",
    "        table_name,\n",
    "        filters={'hospitalization_id': sofa_cohort_ids},\n",
    "        columns=table_cols\n",
    "    )\n",
    "\n",
    "co.encounter_mapping = encounter_mapping[encounter_mapping['hospitalization_id'].isin(cohort_df['hospitalization_id'])]\n",
    "print(\"✅ All required tables loaded for SOFA computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfca2037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting medication units to mcg/kg/min for SOFA...\n",
      "Converting 4 medications: ['norepinephrine', 'epinephrine', 'dopamine', 'dobutamine']\n",
      "No weight_kg column found, adding the most recent from vitals\n",
      "Cannot accommodate the conversion to the following preferred units: {'meq/hr', 'meq/kg/hr', 'ppm'}. Consult the function documentation for a list of acceptable units.\n",
      "\n",
      "=== Conversion Summary ===\n",
      "Total conversion records: 89\n",
      "Successful conversions: 1,497,656 / 1,530,705 (97.8%)\n",
      "\n",
      "⚠️ Found 21 conversion issues:\n",
      "  angiotensin: ng/kg/min → cannot convert to a weighted unit if weight_kg is missing (253 records)\n",
      "  argatroban: mcg/kg/min → cannot convert to a weighted unit if weight_kg is missing (1 records)\n",
      "  cangrelor: mcg/kg/min → cannot convert to a weighted unit if weight_kg is missing (8 records)\n",
      "  cisatracurium: mcg/kg/min → cannot convert to a weighted unit if weight_kg is missing (42 records)\n",
      "  dexmedetomidine: mcg/kg/hr → cannot convert to a weighted unit if weight_kg is missing (358 records)\n",
      "  dobutamine: mcg/kg/min → cannot convert to a weighted unit if weight_kg is missing (310 records)\n",
      "  dopamine: mcg/kg/min → cannot convert to a weighted unit if weight_kg is missing (104 records)\n",
      "  epinephrine: mcg/kg/min → cannot convert to a weighted unit if weight_kg is missing (458 records)\n",
      "  epoprostenol: ng/kg/min → cannot convert to a weighted unit if weight_kg is missing (1 records)\n",
      "  esmolol: mcg/kg/min → cannot convert to a weighted unit if weight_kg is missing (28 records)\n",
      "\n",
      "✅ Medication unit conversion completed\n"
     ]
    }
   ],
   "source": [
    "# Convert medication units to mcg/kg/min for SOFA computation\n",
    "print(\"Converting medication units to mcg/kg/min for SOFA...\")\n",
    "\n",
    "# Define preferred units for SOFA medications\n",
    "preferred_units = {\n",
    "    'norepinephrine': 'mcg/kg/min',\n",
    "    'epinephrine': 'mcg/kg/min',\n",
    "    'dopamine': 'mcg/kg/min',\n",
    "    'dobutamine': 'mcg/kg/min'\n",
    "}\n",
    "\n",
    "print(f\"Converting {len(preferred_units)} medications: {list(preferred_units.keys())}\")\n",
    "\n",
    "# Convert units (uses vitals table for weight data)\n",
    "co.convert_dose_units_for_continuous_meds(\n",
    "    preferred_units=preferred_units,\n",
    "    override = True, \n",
    "    save_to_table=True  # Saves to co.medication_admin_continuous.df_converted\n",
    ")\n",
    "\n",
    "# Check conversion results\n",
    "conversion_counts = co.medication_admin_continuous.conversion_counts\n",
    "\n",
    "print(\"\\n=== Conversion Summary ===\")\n",
    "print(f\"Total conversion records: {len(conversion_counts):,}\")\n",
    "\n",
    "# Check for conversion failures\n",
    "success_count = conversion_counts[conversion_counts['_convert_status'] == 'success']['count'].sum()\n",
    "total_count = conversion_counts['count'].sum()\n",
    "\n",
    "print(f\"Successful conversions: {success_count:,} / {total_count:,} ({100*success_count/total_count:.1f}%)\")\n",
    "\n",
    "# Show any failed conversions\n",
    "failed_conversions = conversion_counts[conversion_counts['_convert_status'] != 'success']\n",
    "if len(failed_conversions) > 0:\n",
    "    print(f\"\\n⚠️ Found {len(failed_conversions)} conversion issues:\")\n",
    "    for _, row in failed_conversions.head(10).iterrows():\n",
    "        print(f\"  {row['med_category']}: {row['_clean_unit']} → {row['_convert_status']} ({row['count']} records)\")\n",
    "else:\n",
    "    print(\"✅ All conversions successful!\")\n",
    "\n",
    "print(\"\\n✅ Medication unit conversion completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "685be409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hospitalization_id in meds df: 2225\n"
     ]
    }
   ],
   "source": [
    "meds = co.medication_admin_continuous.df\n",
    "unique_hosp_ids = meds['hospitalization_id'].nunique()\n",
    "print(f\"Unique hospitalization_id in meds df: {unique_hosp_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8bb38488",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Series' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msofa_cohort_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'Series' object is not callable"
     ]
    }
   ],
   "source": [
    "sofa_cohort_df.dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_scores = co.compute_sofa_scores(\n",
    "    cohort_df=sofa_cohort_df,\n",
    "    id_name='encounter_block'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55babf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SOFA Score Summary Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SOFA Score Summary for CRRT Cohort\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Assuming your SOFA dataframe is called 'sofa_df' or similar\n",
    "# Replace 'sofa_df' with your actual dataframe name\n",
    "sofa_df = sofa_scores.copy()\n",
    "print(f\"\\n📊 Dataset Overview:\")\n",
    "print(f\"   Total encounters: {len(sofa_df):,}\")\n",
    "print(f\"   Total columns: {sofa_df.shape[1]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Completeness Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Data Completeness\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "sofa_columns = ['sofa_cv_97', 'sofa_coag', 'sofa_liver', 'sofa_resp', \n",
    "                'sofa_cns', 'sofa_renal', 'sofa_total']\n",
    "\n",
    "print(f\"\\n{'Component':<20} {'Available':<15} {'Missing':<15} {'% Complete':<12}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for col in sofa_columns:\n",
    "    available = sofa_df[col].notna().sum()\n",
    "    missing = sofa_df[col].isna().sum()\n",
    "    pct_complete = (available / len(sofa_df)) * 100\n",
    "    print(f\"{col:<20} {available:>6,} ({pct_complete:>5.1f}%)   {missing:>6,} ({100-pct_complete:>5.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SOFA Component Score Distributions\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SOFA Component Score Distributions\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for col in sofa_columns:\n",
    "    valid_scores = sofa_df[col].dropna()\n",
    "    \n",
    "    if len(valid_scores) > 0:\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(f\"   N = {len(valid_scores):,}\")\n",
    "        print(f\"   Mean ± SD: {valid_scores.mean():.2f} ± {valid_scores.std():.2f}\")\n",
    "        print(f\"   Median [IQR]: {valid_scores.median():.1f} [{valid_scores.quantile(0.25):.1f}-{valid_scores.quantile(0.75):.1f}]\")\n",
    "        print(f\"   Range: {valid_scores.min():.0f} - {valid_scores.max():.0f}\")\n",
    "        \n",
    "        # Value counts for component scores\n",
    "        if col != 'sofa_total':  # Component scores are 0-4\n",
    "            print(f\"   Distribution:\")\n",
    "            value_counts = valid_scores.value_counts().sort_index()\n",
    "            for score, count in value_counts.items():\n",
    "                print(f\"     Score {int(score)}: {count:>5,} ({count/len(valid_scores)*100:>5.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Total SOFA Score Categories\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Total SOFA Score Categories\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "valid_total = sofa_df['sofa_total'].dropna()\n",
    "\n",
    "if len(valid_total) > 0:\n",
    "    print(f\"\\nClinical Severity Categories (based on total SOFA):\")\n",
    "    print(f\"   {'Category':<25} {'N':<10} {'%'}\")\n",
    "    print(f\"   {'-'*45}\")\n",
    "    \n",
    "    # Standard SOFA severity categories\n",
    "    categories = [\n",
    "        (\"Low (0-6)\", (valid_total <= 6).sum()),\n",
    "        (\"Moderate (7-9)\", ((valid_total >= 7) & (valid_total <= 9)).sum()),\n",
    "        (\"High (10-12)\", ((valid_total >= 10) & (valid_total <= 12)).sum()),\n",
    "        (\"Very High (13-14)\", ((valid_total >= 13) & (valid_total <= 14)).sum()),\n",
    "        (\"Extreme (≥15)\", (valid_total >= 15).sum())\n",
    "    ]\n",
    "    \n",
    "    for category, count in categories:\n",
    "        pct = (count / len(valid_total)) * 100\n",
    "        print(f\"   {category:<25} {count:<10,} {pct:>5.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# P/F Ratio Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"P/F Ratio Analysis\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "pf_available = sofa_df['p_f'].notna().sum()\n",
    "pf_imputed_available = sofa_df['p_f_imputed'].notna().sum()\n",
    "\n",
    "print(f\"\\nP/F Ratio Availability:\")\n",
    "print(f\"   Original P/F: {pf_available:,} ({pf_available/len(sofa_df)*100:.1f}%)\")\n",
    "print(f\"   Imputed P/F: {pf_imputed_available:,} ({pf_imputed_available/len(sofa_df)*100:.1f}%)\")\n",
    "print(f\"   Imputation helped: {pf_imputed_available - pf_available:,} additional encounters\")\n",
    "\n",
    "if pf_imputed_available > 0:\n",
    "    pf_stats = sofa_df['p_f_imputed'].describe()\n",
    "    print(f\"\\nP/F Ratio Statistics (using imputed):\")\n",
    "    print(f\"   Mean ± SD: {pf_stats['mean']:.1f} ± {pf_stats['std']:.1f}\")\n",
    "    print(f\"   Median [IQR]: {pf_stats['50%']:.1f} [{pf_stats['25%']:.1f}-{pf_stats['75%']:.1f}]\")\n",
    "    print(f\"   Range: {pf_stats['min']:.1f} - {pf_stats['max']:.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Missing Pattern Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Missing Data Patterns\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check how many encounters have complete SOFA scores\n",
    "complete_components = sofa_df[['sofa_cv_97', 'sofa_coag', 'sofa_liver', \n",
    "                                'sofa_resp', 'sofa_cns', 'sofa_renal']].notna().all(axis=1).sum()\n",
    "\n",
    "print(f\"\\nComplete SOFA Assessments:\")\n",
    "print(f\"   All 6 components available: {complete_components:,} ({complete_components/len(sofa_df)*100:.1f}%)\")\n",
    "\n",
    "# Count missing components per encounter\n",
    "missing_per_encounter = sofa_df[['sofa_cv_97', 'sofa_coag', 'sofa_liver', \n",
    "                                  'sofa_resp', 'sofa_cns', 'sofa_renal']].isna().sum(axis=1)\n",
    "\n",
    "print(f\"\\n   Distribution of missing components:\")\n",
    "for n_missing in range(7):\n",
    "    count = (missing_per_encounter == n_missing).sum()\n",
    "    if count > 0:\n",
    "        print(f\"     {n_missing} components missing: {count:,} ({count/len(sofa_df)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Correlation Matrix\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SOFA Component Correlations\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "correlation_cols = ['sofa_cv_97', 'sofa_coag', 'sofa_liver', \n",
    "                    'sofa_resp', 'sofa_cns', 'sofa_renal']\n",
    "corr_matrix = sofa_df[correlation_cols].corr()\n",
    "\n",
    "print(\"\\nCorrelation between SOFA components:\")\n",
    "print(corr_matrix.round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ SOFA Score Summary Complete\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e822752",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_with_sofa = outcomes_df.merge(sofa_scores, on='encounter_block', how='left')\n",
    "outcomes_with_sofa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170325ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".clif_table_one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
