{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cc294e",
   "metadata": {},
   "source": [
    "## CLIF Table One\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "Date v1: May 13, 2025\n",
    "\n",
    "This script identifies the cohort of encounters with at least one ICU stay and then summarizes the cohort data into one table. \n",
    "\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`, `clif_patient_assessments`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **clif_patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **clif_hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`,`discharge_dttm`, `age_at_admission` | - |\n",
    "| **clif_adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category` | - |\n",
    "| **clif_vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | weight_kg |\n",
    "| **clif_labs** | `hospitalization_id`, `lab_result_dttm`, `lab_order_dttm`, `lab_category`, `lab_value_numeric` | creatinine, bilirubin_total, po2_arterial, platelet_count |\n",
    "| **clif_medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin(optional) |\n",
    "| **clif_respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`,  `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs`, `tidal_volume_set`, `pressure_control_set`, `pressure_support_set` | - |\n",
    "| **clif_patient_assessments** | `hospitalization_id`, `recorded_dttm` , `assessment_category`, `numerical_value`| `gcs_total` |\n",
    "| **clif_crrt_therapy** | `hospitalization_id`, `recorded_dttm` | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bf46d",
   "metadata": {},
   "source": [
    "## Cohort Identification\n",
    "\n",
    "\n",
    "## Inclusion \n",
    "1. Adults\n",
    "2. Patients with at least one ICU stay or those who had only emergency department or ward encounters and either died or received life support at any point. Life support is defined as the administration of any vasoactive drugs or respiratory support exceeding low-flow oxygen.\n",
    "\n",
    "Respiratory support device: 'IMV', 'NIPPV', 'CPAP', 'High Flow NC'  \n",
    "\n",
    "Vasoactive: 'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import clifpy\n",
    "import os\n",
    "\n",
    "print(\"=== Environment Verification ===\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"clifpy version: {clifpy.__version__}\")\n",
    "print(f\"clifpy location: {clifpy.__file__}\")\n",
    "\n",
    "print(\"\\n=== Python Path Check ===\")\n",
    "local_clifpy_path = \"/Users/kavenchhikara/Desktop/CLIF/CLIFpy\"\n",
    "if any(local_clifpy_path in path for path in sys.path):\n",
    "    print(\"⚠️  WARNING: Local CLIFpy still in path!\")\n",
    "    for path in sys.path:\n",
    "        if local_clifpy_path in path:\n",
    "            print(f\"   Found: {path}\")\n",
    "else:\n",
    "    print(\"✅ Clean environment - no local CLIFpy in path\")\n",
    "\n",
    "print(f\"\\n=== Working Directory ===\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../config/config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Create the output directory for tableone results if it does not already exist\n",
    "output_dir = Path(\"../output/final/tableone\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n=� Configuration:\")\n",
    "print(f\"   Data directory: {config['tables_path']}\")\n",
    "print(f\"   File type: {config['file_type']}\")\n",
    "print(f\"   Timezone: {config['timezone']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127edcbe",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4319503",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Defining Required Data Elements\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Full patient table \n",
    "\n",
    "# Full hospitalization table \n",
    "\n",
    "# Full ADT table\n",
    "\n",
    "# Vitals\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "# Respiratory Support \n",
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set', \n",
    "    'pressure_control_set',\n",
    "    'pressure_support_set',\n",
    "    'peak_inspiratory_pressure_set',\n",
    "    'peak_inspiratory_pressure_obs',\n",
    "    'plateau_pressure_obs',\n",
    "    'minute_vent_obs'\n",
    "]\n",
    "\n",
    "\n",
    "# Continuous administered meds\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol',\n",
    "    'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', \n",
    "    'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee461357",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7cbe42",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_hourly_sequence(group):\n",
    "#     blk = group.name  # use group name from groupby\n",
    "#     start_time = group['vent_episode_start_dttm'].iloc[0]\n",
    "#     end_time   = group['vent_end_dttm_72h'].iloc[0]\n",
    "#     hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "#     return pd.DataFrame({\n",
    "#         'hospitalization_id': blk,\n",
    "#         'recorded_dttm': hourly_timestamps\n",
    "#     })\n",
    "\n",
    "# def calculate_ibw(height_cm, sex):\n",
    "#     if pd.isna(height_cm) or pd.isna(sex):\n",
    "#         return np.nan\n",
    "#     height_inches = height_cm / 2.54\n",
    "#     sex = str(sex).lower()\n",
    "#     if sex == 'male':\n",
    "#         return 50 + 2.3 * (height_inches - 60)\n",
    "#     elif sex == 'female':\n",
    "#         return 45.5 + 2.3 * (height_inches - 60)\n",
    "#     else:\n",
    "#         return np.nan\n",
    "\n",
    "# def calculate_base_excess(ph, hco3):\n",
    "#     \"\"\"\n",
    "#     Calculate Base Excess using simplified formula\n",
    "#     BE = (HCO3 - 24.4) + (8.3 * (pH - 7.4))\n",
    "#     \"\"\"\n",
    "#     return (hco3 - 24.4) + (8.3 * (ph - 7.4))\n",
    "\n",
    "# def calculate_pf_ratio(po2, fio2):\n",
    "#     \"\"\"\n",
    "#     Vectorized calculation of P/F ratio (PaO2/FiO2)\n",
    "#     FiO2 should be as fraction (0.21-1.0), not percentage\n",
    "#     Handles pandas Series input.\n",
    "#     \"\"\"\n",
    "#     fio2 = fio2.copy()\n",
    "#     # Convert percentage to fraction if needed\n",
    "#     mask_pct = fio2 > 1\n",
    "#     fio2[mask_pct] = fio2[mask_pct] / 100\n",
    "#     # Set minimum fio2 to 0.21 (room air)\n",
    "#     fio2 = fio2.clip(lower=0.21)\n",
    "#     return po2 / fio2\n",
    "\n",
    "# def process_crrt_waterfall(\n",
    "#     crrt: pd.DataFrame,\n",
    "#     *,\n",
    "#     id_col: str = \"hospitalization_id\",\n",
    "#     gap_thresh: Union[str, pd.Timedelta] = \"2h\",\n",
    "#     infer_modes: bool = True,          # infer missing mode from numeric pattern\n",
    "#     flag_missing_bfr: bool = True,     # add QC flag if blood-flow still NaN\n",
    "#     wipe_unused: bool = True,          # null parameters not used by the mode\n",
    "#     fix_islands: bool = True,          # relabel single-row SCUF islands\n",
    "#     verbose: bool = True,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Clean + episode-aware forward-fill for the CLIF `crrt_therapy` table.\n",
    "#     Episode-aware clean-up and forward-fill of the CLIF `crrt_therapy` table.\n",
    "\n",
    "#     The function mirrors the respiratory-support “waterfall” logic but adapts it to\n",
    "#     the quirks of Continuous Renal Replacement Therapy (CRRT):\n",
    "\n",
    "#     1. **Episode detection** - a new `crrt_episode_id` starts whenever  \n",
    "#        • `crrt_mode_category` changes **OR**  \n",
    "#        • the gap between successive rows exceeds *gap_thresh* (default 2 h).\n",
    "#     2. **Numeric forward-fill inside an episode** - fills *only* the parameters\n",
    "#        that are clinically relevant for the active mode.\n",
    "#     3. **Mode-specific wiping** after filling, parameters that are **not used**\n",
    "#        in the current mode (e.g. `dialysate_flow_rate` in SCUF) are nulled so\n",
    "#        stale data never bleed across modes.\n",
    "#     4. **Deduplication & ordering** guarantees exactly **one row per\n",
    "#        `(id_col, recorded_dttm)`**, chronologically ordered.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     crrt : pd.DataFrame\n",
    "#         Raw `crrt_therapy` table **in UTC**. Must contain the schema columns\n",
    "#         defined on the CLIF website (see docstring footer).\n",
    "#     id_col : str, default ``\"hospitalization_id\"``\n",
    "#         Encounter-level identifier.\n",
    "#     gap_thresh : str or pd.Timedelta, default ``\"2h\"``\n",
    "#         Maximum tolerated gap **inside** an episode before a new episode is\n",
    "#         forced. Accepts any pandas-parsable offset string (``\"90min\"``, ``\"3h\"``,\n",
    "#         …) or a ``pd.Timedelta``.\n",
    "#     verbose : bool, default ``True``\n",
    "#         If *True* prints progress banners.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         Processed CRRT DataFrame with\n",
    "\n",
    "#         * ``crrt_episode_id`` (int32) - sequential per encounter,\n",
    "#         * forward-filled numeric parameters **within** each episode,\n",
    "#         * unused parameters blanked per mode,\n",
    "#         * unique, ordered rows ``id_col, recorded_dttm``.\n",
    "\n",
    "#     Add-ons v2.0\n",
    "#     ------------\n",
    "#     • Optional numeric-pattern inference of `crrt_mode_category`.\n",
    "#     • Flags rows that *should* have blood-flow but don't.\n",
    "#     • Optional fix for single-row modality islands (sandwiched rows).\n",
    "#     • Optional wipe vs. keep of parameters not used by the active mode.\n",
    "\n",
    "#     Key steps\n",
    "#     ----------\n",
    "#     0.  Lower-case strings, coerce numerics, **infer** mode when blank.\n",
    "#     1.  **Relabel single-row SCUF islands** (if *fix_islands*).\n",
    "#     2.  Detect `crrt_episode_id` (mode change or >gap_thresh).\n",
    "#     3.  Forward-fill numeric parameters *within* an episode.\n",
    "#     4.  QC flag → `blood_flow_missing_after_ffill` (optional).\n",
    "#     5.  Wipe / flag parameters not valid for the mode (configurable).\n",
    "#     6.  Deduplicate & order ⇒ one row per ``(id_col, recorded_dttm)``.\n",
    "#     \"\"\"\n",
    "#     p = print if verbose else (lambda *_, **__: None)\n",
    "#     gap_thresh = pd.Timedelta(gap_thresh)\n",
    "\n",
    "#     # ───────────── Phase 0 — prep, numeric coercion, optional inference\n",
    "#     p(\"✦ Phase 0: prep & numeric coercion (+optional mode inference)\")\n",
    "#     df = crrt.copy()\n",
    "\n",
    "#     df[\"crrt_mode_category\"] = df[\"crrt_mode_category\"].str.lower()\n",
    "#     # save original dialysate_flow_rate values\n",
    "#     df[\"_orig_df\"] = df[\"dialysate_flow_rate\"]\n",
    "\n",
    "#     # 0a) RAW SCUF DF‐OUT sanity check\n",
    "#     # look for rows that are already labeled “scuf”\n",
    "#     # and that have a non‐zero dialysate_flow_rate in the raw data\n",
    "#     raw_scuf = df[\"crrt_mode_category\"].str.lower() == \"scuf\"\n",
    "#     raw_df_positive = df[\"_orig_df\"].fillna(0) > 0\n",
    "\n",
    "#     n_bad = (raw_scuf & raw_df_positive).sum()\n",
    "#     if n_bad:\n",
    "#         print(f\"!!!  Found {n_bad} raw SCUF rows with dialysate_flow_rate > 0 (should be 0 or NA)\")\n",
    "#         print(\" Converting these mode category to NA, keep recorded numerical values as the ground truth\")\n",
    "#         df.loc[raw_df_positive, \"crrt_mode_category\"] = np.nan\n",
    "#     else:\n",
    "#         print(\"!!! No raw SCUF rows had dialysate_flow_rate > 0\")\n",
    "\n",
    "#     NUM_COLS = [\n",
    "#         \"blood_flow_rate\",\n",
    "#         \"pre_filter_replacement_fluid_rate\",\n",
    "#         \"post_filter_replacement_fluid_rate\",\n",
    "#         \"dialysate_flow_rate\",\n",
    "#         \"ultrafiltration_out\",\n",
    "#     ]\n",
    "#     NUM_COLS = [c for c in NUM_COLS if c in df.columns]\n",
    "#     df[NUM_COLS] = df[NUM_COLS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "#     #  any row whose original ultrafiltration_out was >0 must never be SCUF\n",
    "#     def drop_scuf_on_positive_df(df, p):\n",
    "#         bad_df  = df[\"_orig_df\"].fillna(0) > 0\n",
    "#         scuf_now = df[\"crrt_mode_category\"] == \"scuf\"\n",
    "#         n = (bad_df & scuf_now).sum()\n",
    "#         if n:\n",
    "#             p(f\"→ Removing {n:,} SCUF labels on rows with DF>0\")\n",
    "#             df.loc[bad_df & scuf_now, \"crrt_mode_category\"] = np.nan\n",
    "            \n",
    "\n",
    "#     if infer_modes:\n",
    "#         miss = df[\"crrt_mode_category\"].isna()\n",
    "#         pre  = df[\"pre_filter_replacement_fluid_rate\"].notna()\n",
    "#         post = df[\"post_filter_replacement_fluid_rate\"].notna()\n",
    "#         dial = df[\"dialysate_flow_rate\"].notna()\n",
    "#         bf   = df[\"blood_flow_rate\"].notna()\n",
    "#         uf   = df[\"ultrafiltration_out\"].notna()\n",
    "#         all_num_present = df[NUM_COLS].notna().all(axis=1)\n",
    "\n",
    "#         df.loc[miss & all_num_present,                       \"crrt_mode_category\"] = \"cvvhdf\"\n",
    "#         df.loc[miss & (~dial) & pre & post,                  \"crrt_mode_category\"] = \"cvvh\"\n",
    "#         df.loc[miss & dial & (~pre) & (~post),               \"crrt_mode_category\"] = \"cvvhd\"\n",
    "#         df.loc[miss & (~dial) & (~pre) & (~post) & bf & uf,  \"crrt_mode_category\"] = \"scuf\"\n",
    "\n",
    "#         filled = (miss & df[\"crrt_mode_category\"].notna()).sum()\n",
    "#         p(f\"  • numeric-pattern inference filled {filled:,} missing modes\")\n",
    "#         drop_scuf_on_positive_df(df, p)\n",
    "\n",
    "#     # ───────────── Phase 1 — sort and *fix islands before episodes*\n",
    "#     p(\"✦ Phase 1: sort + SCUF-island fix\")\n",
    "#     df = df.sort_values([id_col, \"recorded_dttm\"]).reset_index(drop=True)\n",
    "\n",
    "#     if fix_islands:\n",
    "#         # after sorting, BEFORE episode detection\n",
    "#         prev_mode = df.groupby(id_col)[\"crrt_mode_category\"].shift()\n",
    "#         next_mode = df.groupby(id_col)[\"crrt_mode_category\"].shift(-1)\n",
    "\n",
    "#         scuf_island = (\n",
    "#             (df[\"crrt_mode_category\"] == \"scuf\") &\n",
    "#             (prev_mode.notna()) & (next_mode.notna()) &     # ensure we have neighbours\n",
    "#             (prev_mode == next_mode)                        # both neighbours agree\n",
    "#         )\n",
    "\n",
    "#         df.loc[scuf_island, \"crrt_mode_category\"] = prev_mode[scuf_island]\n",
    "#         n_fixed = scuf_island.sum()\n",
    "#         p(f\"  • relabelled {n_fixed:,} SCUF-island rows\")\n",
    "#         drop_scuf_on_positive_df(df, p)\n",
    "\n",
    "\n",
    "#     # ───────────── Phase 2 — episode detection (now with fixed modes)\n",
    "#     p(\"✦ Phase 2: derive `crrt_episode_id`\")\n",
    "#     mode_change = (\n",
    "#         df.groupby(id_col)[\"crrt_mode_category\"]\n",
    "#           .apply(lambda s: s != s.shift())\n",
    "#           .reset_index(level=0, drop=True)\n",
    "#     )\n",
    "#     time_gap = df.groupby(id_col)[\"recorded_dttm\"].diff().gt(gap_thresh).fillna(False)\n",
    "#     df[\"crrt_episode_id\"] = ((mode_change | time_gap)\n",
    "#                               .groupby(df[id_col]).cumsum()\n",
    "#                               .astype(\"int32\"))\n",
    "\n",
    "#     # ───────────── Phase 3 — forward-fill numerics inside episodes\n",
    "#     p(\"✦ Phase 3: forward-fill numeric vars inside episodes\")\n",
    "#     tqdm.pandas(disable=not verbose, desc=\"ffill per episode\")\n",
    "#     df[NUM_COLS] = (\n",
    "#         df.groupby([id_col, \"crrt_episode_id\"], sort=False, group_keys=False)[NUM_COLS]\n",
    "#           .progress_apply(lambda g: g.ffill())\n",
    "#     )\n",
    "\n",
    "#     # QC: blood-flow still missing?\n",
    "#     if flag_missing_bfr and \"blood_flow_rate\" in NUM_COLS:\n",
    "#         need_bfr = df[\"crrt_mode_category\"].isin([\"scuf\", \"cvvh\", \"cvvhd\", \"cvvhdf\"])\n",
    "#         df[\"blood_flow_missing_after_ffill\"] = need_bfr & df[\"blood_flow_rate\"].isna()\n",
    "#         p(f\"  • blood-flow still missing where required: \"\n",
    "#           f\"{df['blood_flow_missing_after_ffill'].mean():.1%}\")\n",
    "        \n",
    "#     # Bridge tiny episodes\n",
    "    \n",
    "#     single_row_ep = (\n",
    "#         df.groupby([id_col, \"crrt_episode_id\"]).size() == 1\n",
    "#     ).reset_index(name=\"n\").query(\"n == 1\")\n",
    "#     print(\"Bridging single row episodes\")\n",
    "\n",
    "#     rows_to_bridge = df.merge(single_row_ep[[id_col, \"crrt_episode_id\"]],\n",
    "#                             on=[id_col, \"crrt_episode_id\"]).index\n",
    "    \n",
    "#     CAT_COLS = [c for c in [\"crrt_mode_category\"] if c in df.columns]\n",
    "\n",
    "#     # Combine with the numeric columns we already had\n",
    "#     BRIDGE_COLS = NUM_COLS + CAT_COLS\n",
    "\n",
    "#     # Forward-fill (and back-fill just in case the island is the first row of the encounter)\n",
    "#     df.loc[rows_to_bridge, BRIDGE_COLS] = (\n",
    "#         df.loc[rows_to_bridge, BRIDGE_COLS]\n",
    "#         .groupby(df.loc[rows_to_bridge, id_col])      # keep encounter boundaries\n",
    "#         .apply(lambda g: g.ffill())          \n",
    "#         .reset_index(level=0, drop=True)\n",
    "#     )\n",
    "#     drop_scuf_on_positive_df(df, p)\n",
    "#     # ───────────── Phase 4 — wipe / flag unused parameters\n",
    "#     p(\"✦ Phase 4: handle parameters not valid for the mode\")\n",
    "#     MODE_PARAM_MAP = {\n",
    "#         \"scuf\":   {\"blood_flow_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvh\":   {\"blood_flow_rate\", \"pre_filter_replacement_fluid_rate\",\n",
    "#                    \"post_filter_replacement_fluid_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvhd\":  {\"blood_flow_rate\", \"dialysate_flow_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvhdf\": {\"blood_flow_rate\", \"pre_filter_replacement_fluid_rate\",\"post_filter_replacement_fluid_rate\",\n",
    "#                    \"dialysate_flow_rate\", \"ultrafiltration_out\"},\n",
    "#     }\n",
    "\n",
    "#     wiped_totals = {c: 0 for c in NUM_COLS}\n",
    "#     for mode, keep in MODE_PARAM_MAP.items():\n",
    "#         mask = df[\"crrt_mode_category\"] == mode\n",
    "#         drop_cols = list(set(NUM_COLS) - keep)\n",
    "#         if wipe_unused:\n",
    "#             for col in drop_cols:\n",
    "#                 wiped_totals[col] += df.loc[mask, col].notna().sum()\n",
    "#             df.loc[mask, drop_cols] = np.nan\n",
    "#         else:\n",
    "#             for col in drop_cols:\n",
    "#                 df.loc[mask & df[col].notna(), f\"{col}_unexpected\"] = True\n",
    "\n",
    "#     if verbose and wipe_unused:\n",
    "#         p(\"  • cells set → NA by wipe:\")\n",
    "#         for col, n in wiped_totals.items():\n",
    "#             p(f\"    {col:<35} {n:>8,}\")\n",
    "#     # ───────────── Phase 4a — SCUF‐specific sanity check\n",
    "#     if \"dialysate_flow_rate\" in df.columns:\n",
    "#         # only consider rows that were originally SCUF mode\n",
    "#         # and whose original _orig_df was non‐zero/non‐NA\n",
    "#         scuf_rows = df[\"crrt_mode_category\"] == \"scuf\"\n",
    "#         orig_bad = df[\"_orig_df\"].fillna(0) > 0\n",
    "\n",
    "#         # these are rows where the *original* data had UF>0 despite SCUF\n",
    "#         bad_orig_scuf = scuf_rows & orig_bad\n",
    "\n",
    "#         n_bad_orig = bad_orig_scuf.sum()\n",
    "#         if n_bad_orig:\n",
    "#             p(f\"!!! {n_bad_orig} rows originally labeled SCUF had DF>0 (raw data); forcing DF→NA for those\")\n",
    "#             df.loc[bad_orig_scuf, \"dialysate_flow_rate\"] = np.nan\n",
    "#         else:\n",
    "#             p(\"!!! No SCUF rows with DF>0\")\n",
    "\n",
    "#     # then drop the helper column\n",
    "#     df = df.drop(columns=\"_orig_df\")\n",
    "\n",
    "#     # ───────────── Phase 5 — deduplicate & order\n",
    "#     p(\"✦ Phase 5: deduplicate & order\")\n",
    "#     pre = len(df)\n",
    "#     df = (\n",
    "#         df.drop_duplicates(subset=[id_col, \"recorded_dttm\"])\n",
    "#           .sort_values([id_col, \"recorded_dttm\"])\n",
    "#           .reset_index(drop=True)\n",
    "#     )\n",
    "#     p(f\"  • dropped {pre - len(df):,} duplicate rows\")\n",
    "\n",
    "#     if verbose:\n",
    "#         sparse = df[NUM_COLS].isna().all(axis=1).mean()\n",
    "#         p(f\"  • rows with all NUM_COLS missing: {sparse:.1%}\")\n",
    "\n",
    "#     p(\"[OK] CRRT waterfall complete.\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffe16d",
   "metadata": {},
   "source": [
    "## Cohort identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Loading CLIF Tables\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from clifpy.clif_orchestrator import ClifOrchestrator\n",
    "\n",
    "# Initialize ClifOrchestrator\n",
    "clif = ClifOrchestrator(\n",
    "    data_directory=config['tables_path'],\n",
    "    filetype=config['file_type'],\n",
    "    timezone=config['timezone']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553499b",
   "metadata": {},
   "source": [
    "## Step0: Load Core Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc9ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 0: Load Core Tables (Patient, Hospitalization, ADT)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 0: Load Core Tables (Patient, Hospitalization, ADT)\")\n",
    "print(\"=\" * 80)\n",
    "core_tables = ['patient', 'hospitalization', 'adt']\n",
    "\n",
    "print(f\"\\nLoading {len(core_tables)} core tables...\")\n",
    "for table_name in core_tables:\n",
    "    print(f\"   Loading {table_name}...\", end=\" \")\n",
    "    try:\n",
    "        clif.load_table(table_name)\n",
    "        table = getattr(clif, table_name)\n",
    "        print(f\"✓ ({len(table.df):,} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nCore tables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_df = clif.hospitalization.df\n",
    "adt_df = clif.adt.df\n",
    "\n",
    "# Merge to get age information\n",
    "all_encounters = pd.merge(\n",
    "    hosp_df[[\"patient_id\", \"hospitalization_id\", \"admission_dttm\", \"discharge_dttm\", \n",
    "             \"age_at_admission\", \"discharge_category\"]],\n",
    "    adt_df[[\"hospitalization_id\", \"hospital_id\", \"in_dttm\", \"out_dttm\", \n",
    "            \"location_category\", \"location_type\"]],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates by ['hospitalization_id', 'in_dttm', 'out_dttm']\n",
    "dup_counts = all_encounters.duplicated(subset=['hospitalization_id', 'in_dttm', 'out_dttm']).sum()\n",
    "if dup_counts > 0:\n",
    "    print(f\"Warning: {dup_counts} duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")\n",
    "else:\n",
    "    print(\"No duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5f8fc",
   "metadata": {},
   "source": [
    "## Step1: Date & Age filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encounters.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Identify Adult Patients (Age >= 18) and Admissions 2018-2024\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 1: Identifying Adult Patients (Age >= 18) and Admissions 2018-2024\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Applying initial cohort filters...\")\n",
    "\n",
    "# Use only the relevant columns from all_encounters\n",
    "adult_encounters = all_encounters[\n",
    "    [\n",
    "        'patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
    "        'age_at_admission', 'discharge_category', 'hospital_id',\n",
    "        'in_dttm', 'out_dttm', 'location_category', 'location_type'\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "if config['timezone'].lower() == \"mimic\":\n",
    "    # MIMIC: only age >= 18, no admit year restriction\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['age_at_admission'] >= 18) & (adult_encounters['age_at_admission'].notna())\n",
    "    ]\n",
    "else:\n",
    "    # Other sites: age >= 18 and admission between 2018-2024 inclusive\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['age_at_admission'] >= 18) &\n",
    "        (adult_encounters['age_at_admission'].notna()) &\n",
    "        (adult_encounters['admission_dttm'].dt.year >= 2018) &\n",
    "        (adult_encounters['admission_dttm'].dt.year <= 2024)\n",
    "    ]\n",
    "\n",
    "print(f\"\\nFiltering Results:\")\n",
    "print(f\"   Total hospitalizations: {len(all_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Adult hospitalizations (age >= 18, 2018-2024): {len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Excluded (age < 18 or outside 2018-2024): {len(all_encounters['hospitalization_id'].unique()) - len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "\n",
    "\n",
    "strobe_counts[\"0_total_hospitalizations\"] = len(all_encounters['hospitalization_id'].unique())\n",
    "strobe_counts[\"1_adult_hospitalizations\"] = len(adult_encounters['hospitalization_id'].unique())\n",
    "# Get list of adult hospitalization IDs for filtering\n",
    "adult_hosp_ids = set(adult_encounters['hospitalization_id'].unique())\n",
    "print(f\"\\n   Unique adult hospitalization IDs: {len(adult_hosp_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66edf5",
   "metadata": {},
   "source": [
    "### Stitch hospitalizations \n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_id`, then we combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block.\n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_joined_id` from the hospitalization table, then we consider the user has already stitched similar encounters, and we will consider that as the primary id column for all table joins moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4651cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clifpy.utils.stitching_encounters import stitch_encounters\n",
    "\n",
    "# stitch hospitalizations\n",
    "hosp_filtered = clif.hospitalization.df[clif.hospitalization.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "adt_filtered = clif.adt.df[clif.adt.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "\n",
    "hosp_stitched, adt_stitched, encounter_mapping = stitch_encounters(\n",
    "    hospitalization=hosp_filtered,\n",
    "    adt=adt_filtered,\n",
    "    time_interval=6  \n",
    ")\n",
    "\n",
    "# Direct assignment without additional copies\n",
    "clif.hospitalization.df = hosp_stitched\n",
    "clif.adt.df = adt_stitched\n",
    "\n",
    "# Store the encounter mapping in the orchestrator for later use\n",
    "clif.encounter_mapping = encounter_mapping\n",
    "\n",
    "# Clean up intermediate variables\n",
    "del hosp_filtered, adt_filtered\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After your stitching code, add these calculations:\n",
    "\n",
    "# Calculate stitching statistics\n",
    "strobe_counts['1b_before_stitching'] = len(adult_hosp_ids)  # Original adult hospitalizations\n",
    "strobe_counts['1b_after_stitching'] = len(hosp_stitched['encounter_block'].unique())  # Unique encounter blocks after stitching\n",
    "strobe_counts['1b_stitched_hosp_ids'] = strobe_counts['1b_before_stitching'] - strobe_counts['1b_after_stitching']  # Number of hospitalizations that were linked\n",
    "\n",
    "print(f\"\\nEncounter Stitching Results:\")\n",
    "print(f\"   Number of unique hospitalizations before stitching: {strobe_counts['1b_before_stitching']:,}\")\n",
    "print(f\"   Number of unique encounter blocks after stitching: {strobe_counts['1b_after_stitching']:,}\")\n",
    "print(f\"   Number of linked hospitalization ids: {strobe_counts['1b_stitched_hosp_ids']:,}\")\n",
    "\n",
    "# Optional: Show the encounter mapping details\n",
    "print(f\"\\nEncounter Mapping Details:\")\n",
    "print(f\"   Total encounter mappings created: {len(encounter_mapping):,}\")\n",
    "if len(encounter_mapping) > 0:\n",
    "    # Show some examples of how many original hospitalizations were combined\n",
    "    mapping_counts = encounter_mapping.groupby('encounter_block').size()\n",
    "    print(f\"   Encounter blocks with multiple hospitalizations: {(mapping_counts > 1).sum():,}\")\n",
    "    print(f\"   Maximum hospitalizations combined into one block: {mapping_counts.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806ca75",
   "metadata": {},
   "source": [
    "# ADT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all_encounters with encounter_mapping to get encounter_block information\n",
    "all_encounters = pd.merge(all_encounters, encounter_mapping, on='hospitalization_id', how='left')\n",
    "\n",
    "# Convert location_category and discharge_category to lowercase in place (vectorized)\n",
    "all_encounters['location_category'] = all_encounters['location_category'].str.lower()\n",
    "all_encounters['discharge_category'] = all_encounters['discharge_category'].str.lower()\n",
    "\n",
    "# Create vectorized ICU and death masks\n",
    "icu_mask = all_encounters['location_category'].str.contains('icu', na=False)\n",
    "death_mask = all_encounters['discharge_category'].isin(['expired', 'hospice'])\n",
    "\n",
    "# Vectorized: For each encounter_block, does any row have ICU or death? (much faster)\n",
    "# Use groupby('encounter_block')[mask].transform('any') to vectorize\n",
    "all_encounters['icu_enc'] = icu_mask.groupby(all_encounters['encounter_block']).transform('any').astype(int)\n",
    "all_encounters['death_enc'] = death_mask.groupby(all_encounters['encounter_block']).transform('any').astype(int)\n",
    "\n",
    "# Cohort flag using logical OR (vectorized)\n",
    "all_encounters['cohort_enc'] = (all_encounters['icu_enc'] | all_encounters['death_enc']).astype(int)\n",
    "\n",
    "# Store hospitalization_ids for cohort_enc==1 in a list (as before)\n",
    "cohort_enc_hospitalization_ids = all_encounters.loc[all_encounters['cohort_enc'] == 1, 'hospitalization_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982478d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify encounters where death occurred\n",
    "death_encounters = all_encounters[all_encounters['death_enc'] == 1]\n",
    "# Identify those that never touched the ICU\n",
    "non_icu_deaths = death_encounters[~death_encounters['icu_enc'].astype(bool)]\n",
    "# Count the number of unique encounters with deaths outside of ICU\n",
    "num_deaths_outside_icu = non_icu_deaths['encounter_block'].nunique()\n",
    "# Calculate total deaths (unique encounter blocks with death)\n",
    "total_encounters = all_encounters['encounter_block'].nunique()\n",
    "# Calculate the percentage\n",
    "pct_deaths_outside_icu = (num_deaths_outside_icu / total_encounters * 100) if total_encounters > 0 else 0\n",
    "print(f\"Number of deaths outside ICU: {num_deaths_outside_icu} ({pct_deaths_outside_icu:.1f}% of all hospitalizations)\")\n",
    "\n",
    "# Add ICU encounters to strobe counts as 1_icu_encounters\n",
    "num_icu_encounters = all_encounters[all_encounters['icu_enc'] == 1]['encounter_block'].nunique()\n",
    "if 'strobe_counts' not in globals():\n",
    "    strobe_counts = {}\n",
    "strobe_counts['1_icu_encounters'] = num_icu_encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df96416",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cohort = all_encounters[\n",
    "    all_encounters['hospitalization_id'].isin(cohort_enc_hospitalization_ids)\n",
    "][['encounter_block', 'icu_enc', 'death_enc', 'cohort_enc']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1672f",
   "metadata": {},
   "source": [
    "# Respiratory Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Load Respiratory Support and Identify Patients on Advanced Respiratory support \n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" Loading Respiratory Support and Identifying IMV Patients\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nLoading respiratory_support table...\")\n",
    "clif.load_table('respiratory_support',\n",
    "                        columns=rst_required_columns,\n",
    "                        filters={'hospitalization_id': list(adult_hosp_ids)})\n",
    "print(f\"Respiratory support loaded ({len(clif.respiratory_support.df):,} rows)\")\n",
    "\n",
    "# Standardize category columns to lowercase\n",
    "print(f\"\\nStandardizing category columns...\")\n",
    "category_cols = [col for col in clif.respiratory_support.df.columns if col.endswith('_category')]\n",
    "for col in category_cols:\n",
    "    clif.respiratory_support.df[col] = clif.respiratory_support.df[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06229735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hospitalizations on advanced mechanical support\n",
    "print(f\"\\nIdentifying hospitalizations with advanced respiratory support devices...\")\n",
    "device_types = ['imv', 'nippv', 'cpap', 'high flow nc']\n",
    "clif.respiratory_support.df = pd.merge(clif.respiratory_support.df, encounter_mapping, \n",
    "                                        on='hospitalization_id', how='left')\n",
    "advanced_support_hosp_ids = clif.respiratory_support.df.loc[\n",
    "    clif.respiratory_support.df['device_category'].str.lower().isin([d.lower() for d in device_types]),\n",
    "    'encounter_block'\n",
    "].unique()\n",
    "print(f\"Hospitalizations with any advanced resp. device ({', '.join(device_types).upper()}): {len(advanced_support_hosp_ids):,}\")\n",
    "strobe_counts[\"2_advanced_resp_support_hospitalizations\"] = len(advanced_support_hosp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e77873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with advanced_support_hosp_ids and 'high_support_en' == 1\n",
    "advanced_support_df = pd.DataFrame({\n",
    "    'encounter_block': advanced_support_hosp_ids,\n",
    "    'high_support_enc': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a full join (outer merge) of final_cohort and advanced_support_df on 'encounter_block'\n",
    "final_cohort = final_cohort.merge(\n",
    "    advanced_support_df,\n",
    "    on='encounter_block',\n",
    "    how='outer'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170325ca",
   "metadata": {},
   "source": [
    "# Vasoactives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efe800",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading medication_admin_continuous table...\")\n",
    "clif.load_table(\n",
    "    'medication_admin_continuous',\n",
    "    columns=meds_required_columns,\n",
    "    filters={\n",
    "        'hospitalization_id': list(adult_hosp_ids),\n",
    "        'med_category': meds_of_interest\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c276d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hospitalizations on advanced mechanical support\n",
    "print(f\"\\nIdentifying hospitalizations with advanced respiratory support devices...\")\n",
    "vasoactive_meds = ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "                   'dopamine', 'angiotensin']\n",
    "clif.medication_admin_continuous.df= pd.merge(clif.medication_admin_continuous.df, encounter_mapping, \n",
    "                                        on='hospitalization_id', how='left')\n",
    "vasoactive_hosp_ids = clif.medication_admin_continuous.df.loc[\n",
    "    clif.medication_admin_continuous.df['med_category'].str.lower().isin([d.lower() for d in vasoactive_meds]),\n",
    "    'encounter_block'\n",
    "].unique()\n",
    "print(f\"Hospitalizations with any vasoactives. device ({', '.join(vasoactive_meds).upper()}): {len(vasoactive_hosp_ids):,}\")\n",
    "strobe_counts[\"3_vasoactive_hospitalizations\"] = len(vasoactive_hosp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e522d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with advanced_support_hosp_ids and 'high_support_en' == 1\n",
    "vasoactives_df = pd.DataFrame({\n",
    "    'encounter_block': vasoactive_hosp_ids,\n",
    "    'vaso_support_enc': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join vasoactives_df with final cohort on hospitalization_id\n",
    "final_cohort = final_cohort.merge(\n",
    "    vasoactives_df,\n",
    "    on='encounter_block',\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1473ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing high_support_en means not on advanced support\n",
    "final_cohort['vaso_support_enc'] = final_cohort['vaso_support_enc'].fillna(0).astype(int)\n",
    "# Missing high_support_en means not on advanced support\n",
    "final_cohort['high_support_enc'] = final_cohort['high_support_enc'].fillna(0).astype(int)\n",
    "# Missing icu_enc means not ICU\n",
    "final_cohort['icu_enc'] = final_cohort['icu_enc'].fillna(0).astype(int)\n",
    "# Define the criteria for other critically ill\n",
    "final_cohort['other_critically_ill'] = (\n",
    "    (final_cohort[['icu_enc', 'vaso_support_enc', 'high_support_enc']].sum(axis=1) == 0)\n",
    ").astype(int)\n",
    "# Calculate the count\n",
    "strobe_counts['4_other_critically_ill'] = final_cohort.loc[final_cohort['other_critically_ill'] == 1, \n",
    "                                                            'encounter_block'].nunique()\n",
    "strobe_counts['5_all_critically_ill'] = final_cohort['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec68a76",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa9305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "strobe_counts_df = pd.DataFrame(list(strobe_counts.items()), columns=['count_name', 'count_value'])\n",
    "strobe_counts_df.to_csv('../output/final/tableone/strobe_counts.csv', index=False)\n",
    "# Calculate mortality rates\n",
    "mortality_rates = {\n",
    "    'ICU Hospitalizations': final_cohort.loc[final_cohort['icu_enc'] == 1, 'death_enc'].mean() * 100,\n",
    "    'Advanced Respiratory Support': final_cohort.loc[final_cohort['high_support_enc'] == 1, 'death_enc'].mean() * 100,\n",
    "    'Vasoactive Hospitalizations': final_cohort.loc[final_cohort['vaso_support_enc'] == 1, 'death_enc'].mean() * 100,\n",
    "    'Other Critically Ill': final_cohort.loc[final_cohort['other_critically_ill'] == 1, 'death_enc'].mean() * 100,\n",
    "    'All Critically Ill Adults': final_cohort['death_enc'].mean() * 100,\n",
    "}\n",
    "mortality_rates_df = pd.DataFrame(list(mortality_rates.items()), columns=['count_name', 'count_value'])\n",
    "mortality_rates_df.to_csv('../output/final/tableone/mortality_rates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "def create_consort_diagram(strobe_counts, mortality_rates):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "    ax.set_xlim(-1, 13)\n",
    "    ax.set_ylim(0, 14)\n",
    "    ax.axis('off')\n",
    "\n",
    "    box_style = \"round,pad=0.1\"\n",
    "    boxes = {}\n",
    "\n",
    "    def create_box(x, y, width, height, text, box_id=None, fontsize=10, fontweight='normal'):\n",
    "        box = FancyBboxPatch(\n",
    "            (x - width/2, y - height/2), width, height,\n",
    "            boxstyle=box_style, facecolor='white', edgecolor='black', linewidth=1.5\n",
    "        )\n",
    "        ax.add_patch(box)\n",
    "        ax.text(x, y, text, ha='center', va='center', fontsize=fontsize, fontweight=fontweight, wrap=True)\n",
    "        \n",
    "        return {\n",
    "            'x': x, 'y': y, 'width': width, 'height': height,\n",
    "            'left': x - width/2, 'right': x + width/2,\n",
    "            'top': y + height/2, 'bottom': y - height/2\n",
    "        }\n",
    "\n",
    "    def create_arrow(from_box, to_box):\n",
    "        x1, y1 = from_box['x'], from_box['bottom'] - 0.1\n",
    "        x2, y2 = to_box['x'], to_box['top'] + 0.1\n",
    "        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "\n",
    "    ax.text(5, 13, 'Cohort', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Define and arrange the boxes\n",
    "    box1 = create_box(5, 12, 3, 0.7, \n",
    "                      f\"Total Hospitalizations\\nn = {strobe_counts['0_total_hospitalizations']:,}\",\n",
    "                      'total', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box2 = create_box(5, 10.5, 3, 0.7,\n",
    "                      f\"Adult Hospitalizations\\nn = {strobe_counts['1b_after_stitching']:,}\",\n",
    "                      'adult', fontsize=11, fontweight='bold')\n",
    "    create_arrow(box1, box2)\n",
    "\n",
    "    # Define ICU, respiratory support, vasoactive, and other critically ill categories\n",
    "    box3_icu = create_box(1, 8, 3, 0.9,\n",
    "                          f\"ICU Hospitalizations\\nn = {strobe_counts['1_icu_encounters']:,}\\nMortality: {mortality_rates['ICU Hospitalizations']:.2f}%\",\n",
    "                          'icu', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box3_resp = create_box(4.5, 8, 3, 0.9,\n",
    "                           f\"Advanced Respiratory Support\\nn = {strobe_counts['2_advanced_resp_support_hospitalizations']:,}\\nMortality: {mortality_rates['Advanced Respiratory Support']:.2f}%\",\n",
    "                           'resp_support', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box3_vaso = create_box(8, 8, 3, 0.9,\n",
    "                           f\"Vasoactive Hospitalizations\\nn = {strobe_counts['3_vasoactive_hospitalizations']:,}\\nMortality: {mortality_rates['Vasoactive Hospitalizations']:.2f}%\",\n",
    "                           'vasoactive', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box3_other = create_box(11.3, 8, 3, 0.9,\n",
    "                            f\"Other Critically Ill\\nn = {strobe_counts['4_other_critically_ill']:,}\\nMortality: {mortality_rates['Other Critically Ill']:.2f}%\",\n",
    "                            'other', fontsize=11, fontweight='bold')\n",
    "\n",
    "    create_arrow(box2, box3_icu)\n",
    "    create_arrow(box2, box3_resp)\n",
    "    create_arrow(box2, box3_vaso)\n",
    "    create_arrow(box2, box3_other)\n",
    "\n",
    "    # Add a final box for \"All Critically Ill Adults\"\n",
    "    box_final = create_box(5.7, 4.5, 5.2, 1.1,\n",
    "        f\"All Critically Ill Adults\\nn = {final_cohort['encounter_block'].nunique():,}\\nMortality: {mortality_rates['All Critically Ill Adults']:.2f}%\",\n",
    "        'all_critically_ill', fontsize=13, fontweight='bold')\n",
    "\n",
    "    # Do NOT draw arrows from the four groups to the all critically ill adults box\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../output/final/tableone/consort_flow_diagram.png', dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf13e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from upsetplot import UpSet, from_indicators\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='upsetplot')\n",
    "\n",
    "# Your UpSet plot code here...\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../output/final/tableone', exist_ok=True)\n",
    "\n",
    "# Prepare final_cohort data for UpSet plot\n",
    "summary_df = final_cohort[['encounter_block', 'icu_enc', 'death_enc', 'high_support_enc', 'vaso_support_enc']].drop_duplicates()\n",
    "\n",
    "# Rename columns to match CONSORT flow labels\n",
    "summary_df = summary_df.rename(columns={\n",
    "    'icu_enc': 'ICU Hospitalizations',\n",
    "    'death_enc': 'Died',\n",
    "    'high_support_enc': 'Advanced O2 Support',\n",
    "    'vaso_support_enc': 'Vasoactive Support'\n",
    "})\n",
    "\n",
    "# Convert to boolean for UpSet plot\n",
    "summary_df['ICU Hospitalizations'] = summary_df['ICU Hospitalizations'].astype(bool)\n",
    "summary_df['Died'] = summary_df['Died'].astype(bool)\n",
    "summary_df['Advanced O2 Support'] = summary_df['Advanced O2 Support'].astype(bool)\n",
    "summary_df['Vasoactive Support'] = summary_df['Vasoactive Support'].astype(bool)\n",
    "\n",
    "# Create UpSet plot\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "upset_data = from_indicators(\n",
    "    ['ICU Hospitalizations', 'Died', 'Advanced O2 Support', 'Vasoactive Support'], \n",
    "    data=summary_df.set_index('encounter_block')\n",
    ")\n",
    "\n",
    "upset = UpSet(upset_data, \n",
    "              subset_size='count',\n",
    "              show_counts=True,\n",
    "              sort_by='cardinality',\n",
    "              element_size=50,\n",
    "              with_lines=True)\n",
    "\n",
    "upset.plot(fig=fig)\n",
    "\n",
    "plt.subplots_adjust(left=0.2, bottom=0.2, right=0.95, top=0.85, hspace=0.3, wspace=0.3)\n",
    "plt.suptitle('Clinical Cohort Intersections', fontsize=16, y=0.95)\n",
    "\n",
    "# Adjust font sizes for better readability\n",
    "for ax in fig.get_axes():\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                 ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('../output/final/cohort_intersect_upset_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ee2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the diagram\n",
    "create_consort_diagram(strobe_counts, mortality_rates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".clif_table_one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
