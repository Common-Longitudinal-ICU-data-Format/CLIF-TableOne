{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cc294e",
   "metadata": {},
   "source": [
    "## CLIF Table One\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "Date v1: May 13, 2025\n",
    "\n",
    "This script identifies the cohort of encounters with at least one ICU stay and then summarizes the cohort data into one table. \n",
    "\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`, `clif_patient_assessments`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **clif_patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **clif_hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`,`discharge_dttm`, `age_at_admission` | - |\n",
    "| **clif_adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category` | - |\n",
    "| **clif_vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | weight_kg |\n",
    "| **clif_labs** | `hospitalization_id`, `lab_result_dttm`, `lab_order_dttm`, `lab_category`, `lab_value_numeric` | creatinine, bilirubin_total, po2_arterial, platelet_count |\n",
    "| **clif_medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin(optional) |\n",
    "| **clif_respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`,  `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs`, `tidal_volume_set`, `pressure_control_set`, `pressure_support_set` | - |\n",
    "| **clif_patient_assessments** | `hospitalization_id`, `recorded_dttm` , `assessment_category`, `numerical_value`| `gcs_total` |\n",
    "| **clif_crrt_therapy** | `hospitalization_id`, `recorded_dttm` | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bf46d",
   "metadata": {},
   "source": [
    "## Cohort Identification\n",
    "\n",
    "\n",
    "## Inclusion \n",
    "1. Adults\n",
    "2. Patients with at least one ICU stay or those who had only emergency department or ward encounters and either died or received life support at any point. Life support is defined as the administration of any vasoactive drugs or respiratory support exceeding low-flow oxygen.\n",
    "\n",
    "Respiratory support device: 'IMV', 'NIPPV', 'CPAP', 'High Flow NC'  \n",
    "\n",
    "Vasoactive: 'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import clifpy\n",
    "import os\n",
    "\n",
    "print(\"=== Environment Verification ===\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"clifpy version: {clifpy.__version__}\")\n",
    "print(f\"clifpy location: {clifpy.__file__}\")\n",
    "\n",
    "print(\"\\n=== Python Path Check ===\")\n",
    "local_clifpy_path = \"/Users/kavenchhikara/Desktop/CLIF/CLIFpy\"\n",
    "if any(local_clifpy_path in path for path in sys.path):\n",
    "    print(\"⚠️  WARNING: Local CLIFpy still in path!\")\n",
    "    for path in sys.path:\n",
    "        if local_clifpy_path in path:\n",
    "            print(f\"   Found: {path}\")\n",
    "else:\n",
    "    print(\"✅ Clean environment - no local CLIFpy in path\")\n",
    "\n",
    "print(f\"\\n=== Working Directory ===\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../config/config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Create the output directory for tableone results if it does not already exist\n",
    "output_dir = Path(\"../output/final/tableone\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n=� Configuration:\")\n",
    "print(f\"   Data directory: {config['tables_path']}\")\n",
    "print(f\"   File type: {config['file_type']}\")\n",
    "print(f\"   Timezone: {config['timezone']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127edcbe",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4319503",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Defining Required Data Elements\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Full patient table \n",
    "\n",
    "# Full hospitalization table \n",
    "\n",
    "# Full ADT table\n",
    "\n",
    "# Vitals\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "# Respiratory Support \n",
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set', \n",
    "    'pressure_control_set',\n",
    "    'pressure_support_set',\n",
    "    'peak_inspiratory_pressure_set',\n",
    "    'peak_inspiratory_pressure_obs',\n",
    "    'plateau_pressure_obs',\n",
    "    'minute_vent_obs'\n",
    "]\n",
    "\n",
    "\n",
    "# Continuous administered meds\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol',\n",
    "    'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', \n",
    "    'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee461357",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7cbe42",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_hourly_sequence(group):\n",
    "#     blk = group.name  # use group name from groupby\n",
    "#     start_time = group['vent_episode_start_dttm'].iloc[0]\n",
    "#     end_time   = group['vent_end_dttm_72h'].iloc[0]\n",
    "#     hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "#     return pd.DataFrame({\n",
    "#         'hospitalization_id': blk,\n",
    "#         'recorded_dttm': hourly_timestamps\n",
    "#     })\n",
    "\n",
    "# def calculate_ibw(height_cm, sex):\n",
    "#     if pd.isna(height_cm) or pd.isna(sex):\n",
    "#         return np.nan\n",
    "#     height_inches = height_cm / 2.54\n",
    "#     sex = str(sex).lower()\n",
    "#     if sex == 'male':\n",
    "#         return 50 + 2.3 * (height_inches - 60)\n",
    "#     elif sex == 'female':\n",
    "#         return 45.5 + 2.3 * (height_inches - 60)\n",
    "#     else:\n",
    "#         return np.nan\n",
    "\n",
    "# def calculate_base_excess(ph, hco3):\n",
    "#     \"\"\"\n",
    "#     Calculate Base Excess using simplified formula\n",
    "#     BE = (HCO3 - 24.4) + (8.3 * (pH - 7.4))\n",
    "#     \"\"\"\n",
    "#     return (hco3 - 24.4) + (8.3 * (ph - 7.4))\n",
    "\n",
    "# def calculate_pf_ratio(po2, fio2):\n",
    "#     \"\"\"\n",
    "#     Vectorized calculation of P/F ratio (PaO2/FiO2)\n",
    "#     FiO2 should be as fraction (0.21-1.0), not percentage\n",
    "#     Handles pandas Series input.\n",
    "#     \"\"\"\n",
    "#     fio2 = fio2.copy()\n",
    "#     # Convert percentage to fraction if needed\n",
    "#     mask_pct = fio2 > 1\n",
    "#     fio2[mask_pct] = fio2[mask_pct] / 100\n",
    "#     # Set minimum fio2 to 0.21 (room air)\n",
    "#     fio2 = fio2.clip(lower=0.21)\n",
    "#     return po2 / fio2\n",
    "\n",
    "# def process_crrt_waterfall(\n",
    "#     crrt: pd.DataFrame,\n",
    "#     *,\n",
    "#     id_col: str = \"hospitalization_id\",\n",
    "#     gap_thresh: Union[str, pd.Timedelta] = \"2h\",\n",
    "#     infer_modes: bool = True,          # infer missing mode from numeric pattern\n",
    "#     flag_missing_bfr: bool = True,     # add QC flag if blood-flow still NaN\n",
    "#     wipe_unused: bool = True,          # null parameters not used by the mode\n",
    "#     fix_islands: bool = True,          # relabel single-row SCUF islands\n",
    "#     verbose: bool = True,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Clean + episode-aware forward-fill for the CLIF `crrt_therapy` table.\n",
    "#     Episode-aware clean-up and forward-fill of the CLIF `crrt_therapy` table.\n",
    "\n",
    "#     The function mirrors the respiratory-support “waterfall” logic but adapts it to\n",
    "#     the quirks of Continuous Renal Replacement Therapy (CRRT):\n",
    "\n",
    "#     1. **Episode detection** - a new `crrt_episode_id` starts whenever  \n",
    "#        • `crrt_mode_category` changes **OR**  \n",
    "#        • the gap between successive rows exceeds *gap_thresh* (default 2 h).\n",
    "#     2. **Numeric forward-fill inside an episode** - fills *only* the parameters\n",
    "#        that are clinically relevant for the active mode.\n",
    "#     3. **Mode-specific wiping** after filling, parameters that are **not used**\n",
    "#        in the current mode (e.g. `dialysate_flow_rate` in SCUF) are nulled so\n",
    "#        stale data never bleed across modes.\n",
    "#     4. **Deduplication & ordering** guarantees exactly **one row per\n",
    "#        `(id_col, recorded_dttm)`**, chronologically ordered.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     crrt : pd.DataFrame\n",
    "#         Raw `crrt_therapy` table **in UTC**. Must contain the schema columns\n",
    "#         defined on the CLIF website (see docstring footer).\n",
    "#     id_col : str, default ``\"hospitalization_id\"``\n",
    "#         Encounter-level identifier.\n",
    "#     gap_thresh : str or pd.Timedelta, default ``\"2h\"``\n",
    "#         Maximum tolerated gap **inside** an episode before a new episode is\n",
    "#         forced. Accepts any pandas-parsable offset string (``\"90min\"``, ``\"3h\"``,\n",
    "#         …) or a ``pd.Timedelta``.\n",
    "#     verbose : bool, default ``True``\n",
    "#         If *True* prints progress banners.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         Processed CRRT DataFrame with\n",
    "\n",
    "#         * ``crrt_episode_id`` (int32) - sequential per encounter,\n",
    "#         * forward-filled numeric parameters **within** each episode,\n",
    "#         * unused parameters blanked per mode,\n",
    "#         * unique, ordered rows ``id_col, recorded_dttm``.\n",
    "\n",
    "#     Add-ons v2.0\n",
    "#     ------------\n",
    "#     • Optional numeric-pattern inference of `crrt_mode_category`.\n",
    "#     • Flags rows that *should* have blood-flow but don't.\n",
    "#     • Optional fix for single-row modality islands (sandwiched rows).\n",
    "#     • Optional wipe vs. keep of parameters not used by the active mode.\n",
    "\n",
    "#     Key steps\n",
    "#     ----------\n",
    "#     0.  Lower-case strings, coerce numerics, **infer** mode when blank.\n",
    "#     1.  **Relabel single-row SCUF islands** (if *fix_islands*).\n",
    "#     2.  Detect `crrt_episode_id` (mode change or >gap_thresh).\n",
    "#     3.  Forward-fill numeric parameters *within* an episode.\n",
    "#     4.  QC flag → `blood_flow_missing_after_ffill` (optional).\n",
    "#     5.  Wipe / flag parameters not valid for the mode (configurable).\n",
    "#     6.  Deduplicate & order ⇒ one row per ``(id_col, recorded_dttm)``.\n",
    "#     \"\"\"\n",
    "#     p = print if verbose else (lambda *_, **__: None)\n",
    "#     gap_thresh = pd.Timedelta(gap_thresh)\n",
    "\n",
    "#     # ───────────── Phase 0 — prep, numeric coercion, optional inference\n",
    "#     p(\"✦ Phase 0: prep & numeric coercion (+optional mode inference)\")\n",
    "#     df = crrt.copy()\n",
    "\n",
    "#     df[\"crrt_mode_category\"] = df[\"crrt_mode_category\"].str.lower()\n",
    "#     # save original dialysate_flow_rate values\n",
    "#     df[\"_orig_df\"] = df[\"dialysate_flow_rate\"]\n",
    "\n",
    "#     # 0a) RAW SCUF DF‐OUT sanity check\n",
    "#     # look for rows that are already labeled “scuf”\n",
    "#     # and that have a non‐zero dialysate_flow_rate in the raw data\n",
    "#     raw_scuf = df[\"crrt_mode_category\"].str.lower() == \"scuf\"\n",
    "#     raw_df_positive = df[\"_orig_df\"].fillna(0) > 0\n",
    "\n",
    "#     n_bad = (raw_scuf & raw_df_positive).sum()\n",
    "#     if n_bad:\n",
    "#         print(f\"!!!  Found {n_bad} raw SCUF rows with dialysate_flow_rate > 0 (should be 0 or NA)\")\n",
    "#         print(\" Converting these mode category to NA, keep recorded numerical values as the ground truth\")\n",
    "#         df.loc[raw_df_positive, \"crrt_mode_category\"] = np.nan\n",
    "#     else:\n",
    "#         print(\"!!! No raw SCUF rows had dialysate_flow_rate > 0\")\n",
    "\n",
    "#     NUM_COLS = [\n",
    "#         \"blood_flow_rate\",\n",
    "#         \"pre_filter_replacement_fluid_rate\",\n",
    "#         \"post_filter_replacement_fluid_rate\",\n",
    "#         \"dialysate_flow_rate\",\n",
    "#         \"ultrafiltration_out\",\n",
    "#     ]\n",
    "#     NUM_COLS = [c for c in NUM_COLS if c in df.columns]\n",
    "#     df[NUM_COLS] = df[NUM_COLS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "#     #  any row whose original ultrafiltration_out was >0 must never be SCUF\n",
    "#     def drop_scuf_on_positive_df(df, p):\n",
    "#         bad_df  = df[\"_orig_df\"].fillna(0) > 0\n",
    "#         scuf_now = df[\"crrt_mode_category\"] == \"scuf\"\n",
    "#         n = (bad_df & scuf_now).sum()\n",
    "#         if n:\n",
    "#             p(f\"→ Removing {n:,} SCUF labels on rows with DF>0\")\n",
    "#             df.loc[bad_df & scuf_now, \"crrt_mode_category\"] = np.nan\n",
    "            \n",
    "\n",
    "#     if infer_modes:\n",
    "#         miss = df[\"crrt_mode_category\"].isna()\n",
    "#         pre  = df[\"pre_filter_replacement_fluid_rate\"].notna()\n",
    "#         post = df[\"post_filter_replacement_fluid_rate\"].notna()\n",
    "#         dial = df[\"dialysate_flow_rate\"].notna()\n",
    "#         bf   = df[\"blood_flow_rate\"].notna()\n",
    "#         uf   = df[\"ultrafiltration_out\"].notna()\n",
    "#         all_num_present = df[NUM_COLS].notna().all(axis=1)\n",
    "\n",
    "#         df.loc[miss & all_num_present,                       \"crrt_mode_category\"] = \"cvvhdf\"\n",
    "#         df.loc[miss & (~dial) & pre & post,                  \"crrt_mode_category\"] = \"cvvh\"\n",
    "#         df.loc[miss & dial & (~pre) & (~post),               \"crrt_mode_category\"] = \"cvvhd\"\n",
    "#         df.loc[miss & (~dial) & (~pre) & (~post) & bf & uf,  \"crrt_mode_category\"] = \"scuf\"\n",
    "\n",
    "#         filled = (miss & df[\"crrt_mode_category\"].notna()).sum()\n",
    "#         p(f\"  • numeric-pattern inference filled {filled:,} missing modes\")\n",
    "#         drop_scuf_on_positive_df(df, p)\n",
    "\n",
    "#     # ───────────── Phase 1 — sort and *fix islands before episodes*\n",
    "#     p(\"✦ Phase 1: sort + SCUF-island fix\")\n",
    "#     df = df.sort_values([id_col, \"recorded_dttm\"]).reset_index(drop=True)\n",
    "\n",
    "#     if fix_islands:\n",
    "#         # after sorting, BEFORE episode detection\n",
    "#         prev_mode = df.groupby(id_col)[\"crrt_mode_category\"].shift()\n",
    "#         next_mode = df.groupby(id_col)[\"crrt_mode_category\"].shift(-1)\n",
    "\n",
    "#         scuf_island = (\n",
    "#             (df[\"crrt_mode_category\"] == \"scuf\") &\n",
    "#             (prev_mode.notna()) & (next_mode.notna()) &     # ensure we have neighbours\n",
    "#             (prev_mode == next_mode)                        # both neighbours agree\n",
    "#         )\n",
    "\n",
    "#         df.loc[scuf_island, \"crrt_mode_category\"] = prev_mode[scuf_island]\n",
    "#         n_fixed = scuf_island.sum()\n",
    "#         p(f\"  • relabelled {n_fixed:,} SCUF-island rows\")\n",
    "#         drop_scuf_on_positive_df(df, p)\n",
    "\n",
    "\n",
    "#     # ───────────── Phase 2 — episode detection (now with fixed modes)\n",
    "#     p(\"✦ Phase 2: derive `crrt_episode_id`\")\n",
    "#     mode_change = (\n",
    "#         df.groupby(id_col)[\"crrt_mode_category\"]\n",
    "#           .apply(lambda s: s != s.shift())\n",
    "#           .reset_index(level=0, drop=True)\n",
    "#     )\n",
    "#     time_gap = df.groupby(id_col)[\"recorded_dttm\"].diff().gt(gap_thresh).fillna(False)\n",
    "#     df[\"crrt_episode_id\"] = ((mode_change | time_gap)\n",
    "#                               .groupby(df[id_col]).cumsum()\n",
    "#                               .astype(\"int32\"))\n",
    "\n",
    "#     # ───────────── Phase 3 — forward-fill numerics inside episodes\n",
    "#     p(\"✦ Phase 3: forward-fill numeric vars inside episodes\")\n",
    "#     tqdm.pandas(disable=not verbose, desc=\"ffill per episode\")\n",
    "#     df[NUM_COLS] = (\n",
    "#         df.groupby([id_col, \"crrt_episode_id\"], sort=False, group_keys=False)[NUM_COLS]\n",
    "#           .progress_apply(lambda g: g.ffill())\n",
    "#     )\n",
    "\n",
    "#     # QC: blood-flow still missing?\n",
    "#     if flag_missing_bfr and \"blood_flow_rate\" in NUM_COLS:\n",
    "#         need_bfr = df[\"crrt_mode_category\"].isin([\"scuf\", \"cvvh\", \"cvvhd\", \"cvvhdf\"])\n",
    "#         df[\"blood_flow_missing_after_ffill\"] = need_bfr & df[\"blood_flow_rate\"].isna()\n",
    "#         p(f\"  • blood-flow still missing where required: \"\n",
    "#           f\"{df['blood_flow_missing_after_ffill'].mean():.1%}\")\n",
    "        \n",
    "#     # Bridge tiny episodes\n",
    "    \n",
    "#     single_row_ep = (\n",
    "#         df.groupby([id_col, \"crrt_episode_id\"]).size() == 1\n",
    "#     ).reset_index(name=\"n\").query(\"n == 1\")\n",
    "#     print(\"Bridging single row episodes\")\n",
    "\n",
    "#     rows_to_bridge = df.merge(single_row_ep[[id_col, \"crrt_episode_id\"]],\n",
    "#                             on=[id_col, \"crrt_episode_id\"]).index\n",
    "    \n",
    "#     CAT_COLS = [c for c in [\"crrt_mode_category\"] if c in df.columns]\n",
    "\n",
    "#     # Combine with the numeric columns we already had\n",
    "#     BRIDGE_COLS = NUM_COLS + CAT_COLS\n",
    "\n",
    "#     # Forward-fill (and back-fill just in case the island is the first row of the encounter)\n",
    "#     df.loc[rows_to_bridge, BRIDGE_COLS] = (\n",
    "#         df.loc[rows_to_bridge, BRIDGE_COLS]\n",
    "#         .groupby(df.loc[rows_to_bridge, id_col])      # keep encounter boundaries\n",
    "#         .apply(lambda g: g.ffill())          \n",
    "#         .reset_index(level=0, drop=True)\n",
    "#     )\n",
    "#     drop_scuf_on_positive_df(df, p)\n",
    "#     # ───────────── Phase 4 — wipe / flag unused parameters\n",
    "#     p(\"✦ Phase 4: handle parameters not valid for the mode\")\n",
    "#     MODE_PARAM_MAP = {\n",
    "#         \"scuf\":   {\"blood_flow_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvh\":   {\"blood_flow_rate\", \"pre_filter_replacement_fluid_rate\",\n",
    "#                    \"post_filter_replacement_fluid_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvhd\":  {\"blood_flow_rate\", \"dialysate_flow_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvhdf\": {\"blood_flow_rate\", \"pre_filter_replacement_fluid_rate\",\"post_filter_replacement_fluid_rate\",\n",
    "#                    \"dialysate_flow_rate\", \"ultrafiltration_out\"},\n",
    "#     }\n",
    "\n",
    "#     wiped_totals = {c: 0 for c in NUM_COLS}\n",
    "#     for mode, keep in MODE_PARAM_MAP.items():\n",
    "#         mask = df[\"crrt_mode_category\"] == mode\n",
    "#         drop_cols = list(set(NUM_COLS) - keep)\n",
    "#         if wipe_unused:\n",
    "#             for col in drop_cols:\n",
    "#                 wiped_totals[col] += df.loc[mask, col].notna().sum()\n",
    "#             df.loc[mask, drop_cols] = np.nan\n",
    "#         else:\n",
    "#             for col in drop_cols:\n",
    "#                 df.loc[mask & df[col].notna(), f\"{col}_unexpected\"] = True\n",
    "\n",
    "#     if verbose and wipe_unused:\n",
    "#         p(\"  • cells set → NA by wipe:\")\n",
    "#         for col, n in wiped_totals.items():\n",
    "#             p(f\"    {col:<35} {n:>8,}\")\n",
    "#     # ───────────── Phase 4a — SCUF‐specific sanity check\n",
    "#     if \"dialysate_flow_rate\" in df.columns:\n",
    "#         # only consider rows that were originally SCUF mode\n",
    "#         # and whose original _orig_df was non‐zero/non‐NA\n",
    "#         scuf_rows = df[\"crrt_mode_category\"] == \"scuf\"\n",
    "#         orig_bad = df[\"_orig_df\"].fillna(0) > 0\n",
    "\n",
    "#         # these are rows where the *original* data had UF>0 despite SCUF\n",
    "#         bad_orig_scuf = scuf_rows & orig_bad\n",
    "\n",
    "#         n_bad_orig = bad_orig_scuf.sum()\n",
    "#         if n_bad_orig:\n",
    "#             p(f\"!!! {n_bad_orig} rows originally labeled SCUF had DF>0 (raw data); forcing DF→NA for those\")\n",
    "#             df.loc[bad_orig_scuf, \"dialysate_flow_rate\"] = np.nan\n",
    "#         else:\n",
    "#             p(\"!!! No SCUF rows with DF>0\")\n",
    "\n",
    "#     # then drop the helper column\n",
    "#     df = df.drop(columns=\"_orig_df\")\n",
    "\n",
    "#     # ───────────── Phase 5 — deduplicate & order\n",
    "#     p(\"✦ Phase 5: deduplicate & order\")\n",
    "#     pre = len(df)\n",
    "#     df = (\n",
    "#         df.drop_duplicates(subset=[id_col, \"recorded_dttm\"])\n",
    "#           .sort_values([id_col, \"recorded_dttm\"])\n",
    "#           .reset_index(drop=True)\n",
    "#     )\n",
    "#     p(f\"  • dropped {pre - len(df):,} duplicate rows\")\n",
    "\n",
    "#     if verbose:\n",
    "#         sparse = df[NUM_COLS].isna().all(axis=1).mean()\n",
    "#         p(f\"  • rows with all NUM_COLS missing: {sparse:.1%}\")\n",
    "\n",
    "#     p(\"[OK] CRRT waterfall complete.\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffe16d",
   "metadata": {},
   "source": [
    "## Cohort identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Loading CLIF Tables\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from clifpy.clif_orchestrator import ClifOrchestrator\n",
    "\n",
    "# Initialize ClifOrchestrator\n",
    "clif = ClifOrchestrator(\n",
    "    data_directory=config['tables_path'],\n",
    "    filetype=config['file_type'],\n",
    "    timezone=config['timezone']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553499b",
   "metadata": {},
   "source": [
    "## Step0: Load Core Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc9ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 0: Load Core Tables (Patient, Hospitalization, ADT)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 0: Load Core Tables (Patient, Hospitalization, ADT)\")\n",
    "print(\"=\" * 80)\n",
    "core_tables = ['patient', 'hospitalization', 'adt']\n",
    "\n",
    "print(f\"\\nLoading {len(core_tables)} core tables...\")\n",
    "for table_name in core_tables:\n",
    "    print(f\"   Loading {table_name}...\", end=\" \")\n",
    "    try:\n",
    "        clif.load_table(table_name)\n",
    "        table = getattr(clif, table_name)\n",
    "        print(f\"✓ ({len(table.df):,} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nCore tables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_df = clif.hospitalization.df\n",
    "adt_df = clif.adt.df\n",
    "\n",
    "# Merge to get age information\n",
    "all_encounters = pd.merge(\n",
    "    hosp_df[[\"patient_id\", \"hospitalization_id\", \"admission_dttm\", \"discharge_dttm\", \n",
    "             \"age_at_admission\", \"discharge_category\", \"admission_type_category\"]],\n",
    "    adt_df[[\"hospitalization_id\", \"hospital_id\", \"in_dttm\", \"out_dttm\", \n",
    "            \"location_category\", \"location_type\"]],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates by ['hospitalization_id', 'in_dttm', 'out_dttm']\n",
    "dup_counts = all_encounters.duplicated(subset=['hospitalization_id', 'in_dttm', 'out_dttm']).sum()\n",
    "if dup_counts > 0:\n",
    "    print(f\"Warning: {dup_counts} duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")\n",
    "else:\n",
    "    print(\"No duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5f8fc",
   "metadata": {},
   "source": [
    "## Step1: Date & Age filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encounters.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Identify Adult Patients (Age >= 18) and Admissions 2018-2024\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 1: Identifying Adult Patients (Age >= 18) and Admissions 2018-2024\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Applying initial cohort filters...\")\n",
    "\n",
    "# Use only the relevant columns from all_encounters\n",
    "adult_encounters = all_encounters[\n",
    "    [\n",
    "        'patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
    "        'age_at_admission', 'discharge_category', 'admission_type_category' ,'hospital_id',\n",
    "        'in_dttm', 'out_dttm', 'location_category', 'location_type'\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "if config['timezone'].lower() == \"mimic\":\n",
    "    # MIMIC: only age >= 18, no admit year restriction\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['age_at_admission'] >= 18) & (adult_encounters['age_at_admission'].notna())\n",
    "    ]\n",
    "else:\n",
    "    # Other sites: age >= 18 and admission between 2018-2024 inclusive\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['age_at_admission'] >= 18) &\n",
    "        (adult_encounters['age_at_admission'].notna()) &\n",
    "        (adult_encounters['admission_dttm'].dt.year >= 2018) &\n",
    "        (adult_encounters['admission_dttm'].dt.year <= 2024)\n",
    "    ]\n",
    "\n",
    "print(f\"\\nFiltering Results:\")\n",
    "print(f\"   Total hospitalizations: {len(all_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Adult hospitalizations (age >= 18, 2018-2024): {len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Excluded (age < 18 or outside 2018-2024): {len(all_encounters['hospitalization_id'].unique()) - len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "\n",
    "\n",
    "strobe_counts[\"0_total_hospitalizations\"] = len(all_encounters['hospitalization_id'].unique())\n",
    "strobe_counts[\"1_adult_hospitalizations\"] = len(adult_encounters['hospitalization_id'].unique())\n",
    "# Get list of adult hospitalization IDs for filtering\n",
    "adult_hosp_ids = set(adult_encounters['hospitalization_id'].unique())\n",
    "print(f\"\\n   Unique adult hospitalization IDs: {len(adult_hosp_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66edf5",
   "metadata": {},
   "source": [
    "### Stitch hospitalizations \n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_id`, then we combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block.\n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_joined_id` from the hospitalization table, then we consider the user has already stitched similar encounters, and we will consider that as the primary id column for all table joins moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4651cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clifpy.utils.stitching_encounters import stitch_encounters\n",
    "\n",
    "# stitch hospitalizations\n",
    "hosp_filtered = clif.hospitalization.df[clif.hospitalization.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "adt_filtered = clif.adt.df[clif.adt.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "\n",
    "hosp_stitched, adt_stitched, encounter_mapping = stitch_encounters(\n",
    "    hospitalization=hosp_filtered,\n",
    "    adt=adt_filtered,\n",
    "    time_interval=6  \n",
    ")\n",
    "\n",
    "# Direct assignment without additional copies\n",
    "clif.hospitalization.df = hosp_stitched\n",
    "clif.adt.df = adt_stitched\n",
    "\n",
    "# Store the encounter mapping in the orchestrator for later use\n",
    "clif.encounter_mapping = encounter_mapping\n",
    "\n",
    "# Clean up intermediate variables\n",
    "del hosp_filtered, adt_filtered\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After your stitching code, add these calculations:\n",
    "\n",
    "# Calculate stitching statistics\n",
    "strobe_counts['1b_before_stitching'] = len(adult_hosp_ids)  # Original adult hospitalizations\n",
    "strobe_counts['1b_after_stitching'] = len(hosp_stitched['encounter_block'].unique())  # Unique encounter blocks after stitching\n",
    "strobe_counts['1b_stitched_hosp_ids'] = strobe_counts['1b_before_stitching'] - strobe_counts['1b_after_stitching']  # Number of hospitalizations that were linked\n",
    "\n",
    "print(f\"\\nEncounter Stitching Results:\")\n",
    "print(f\"   Number of unique hospitalizations before stitching: {strobe_counts['1b_before_stitching']:,}\")\n",
    "print(f\"   Number of unique encounter blocks after stitching: {strobe_counts['1b_after_stitching']:,}\")\n",
    "print(f\"   Number of linked hospitalization ids: {strobe_counts['1b_stitched_hosp_ids']:,}\")\n",
    "\n",
    "# Optional: Show the encounter mapping details\n",
    "print(f\"\\nEncounter Mapping Details:\")\n",
    "print(f\"   Total encounter mappings created: {len(encounter_mapping):,}\")\n",
    "if len(encounter_mapping) > 0:\n",
    "    # Show some examples of how many original hospitalizations were combined\n",
    "    mapping_counts = encounter_mapping.groupby('encounter_block').size()\n",
    "    print(f\"   Encounter blocks with multiple hospitalizations: {(mapping_counts > 1).sum():,}\")\n",
    "    print(f\"   Maximum hospitalizations combined into one block: {mapping_counts.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806ca75",
   "metadata": {},
   "source": [
    "# ADT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all_encounters with encounter_mapping to get encounter_block information\n",
    "all_encounters = pd.merge(all_encounters, encounter_mapping, on='hospitalization_id', how='left')\n",
    "\n",
    "# Convert location_category and discharge_category to lowercase in place (vectorized)\n",
    "all_encounters['location_category'] = all_encounters['location_category'].str.lower()\n",
    "all_encounters['discharge_category'] = all_encounters['discharge_category'].str.lower()\n",
    "all_encounters['admission_type_category'] = all_encounters['admission_type_category'].str.lower()\n",
    "\n",
    "# Create vectorized ICU and death masks\n",
    "icu_mask = all_encounters['location_category'].str.contains('icu', na=False)\n",
    "death_mask = all_encounters['discharge_category'].isin(['expired', 'hospice'])\n",
    "\n",
    "# Vectorized: For each encounter_block, does any row have ICU or death? (much faster)\n",
    "# Use groupby('encounter_block')[mask].transform('any') to vectorize\n",
    "all_encounters['icu_enc'] = icu_mask.groupby(all_encounters['encounter_block']).transform('any').astype(int)\n",
    "all_encounters['death_enc'] = death_mask.groupby(all_encounters['encounter_block']).transform('any').astype(int)\n",
    "\n",
    "# Cohort flag using logical OR (vectorized)\n",
    "all_encounters['cohort_enc'] = (all_encounters['icu_enc'] | all_encounters['death_enc']).astype(int)\n",
    "\n",
    "# Store hospitalization_ids for cohort_enc==1 in a list (as before)\n",
    "cohort_enc_hospitalization_ids = all_encounters.loc[all_encounters['cohort_enc'] == 1, 'hospitalization_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982478d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify encounters where death occurred\n",
    "death_encounters = all_encounters[all_encounters['death_enc'] == 1]\n",
    "# Identify those that never touched the ICU\n",
    "non_icu_deaths = death_encounters[~death_encounters['icu_enc'].astype(bool)]\n",
    "# Count the number of unique encounters with deaths outside of ICU\n",
    "num_deaths_outside_icu = non_icu_deaths['encounter_block'].nunique()\n",
    "# Calculate total deaths (unique encounter blocks with death)\n",
    "total_encounters = all_encounters['encounter_block'].nunique()\n",
    "# Calculate the percentage\n",
    "pct_deaths_outside_icu = (num_deaths_outside_icu / total_encounters * 100) if total_encounters > 0 else 0\n",
    "print(f\"Number of deaths outside ICU: {num_deaths_outside_icu} ({pct_deaths_outside_icu:.1f}% of all hospitalizations)\")\n",
    "\n",
    "# Add ICU encounters to strobe counts as 1_icu_encounters\n",
    "num_icu_encounters = all_encounters[all_encounters['icu_enc'] == 1]['encounter_block'].nunique()\n",
    "if 'strobe_counts' not in globals():\n",
    "    strobe_counts = {}\n",
    "strobe_counts['1_icu_encounters'] = num_icu_encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df96416",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cohort = all_encounters[\n",
    "    all_encounters['hospitalization_id'].isin(cohort_enc_hospitalization_ids)\n",
    "][['encounter_block', 'icu_enc', 'death_enc', 'cohort_enc']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1672f",
   "metadata": {},
   "source": [
    "# Respiratory Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Load Respiratory Support and Identify Patients on Advanced Respiratory support \n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" Loading Respiratory Support and Identifying IMV Patients\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nLoading respiratory_support table...\")\n",
    "clif.load_table('respiratory_support',\n",
    "                        columns=rst_required_columns,\n",
    "                        filters={'hospitalization_id': list(adult_hosp_ids)})\n",
    "print(f\"Respiratory support loaded ({len(clif.respiratory_support.df):,} rows)\")\n",
    "\n",
    "# Standardize category columns to lowercase\n",
    "print(f\"\\nStandardizing category columns...\")\n",
    "category_cols = [col for col in clif.respiratory_support.df.columns if col.endswith('_category')]\n",
    "for col in category_cols:\n",
    "    clif.respiratory_support.df[col] = clif.respiratory_support.df[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06229735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hospitalizations on advanced mechanical support\n",
    "print(f\"\\nIdentifying hospitalizations with advanced respiratory support devices...\")\n",
    "device_types = ['imv', 'nippv', 'cpap', 'high flow nc']\n",
    "clif.respiratory_support.df = pd.merge(clif.respiratory_support.df, encounter_mapping, \n",
    "                                        on='hospitalization_id', how='left')\n",
    "advanced_support_hosp_ids = clif.respiratory_support.df.loc[\n",
    "    clif.respiratory_support.df['device_category'].str.lower().isin([d.lower() for d in device_types]),\n",
    "    'encounter_block'\n",
    "].unique()\n",
    "print(f\"Hospitalizations with any advanced resp. device ({', '.join(device_types).upper()}): {len(advanced_support_hosp_ids):,}\")\n",
    "strobe_counts[\"2_advanced_resp_support_hospitalizations\"] = len(advanced_support_hosp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e77873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with advanced_support_hosp_ids and 'high_support_en' == 1\n",
    "advanced_support_df = pd.DataFrame({\n",
    "    'encounter_block': advanced_support_hosp_ids,\n",
    "    'high_support_enc': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a full join (outer merge) of final_cohort and advanced_support_df on 'encounter_block'\n",
    "final_cohort = final_cohort.merge(\n",
    "    advanced_support_df,\n",
    "    on='encounter_block',\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170325ca",
   "metadata": {},
   "source": [
    "# Vasoactives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efe800",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading medication_admin_continuous table...\")\n",
    "clif.load_table(\n",
    "    'medication_admin_continuous',\n",
    "    columns=meds_required_columns,\n",
    "    filters={\n",
    "        'hospitalization_id': list(adult_hosp_ids),\n",
    "        'med_category': meds_of_interest\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c276d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hospitalizations on advanced mechanical support\n",
    "print(f\"\\nIdentifying hospitalizations with advanced respiratory support devices...\")\n",
    "vasoactive_meds = ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "                   'dopamine', 'angiotensin']\n",
    "clif.medication_admin_continuous.df= pd.merge(clif.medication_admin_continuous.df, encounter_mapping, \n",
    "                                        on='hospitalization_id', how='left')\n",
    "vasoactive_hosp_ids = clif.medication_admin_continuous.df.loc[\n",
    "    clif.medication_admin_continuous.df['med_category'].str.lower().isin([d.lower() for d in vasoactive_meds]),\n",
    "    'encounter_block'\n",
    "].unique()\n",
    "print(f\"Hospitalizations with any vasoactives. device ({', '.join(vasoactive_meds).upper()}): {len(vasoactive_hosp_ids):,}\")\n",
    "strobe_counts[\"3_vasoactive_hospitalizations\"] = len(vasoactive_hosp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e522d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with advanced_support_hosp_ids and 'high_support_en' == 1\n",
    "vasoactives_df = pd.DataFrame({\n",
    "    'encounter_block': vasoactive_hosp_ids,\n",
    "    'vaso_support_enc': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join vasoactives_df with final cohort on hospitalization_id\n",
    "final_cohort = final_cohort.merge(\n",
    "    vasoactives_df,\n",
    "    on='encounter_block',\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1473ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing high_support_en means not on advanced support\n",
    "final_cohort['vaso_support_enc'] = final_cohort['vaso_support_enc'].fillna(0).astype(int)\n",
    "# Missing high_support_en means not on advanced support\n",
    "final_cohort['high_support_enc'] = final_cohort['high_support_enc'].fillna(0).astype(int)\n",
    "# Missing icu_enc means not ICU\n",
    "final_cohort['icu_enc'] = final_cohort['icu_enc'].fillna(0).astype(int)\n",
    "# Define the criteria for other critically ill\n",
    "final_cohort['other_critically_ill'] = (\n",
    "    (final_cohort[['icu_enc', 'vaso_support_enc', 'high_support_enc']].sum(axis=1) == 0)\n",
    ").astype(int)\n",
    "# Calculate the count\n",
    "strobe_counts['4_other_critically_ill'] = final_cohort.loc[final_cohort['other_critically_ill'] == 1, \n",
    "                                                            'encounter_block'].nunique()\n",
    "strobe_counts['5_all_critically_ill'] = final_cohort['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec68a76",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa9305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "strobe_counts_df = pd.DataFrame(list(strobe_counts.items()), columns=['count_name', 'count_value'])\n",
    "strobe_counts_df.to_csv('../output/final/tableone/strobe_counts.csv', index=False)\n",
    "# Calculate mortality rates\n",
    "mortality_rates = {\n",
    "    'ICU Hospitalizations': final_cohort.loc[final_cohort['icu_enc'] == 1, 'death_enc'].mean() * 100,\n",
    "    'Advanced Respiratory Support': final_cohort.loc[final_cohort['high_support_enc'] == 1, 'death_enc'].mean() * 100,\n",
    "    'Vasoactive Hospitalizations': final_cohort.loc[final_cohort['vaso_support_enc'] == 1, 'death_enc'].mean() * 100,\n",
    "    'Other Critically Ill': final_cohort.loc[final_cohort['other_critically_ill'] == 1, 'death_enc'].mean() * 100,\n",
    "    'All Critically Ill Adults': final_cohort['death_enc'].mean() * 100,\n",
    "}\n",
    "mortality_rates_df = pd.DataFrame(list(mortality_rates.items()), columns=['count_name', 'count_value'])\n",
    "mortality_rates_df.to_csv('../output/final/tableone/mortality_rates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdaffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df = encounter_mapping.copy()\n",
    "cohort_df = cohort_df[cohort_df['encounter_block'].isin(final_cohort['encounter_block'])]\n",
    "\n",
    "# Merge cohort_df with all_encounters on hospitalization_id\n",
    "cohort_df = cohort_df.merge(\n",
    "    all_encounters,\n",
    "    on=['hospitalization_id', 'encounter_block'],\n",
    "    how='left',\n",
    "    suffixes=('', '_allenc')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "def create_consort_diagram(strobe_counts, mortality_rates):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "    ax.set_xlim(-1, 13)\n",
    "    ax.set_ylim(0, 14)\n",
    "    ax.axis('off')\n",
    "\n",
    "    box_style = \"round,pad=0.1\"\n",
    "    boxes = {}\n",
    "\n",
    "    def create_box(x, y, width, height, text, box_id=None, fontsize=10, fontweight='normal'):\n",
    "        box = FancyBboxPatch(\n",
    "            (x - width/2, y - height/2), width, height,\n",
    "            boxstyle=box_style, facecolor='white', edgecolor='black', linewidth=1.5\n",
    "        )\n",
    "        ax.add_patch(box)\n",
    "        ax.text(x, y, text, ha='center', va='center', fontsize=fontsize, fontweight=fontweight, wrap=True)\n",
    "        \n",
    "        return {\n",
    "            'x': x, 'y': y, 'width': width, 'height': height,\n",
    "            'left': x - width/2, 'right': x + width/2,\n",
    "            'top': y + height/2, 'bottom': y - height/2\n",
    "        }\n",
    "\n",
    "    def create_arrow(from_box, to_box):\n",
    "        x1, y1 = from_box['x'], from_box['bottom'] - 0.1\n",
    "        x2, y2 = to_box['x'], to_box['top'] + 0.1\n",
    "        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "\n",
    "    ax.text(5, 13, 'Cohort', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Define and arrange the boxes\n",
    "    box1 = create_box(5, 12, 3, 0.7, \n",
    "                      f\"Total Hospitalizations\\nn = {strobe_counts['0_total_hospitalizations']:,}\",\n",
    "                      'total', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box2 = create_box(5, 10.5, 3, 0.7,\n",
    "                      f\"Adult Hospitalizations\\nn = {strobe_counts['1b_after_stitching']:,}\",\n",
    "                      'adult', fontsize=11, fontweight='bold')\n",
    "    create_arrow(box1, box2)\n",
    "\n",
    "    # Define ICU, respiratory support, vasoactive, and other critically ill categories\n",
    "    box3_icu = create_box(1, 8, 3, 0.9,\n",
    "                          f\"ICU Hospitalizations\\nn = {strobe_counts['1_icu_encounters']:,}\\nMortality: {mortality_rates['ICU Hospitalizations']:.2f}%\",\n",
    "                          'icu', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box3_resp = create_box(4.5, 8, 3, 0.9,\n",
    "                           f\"Advanced Respiratory Support\\nn = {strobe_counts['2_advanced_resp_support_hospitalizations']:,}\\nMortality: {mortality_rates['Advanced Respiratory Support']:.2f}%\",\n",
    "                           'resp_support', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box3_vaso = create_box(8, 8, 3, 0.9,\n",
    "                           f\"Vasoactive Hospitalizations\\nn = {strobe_counts['3_vasoactive_hospitalizations']:,}\\nMortality: {mortality_rates['Vasoactive Hospitalizations']:.2f}%\",\n",
    "                           'vasoactive', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box3_other = create_box(11.3, 8, 3, 0.9,\n",
    "                            f\"Other Critically Ill\\nn = {strobe_counts['4_other_critically_ill']:,}\\nMortality: {mortality_rates['Other Critically Ill']:.2f}%\",\n",
    "                            'other', fontsize=11, fontweight='bold')\n",
    "\n",
    "    create_arrow(box2, box3_icu)\n",
    "    create_arrow(box2, box3_resp)\n",
    "    create_arrow(box2, box3_vaso)\n",
    "    create_arrow(box2, box3_other)\n",
    "\n",
    "    # Add a final box for \"All Critically Ill Adults\"\n",
    "    box_final = create_box(5.7, 4.5, 5.2, 1.1,\n",
    "        f\"All Critically Ill Adults\\nn = {final_cohort['encounter_block'].nunique():,}\\nMortality: {mortality_rates['All Critically Ill Adults']:.2f}%\",\n",
    "        'all_critically_ill', fontsize=13, fontweight='bold')\n",
    "\n",
    "    # Do NOT draw arrows from the four groups to the all critically ill adults box\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../output/final/tableone/consort_flow_diagram.png', dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf13e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from upsetplot import UpSet, from_indicators\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='upsetplot')\n",
    "\n",
    "# Your UpSet plot code here...\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../output/final/tableone', exist_ok=True)\n",
    "\n",
    "# Prepare final_cohort data for UpSet plot\n",
    "summary_df = final_cohort[['encounter_block', 'icu_enc', 'death_enc', 'high_support_enc', 'vaso_support_enc']].drop_duplicates()\n",
    "\n",
    "# Rename columns to match CONSORT flow labels\n",
    "summary_df = summary_df.rename(columns={\n",
    "    'icu_enc': 'ICU Hospitalizations',\n",
    "    'death_enc': 'Died',\n",
    "    'high_support_enc': 'Advanced O2 Support',\n",
    "    'vaso_support_enc': 'Vasoactive Support'\n",
    "})\n",
    "\n",
    "# Convert to boolean for UpSet plot\n",
    "summary_df['ICU Hospitalizations'] = summary_df['ICU Hospitalizations'].astype(bool)\n",
    "summary_df['Died'] = summary_df['Died'].astype(bool)\n",
    "summary_df['Advanced O2 Support'] = summary_df['Advanced O2 Support'].astype(bool)\n",
    "summary_df['Vasoactive Support'] = summary_df['Vasoactive Support'].astype(bool)\n",
    "\n",
    "# Create UpSet plot\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "upset_data = from_indicators(\n",
    "    ['ICU Hospitalizations', 'Died', 'Advanced O2 Support', 'Vasoactive Support'], \n",
    "    data=summary_df.set_index('encounter_block')\n",
    ")\n",
    "\n",
    "upset = UpSet(upset_data, \n",
    "              subset_size='count',\n",
    "              show_counts=True,\n",
    "              sort_by='cardinality',\n",
    "              element_size=50,\n",
    "              with_lines=True)\n",
    "\n",
    "upset.plot(fig=fig)\n",
    "\n",
    "plt.subplots_adjust(left=0.2, bottom=0.2, right=0.95, top=0.85, hspace=0.3, wspace=0.3)\n",
    "plt.suptitle('Clinical Cohort Intersections', fontsize=16, y=0.95)\n",
    "\n",
    "# Adjust font sizes for better readability\n",
    "for ax in fig.get_axes():\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                 ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('../output/final/tableone/cohort_intersect_upset_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ee2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the diagram\n",
    "create_consort_diagram(strobe_counts, mortality_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e6e15",
   "metadata": {},
   "source": [
    "# Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b69828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter patient table to cohort\n",
    "patient_df = clif.patient.df.copy()\n",
    "patient_df = patient_df[['patient_id', 'race_category', 'ethnicity_category',\n",
    "                         'sex_category', 'death_dttm']]\n",
    "\n",
    "# Filter patient_df to those in cohort\n",
    "patient_df = patient_df[patient_df['patient_id'].isin(cohort_df['patient_id'])]\n",
    "# Merge patient_df with cohort_df on patient_id\n",
    "cohort_df = patient_df.merge(cohort_df, on='patient_id', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6bbe8",
   "metadata": {},
   "source": [
    "# Final cohort df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a351afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tableone_df = cohort_df[['patient_id', 'hospitalization_id', 'encounter_block', 'admission_dttm', \n",
    "                                'discharge_dttm', 'age_at_admission', 'discharge_category', 'admission_type_category',\n",
    "                                'race_category', 'ethnicity_category', 'sex_category', 'death_dttm', \n",
    "                                'icu_enc', 'death_enc', 'cohort_enc']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hosp_ids = cohort_df['hospitalization_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73310bee",
   "metadata": {},
   "source": [
    "# Hospital and ICU Admission Summary\n",
    "\n",
    "1. Get the first ICU dttm for ICU encounters \n",
    "2. Calculate ICU LOS and Hospital LOS for each encounter in days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_cohort = cohort_df[['patient_id', 'hospitalization_id', 'encounter_block',\n",
    "                       'hospital_id', 'in_dttm', 'out_dttm', 'location_category',\n",
    "                       'location_type']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974cdb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_admission_summary = (\n",
    "        adt_cohort\n",
    "        .groupby('encounter_block')\n",
    "        .agg(\n",
    "            min_in_dttm = ('in_dttm', 'min'),\n",
    "            max_out_dttm = ('out_dttm', 'max'),\n",
    "            first_admission_location = ('location_category', 'first')\n",
    "        )\n",
    ")\n",
    "hosp_admission_summary['hospital_length_of_stay_days'] = (\n",
    "    (hosp_admission_summary['max_out_dttm'] - hosp_admission_summary['min_in_dttm']) / pd.Timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the column, not the entire df\n",
    "adt_cohort['location_category'] = (\n",
    "    adt_cohort['location_category']\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# restrict to ICU rows\n",
    "icu_df = adt_cohort.query('location_category == \"icu\"')\n",
    "\n",
    "# find first ICU in time per 'encounter_block'\n",
    "first_in = (\n",
    "    icu_df\n",
    "     .groupby('encounter_block', as_index=False)\n",
    "     .agg(first_icu_in_dttm=('in_dttm', 'min'))\n",
    ")\n",
    "\n",
    "# join back to pull the matching out_dttm\n",
    "icu_summary = (\n",
    "    first_in\n",
    "      # bring in that one row’s out_dttm\n",
    "      .merge(\n",
    "          icu_df[['hospitalization_id','in_dttm','out_dttm', 'encounter_block']],\n",
    "          left_on=['encounter_block', 'first_icu_in_dttm'],\n",
    "          right_on=['encounter_block', 'in_dttm'],\n",
    "          how='left'\n",
    "      )\n",
    "      .rename(columns={'out_dttm':'first_icu_out_dttm'})\n",
    ")\n",
    "\n",
    "# compute LOS in days (out - in)\n",
    "icu_summary['first_icu_los_days'] = (\n",
    "    (icu_summary['first_icu_out_dttm'] - icu_summary['first_icu_in_dttm'])\n",
    "    .dt.total_seconds()\n",
    "    / (3600 * 24)\n",
    ")\n",
    "\n",
    "# trim to just the columns you need\n",
    "icu_summary = icu_summary[['encounter_block', 'first_icu_in_dttm',\n",
    "                           'first_icu_out_dttm','first_icu_los_days']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all_ids with icu_summary and hosp_admission_summary\n",
    "final_tableone_df = (\n",
    "    final_tableone_df\n",
    "    .merge(icu_summary, on='encounter_block', how='left')\n",
    "    .merge(hosp_admission_summary, on='encounter_block', how='left')\n",
    ")\n",
    "final_tableone_df['first_admission_location'] = final_tableone_df['first_admission_location'].fillna('Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc45679",
   "metadata": {},
   "source": [
    "# IMV encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only IMV hospitalizations\n",
    "clif.respiratory_support.df = clif.respiratory_support.df[\n",
    "    clif.respiratory_support.df['hospitalization_id'].isin(final_hosp_ids)\n",
    "].copy()\n",
    "print(f\"Respiratory support rows (IMV hospitalizations): {len(clif.respiratory_support.df):,}\")\n",
    "clif.respiratory_support.df = clif.respiratory_support.df.sort_values(['hospitalization_id', 'recorded_dttm'])\n",
    "\n",
    "# Standardize category columns to lowercase\n",
    "print(f\"\\nStandardizing category columns...\")\n",
    "category_cols = [col for col in clif.respiratory_support.df.columns if col.endswith('_category')]\n",
    "for col in category_cols:\n",
    "    clif.respiratory_support.df[col] = clif.respiratory_support.df[col].str.lower()\n",
    "\n",
    "from clifpy.utils import apply_outlier_handling\n",
    "apply_outlier_handling(clif.respiratory_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply waterfall processing to fill sparse data\n",
    "# print(f\"\\nApplying waterfall processing to respiratory support data. This can take 2-20 mins based on your system specs and cohort size...\")\n",
    "# clif.respiratory_support = clif.respiratory_support.waterfall(verbose=True)\n",
    "# print(f\"\\n Waterfall complete: {len(clif.respiratory_support.df):,} rows for {clif.respiratory_support.df['hospitalization_id'].nunique():,} unique hospitalizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf218e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with encounter_block mapping\n",
    "resp_stitched = clif.respiratory_support.df\n",
    "\n",
    "#  Identify IMV rows\n",
    "imv_mask = resp_stitched['device_category'].str.contains(\"imv\", case=False, na=False)\n",
    "resp_stitched_imv = resp_stitched[imv_mask].copy()\n",
    "\n",
    "# Create on_vent column for IMV records\n",
    "resp_stitched_imv['on_vent'] = 1\n",
    "\n",
    "# Get unique encounter IDs from resp_stitched_imv\n",
    "imv_encounters = resp_stitched_imv['encounter_block'].unique()\n",
    "\n",
    "print(f\"Number of IMV encounters: {len(imv_encounters):,}\")\n",
    "strobe_counts[\"IMV encounters\"] = len(imv_encounters)\n",
    "# Determine Vent Start/End for Each Encounter\n",
    "vent_start_end = resp_stitched_imv.groupby('encounter_block').agg(\n",
    "    vent_start_time=('recorded_dttm', 'min'),\n",
    "    vent_end_time=('recorded_dttm', 'max')\n",
    ").reset_index()\n",
    "\n",
    "#  Add on_vent flag to final_cohort\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    resp_stitched_imv[['encounter_block', 'on_vent']].drop_duplicates(),\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "final_tableone_df['on_vent'] = final_tableone_df['on_vent'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a710cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get Initial Mode Category (first mode after vent start)\n",
    "# Subset resp_stitched to only those encounters on IMV\n",
    "resp_imv = resp_stitched[resp_stitched['encounter_block'].isin(imv_encounters)].copy()\n",
    "\n",
    "# Merge in the vent_start_time\n",
    "resp_imv = resp_imv.merge(\n",
    "    vent_start_end[['encounter_block', 'vent_start_time']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Filter to only rows at or after vent start\n",
    "resp_post_start = resp_imv[\n",
    "    resp_imv['recorded_dttm'] >= resp_imv['vent_start_time']\n",
    "]\n",
    "\n",
    "# Group and take first non-NA mode_category per encounter\n",
    "initial_modes = (\n",
    "    resp_post_start\n",
    "    .sort_values(['encounter_block', 'recorded_dttm'])\n",
    "    .groupby('encounter_block', as_index=False)['mode_category']\n",
    "    .first()\n",
    "    .rename(columns={'mode_category': 'initial_mode_category'})\n",
    ")\n",
    "\n",
    "# Fill any entirely-missing groups with \"Missing\"\n",
    "initial_modes['initial_mode_category'] = initial_modes['initial_mode_category'].fillna('Missing')\n",
    "\n",
    "# Merge back onto final_cohort\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    initial_modes,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f68ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tableone_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If some encounters never went on vent, fill those too\n",
    "final_tableone_df['initial_mode_category'] = final_tableone_df['initial_mode_category'].fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9857f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calculate Ventilator Settings Statistics (Median and IQR)\n",
    "# Filter resp_stitched to only those encounters on IMV\n",
    "resp_stitched_final = resp_stitched[resp_stitched['encounter_block'].isin(imv_encounters)]\n",
    "\n",
    "# Define numeric columns to aggregate\n",
    "numeric_cols = [\n",
    "    'fio2_set', 'lpm_set', 'resp_rate_set', 'peep_set',\n",
    "    'tidal_volume_set', 'pressure_control_set', 'pressure_support_set'\n",
    "]\n",
    "\n",
    "# Build named aggregation dict\n",
    "named_aggs = {}\n",
    "for col in numeric_cols:\n",
    "    named_aggs[f'{col}_median'] = (col, 'median')\n",
    "    named_aggs[f'{col}_q1'] = (col, lambda x: x.quantile(0.25))\n",
    "    named_aggs[f'{col}_q3'] = (col, lambda x: x.quantile(0.75))\n",
    "\n",
    "# Aggregate ventilator settings\n",
    "vent_stats = (\n",
    "    resp_stitched_final\n",
    "    .groupby('encounter_block', as_index=False)\n",
    "    .agg(**named_aggs)\n",
    ")\n",
    "\n",
    "# Merge vent stats back to final_cohort\n",
    "final_tableone_df = final_tableone_df.merge(vent_stats, on='encounter_block', how='left')\n",
    "\n",
    "#  Find First Location at IMV Start (closest ADT location to vent start)\n",
    "# Get minimal ADT cohort with required columns and merge with encounter_block\n",
    "print(\"Find First Location at IMV Start (closest ADT location to vent start)\")\n",
    "adt_cohort_subset = clif.adt.df[['hospitalization_id', 'in_dttm', 'location_category']].merge(\n",
    "    encounter_mapping[['hospitalization_id', 'encounter_block']],\n",
    "    on='hospitalization_id'\n",
    ")\n",
    "\n",
    "# Merge with vent start times\n",
    "adt_vent = pd.merge(\n",
    "    vent_start_end[['encounter_block', 'vent_start_time']],\n",
    "    adt_cohort_subset,\n",
    "    on='encounter_block'\n",
    ")\n",
    "\n",
    "# Calculate time difference between vent start and ADT in_dttm\n",
    "adt_vent['time_diff'] = abs(adt_vent['vent_start_time'] - adt_vent['in_dttm'])\n",
    "\n",
    "# Get the closest ADT row for each encounter block\n",
    "closest_adt = (\n",
    "    adt_vent\n",
    "    .sort_values('time_diff')\n",
    "    .groupby('encounter_block')\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "closest_adt = closest_adt.rename(columns={'location_category': 'first_location_imv'})\n",
    "\n",
    "# Merge back to final_cohort\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    closest_adt[['encounter_block', 'first_location_imv']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\n=== IMV Encounter Summary Complete ===\")\n",
    "print(f\"Total encounters: {len(final_tableone_df):,}\")\n",
    "print(f\"Encounters on IMV: {final_tableone_df['on_vent'].sum():,}\")\n",
    "print(f\"Initial mode categories:\\n{final_tableone_df['initial_mode_category'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14935dab",
   "metadata": {},
   "source": [
    "# Meds, Labs and Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58102c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "meds_of_interest   = [\n",
    "                       'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "                       'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol',\n",
    "                       'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', \n",
    "                       'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium'\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc589f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Load Vitals\n",
    "# ----------------------------------------------------------------------------\n",
    "print(f\"\\nLoading vitals table...\")\n",
    "clif.load_table(\n",
    "    'vitals',\n",
    "    columns=vitals_required_columns,\n",
    "    filters={\n",
    "        'hospitalization_id': final_hosp_ids,\n",
    "        'vital_category': vitals_of_interest\n",
    "    }\n",
    ")\n",
    "print(f\"   Vitals loaded: {len(clif.vitals.df):,} rows\")\n",
    "print(f\"   Unique vital categories: {clif.vitals.df['vital_category'].nunique()}\")\n",
    "print(f\"   Unique vital hospitalizations: {clif.vitals.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Load Labs\n",
    "# ----------------------------------------------------------------------------\n",
    "print(f\"\\nLoading labs table...\")\n",
    "clif.load_table(\n",
    "    'labs',\n",
    "    filters={\n",
    "        'hospitalization_id': final_hosp_ids\n",
    "    }\n",
    ")\n",
    "print(f\"   Labs loaded: {len(clif.labs.df):,} rows\")\n",
    "print(f\"   Unique lab categories: {clif.labs.df['lab_category'].nunique()}\")\n",
    "print(f\"   Unique lab hospitalizations: {clif.labs.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Load Medication Administration (Continuous)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(f\"\\nLoading medication_admin_continuous table...\")\n",
    "clif.load_table(\n",
    "    'medication_admin_continuous',\n",
    "    columns=meds_required_columns,\n",
    "    filters={\n",
    "        'hospitalization_id': final_hosp_ids,\n",
    "        'med_category': meds_of_interest\n",
    "    }\n",
    ")\n",
    "print(f\"   Medications loaded: {len(clif.medication_admin_continuous.df):,} rows\")\n",
    "print(f\"   Unique medication categories: {clif.medication_admin_continuous.df['med_category'].nunique()}\")\n",
    "print(f\"   Unique medication_admin_continuous hospitalizations: {clif.medication_admin_continuous.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Summary\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Table Loading Summary\")\n",
    "print(\"=\" * 80)  \n",
    "\n",
    "loaded_tables = clif.get_loaded_tables()\n",
    "for table_name in loaded_tables:\n",
    "    table = getattr(clif, table_name)\n",
    "    print(f\"   {table_name}: {len(table.df):,} rows\")\n",
    "\n",
    "print(f\"\\nCohort size: {len(final_hosp_ids):,} hospitalizations\")\n",
    "\n",
    "# Free memory\n",
    "freed = gc.collect()\n",
    "print(f\"\\nFreed {freed} objects from memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbbfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clif.create_wide_dataset(\n",
    "#     hospitalization_ids=final_hosp_ids,\n",
    "#     tables_to_load=['vitals', 'labs', 'medication_admin_continuous' ],\n",
    "#     category_filters={\n",
    "#         'vitals': vitals_of_interest,\n",
    "#         'labs': labs_of_interest,\n",
    "#         'medication_admin_continuous': meds_of_interest\n",
    "#     },\n",
    "#     show_progress=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Access the created dataset\n",
    "# wide_df = clif.wide_df\n",
    "# print(f\"Sample dataset shape: {wide_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f0fc25",
   "metadata": {},
   "source": [
    "# SOFA Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing ClifOrchestrator for SOFA computation...\")\n",
    "co = ClifOrchestrator(\n",
    "    data_directory=config['tables_path'],\n",
    "    filetype=config['file_type'],\n",
    "    timezone=config['timezone']\n",
    ")\n",
    "print(\"✅ ClifOrchestrator initialized\")\n",
    "\n",
    "# Filter to icu_enc == 1\n",
    "sofa_cohort_df = final_tableone_df[final_tableone_df['icu_enc'] == 1][['hospitalization_id', 'encounter_block', 'icu_enc', 'first_icu_in_dttm']]\n",
    "sofa_cohort_df['start_time'] = sofa_cohort_df['first_icu_in_dttm']\n",
    "sofa_cohort_df['end_time'] = sofa_cohort_df['start_time'] + pd.Timedelta(hours=24)\n",
    "sofa_cohort_df= sofa_cohort_df[['hospitalization_id', 'encounter_block', 'start_time', 'end_time']]\n",
    "sofa_cohort_ids = cohort_df['hospitalization_id'].astype(str).unique().tolist()\n",
    "\n",
    "\n",
    "# Load required tables for SOFA computation with cohort filtering\n",
    "print(\"Loading required tables for SOFA computation...\")\n",
    "print(\"SOFA requires: Labs (creatinine, platelet_count, po2_arterial, bilirubin_total)\")\n",
    "print(\"               Vitals (map, spo2)\")\n",
    "print(\"               Assessments (gcs_total)\")\n",
    "print(\"               Medications (norepinephrine, epinephrine, dopamine, dobutamine)\")\n",
    "print(\"               Respiratory (device_category, fio2_set)\")\n",
    "\n",
    "# Define columns to load for each table (optimize memory usage)\n",
    "sofa_columns = {\n",
    "    'labs': ['hospitalization_id', 'lab_result_dttm', 'lab_category', 'lab_value', 'lab_value_numeric'],\n",
    "    'vitals': ['hospitalization_id', 'recorded_dttm', 'vital_category', 'vital_value'],\n",
    "    'patient_assessments': ['hospitalization_id', 'recorded_dttm', 'assessment_category', 'numerical_value'],\n",
    "    'medication_admin_continuous': None,  # Load all columns\n",
    "    'respiratory_support': None  # Load all columns\n",
    "}\n",
    "\n",
    "sofa_tables = ['labs', 'vitals', 'patient_assessments', 'medication_admin_continuous', 'respiratory_support']\n",
    "\n",
    "for table_name in sofa_tables:\n",
    "    table_cols = sofa_columns.get(table_name)\n",
    "    print(f\"Loading {table_name} with {len(table_cols) if table_cols else 'all'} columns and {len(sofa_cohort_ids)} hospitalization filters...\")\n",
    "    co.load_table(\n",
    "        table_name,\n",
    "        filters={'hospitalization_id': sofa_cohort_ids},\n",
    "        columns=table_cols\n",
    "    )\n",
    "\n",
    "co.encounter_mapping = encounter_mapping[encounter_mapping['hospitalization_id'].isin(cohort_df['hospitalization_id'])]\n",
    "print(\"✅ All required tables loaded for SOFA computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b7135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert medication units to mcg/kg/min for SOFA computation\n",
    "print(\"Converting medication units to mcg/kg/min for SOFA...\")\n",
    "\n",
    "# Define preferred units for SOFA medications\n",
    "preferred_units = {\n",
    "    'norepinephrine': 'mcg/kg/min',\n",
    "    'epinephrine': 'mcg/kg/min',\n",
    "    'dopamine': 'mcg/kg/min',\n",
    "    'dobutamine': 'mcg/kg/min'\n",
    "}\n",
    "\n",
    "print(f\"Converting {len(preferred_units)} medications: {list(preferred_units.keys())}\")\n",
    "\n",
    "# Convert units (uses vitals table for weight data)\n",
    "co.convert_dose_units_for_continuous_meds(\n",
    "    preferred_units=preferred_units,\n",
    "    override = True, \n",
    "    save_to_table=True  # Saves to co.medication_admin_continuous.df_converted\n",
    ")\n",
    "\n",
    "# Check conversion results\n",
    "conversion_counts = co.medication_admin_continuous.conversion_counts\n",
    "\n",
    "print(\"\\n=== Conversion Summary ===\")\n",
    "print(f\"Total conversion records: {len(conversion_counts):,}\")\n",
    "\n",
    "# Check for conversion failures\n",
    "success_count = conversion_counts[conversion_counts['_convert_status'] == 'success']['count'].sum()\n",
    "total_count = conversion_counts['count'].sum()\n",
    "\n",
    "print(f\"Successful conversions: {success_count:,} / {total_count:,} ({100*success_count/total_count:.1f}%)\")\n",
    "\n",
    "# Show any failed conversions\n",
    "failed_conversions = conversion_counts[conversion_counts['_convert_status'] != 'success']\n",
    "if len(failed_conversions) > 0:\n",
    "    print(f\"\\n⚠️ Found {len(failed_conversions)} conversion issues:\")\n",
    "    for _, row in failed_conversions.head(10).iterrows():\n",
    "        print(f\"  {row['med_category']}: {row['_clean_unit']} → {row['_convert_status']} ({row['count']} records)\")\n",
    "else:\n",
    "    print(\"✅ All conversions successful!\")\n",
    "print(\"\\n✅ Medication unit conversion completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ab97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing SOFA\")\n",
    "sofa_scores = co.compute_sofa_scores(\n",
    "    cohort_df=sofa_cohort_df,\n",
    "    id_name='encounter_block'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ba0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_scores = sofa_scores.merge(\n",
    "    final_tableone_df[['encounter_block', 'death_enc']], \n",
    "    how='left', \n",
    "    on='encounter_block'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a98e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#  Prepare the data\n",
    "# Group by SOFA score and calculate mortality rate and counts\n",
    "sofa_mortality = sofa_scores.groupby('sofa_total').agg({\n",
    "    'death_enc': ['mean', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "sofa_mortality.columns = ['sofa_score', 'mortality_rate', 'count']\n",
    "sofa_mortality['mortality_rate'] = sofa_mortality['mortality_rate'] * 100  # Convert to percentage\n",
    "\n",
    "# Step 2: Calculate confidence intervals (optional, for error bars)\n",
    "# Using Wilson score interval for binomial proportions\n",
    "def wilson_ci(successes, n, confidence=0.95):\n",
    "    from scipy import stats\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    p_hat = successes / n\n",
    "    denominator = 1 + z**2 / n\n",
    "    center = (p_hat + z**2 / (2*n)) / denominator\n",
    "    margin = z * np.sqrt((p_hat * (1 - p_hat) + z**2 / (4*n)) / n) / denominator\n",
    "    return center * 100, margin * 100\n",
    "\n",
    "# Calculate number of deaths per score\n",
    "sofa_mortality['deaths'] = (sofa_mortality['mortality_rate'] / 100) * sofa_mortality['count']\n",
    "\n",
    "# Calculate confidence intervals\n",
    "ci_data = [wilson_ci(deaths, n) if n > 0 else (0, 0) \n",
    "           for deaths, n in zip(sofa_mortality['deaths'], sofa_mortality['count'])]\n",
    "sofa_mortality['ci_center'] = [x[0] for x in ci_data]\n",
    "sofa_mortality['ci_margin'] = [x[1] for x in ci_data]\n",
    "\n",
    "# Step 3: Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Create bar chart\n",
    "bars = ax.bar(sofa_mortality['sofa_score'], \n",
    "              sofa_mortality['mortality_rate'],\n",
    "              color='#7FA8B8',  # Steel blue color similar to the image\n",
    "              edgecolor='black',\n",
    "              linewidth=0.5,\n",
    "              alpha=0.9)\n",
    "\n",
    "# Add error bars\n",
    "ax.errorbar(sofa_mortality['sofa_score'], \n",
    "            sofa_mortality['mortality_rate'],\n",
    "            yerr=sofa_mortality['ci_margin'],\n",
    "            fmt='none',\n",
    "            ecolor='black',\n",
    "            capsize=3,\n",
    "            capthick=1,\n",
    "            alpha=0.7)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('SOFA Score', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Mortality, %', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Mortality by SOFA Score (First 24hr of ICU admission)', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Add grid for readability\n",
    "ax.yaxis.grid(True, linestyle='-', alpha=0.3, color='gray')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Set x-axis ticks to show all SOFA scores\n",
    "ax.set_xticks(range(int(sofa_mortality['sofa_score'].min()), \n",
    "                    int(sofa_mortality['sofa_score'].max()) + 1))\n",
    "\n",
    "# Add count labels below x-axis\n",
    "counts_text = '\\n'.join([\n",
    "    'No. of patients per score',\n",
    "    '  '.join([f'{int(count)}' for count in sofa_mortality['count']])\n",
    "])\n",
    "\n",
    "# Create a second table-like annotation below the plot\n",
    "fig.text(0.1, -0.05, 'No. of patients per score', \n",
    "         ha='left', fontsize=10, weight='bold')\n",
    "\n",
    "# Add individual counts\n",
    "x_positions = np.linspace(0.15, 0.9, len(sofa_mortality))\n",
    "for i, (score, count) in enumerate(zip(sofa_mortality['sofa_score'], sofa_mortality['count'])):\n",
    "    if i < len(x_positions):\n",
    "        fig.text(x_positions[i], -0.08, f'{int(count)}', \n",
    "                ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)  # Make room for patient counts\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('../output/final/sofa_mortality_histogram.png', \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Print summary statistics\n",
    "print(\"\\n=== SOFA Score Mortality Summary ===\")\n",
    "print(f\"Total encounters: {sofa_scores['encounter_block'].nunique():,}\")\n",
    "print(f\"SOFA score range: {sofa_mortality['sofa_score'].min():.0f} - {sofa_mortality['sofa_score'].max():.0f}\")\n",
    "print(f\"\\nMortality by SOFA score (First 24hr of ICU admission)\")\n",
    "print(sofa_mortality[['sofa_score', 'mortality_rate', 'count']].to_string(index=False))\n",
    "\n",
    "# Step 5: Prepare data for CSV export\n",
    "# Calculate lower and upper confidence interval bounds\n",
    "sofa_mortality['ci_lower'] = sofa_mortality['mortality_rate'] - sofa_mortality['ci_margin']\n",
    "sofa_mortality['ci_upper'] = sofa_mortality['mortality_rate'] + sofa_mortality['ci_margin']\n",
    "\n",
    "# Ensure CI bounds are within valid range [0, 100]\n",
    "sofa_mortality['ci_lower'] = sofa_mortality['ci_lower'].clip(lower=0)\n",
    "sofa_mortality['ci_upper'] = sofa_mortality['ci_upper'].clip(upper=100)\n",
    "\n",
    "# Create export dataframe with all relevant columns\n",
    "sofa_export = sofa_mortality[[\n",
    "    'sofa_score', \n",
    "    'count',\n",
    "    'deaths',\n",
    "    'mortality_rate', \n",
    "    'ci_lower',\n",
    "    'ci_upper',\n",
    "    'ci_margin'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "sofa_export.columns = [\n",
    "    'sofa_score',\n",
    "    'n_encounters',\n",
    "    'n_deaths',\n",
    "    'mortality_rate_percent',\n",
    "    'ci_lower_95',\n",
    "    'ci_upper_95',\n",
    "    'ci_margin_95'\n",
    "]\n",
    "\n",
    "# Round numeric columns for readability\n",
    "sofa_export['n_encounters'] = sofa_export['n_encounters'].astype(int)\n",
    "sofa_export['n_deaths'] = sofa_export['n_deaths'].round(0).astype(int)\n",
    "sofa_export['mortality_rate_percent'] = sofa_export['mortality_rate_percent'].round(2)\n",
    "sofa_export['ci_lower_95'] = sofa_export['ci_lower_95'].round(2)\n",
    "sofa_export['ci_upper_95'] = sofa_export['ci_upper_95'].round(2)\n",
    "sofa_export['ci_margin_95'] = sofa_export['ci_margin_95'].round(2)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = '../output/final/sofa_mortality_summary.csv'\n",
    "sofa_export.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n=== SOFA Mortality Summary Saved ===\")\n",
    "print(f\"File saved to: {output_path}\")\n",
    "print(f\"\\nPreview of saved data:\")\n",
    "print(sofa_export.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94411ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "569e50ea",
   "metadata": {},
   "source": [
    "# Comorbidity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading vitals table...\")\n",
    "clif.load_table(\n",
    "    'hospital_diagnosis',\n",
    "    filters={\n",
    "        'hospitalization_id': final_hosp_ids\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clifpy.utils.comorbidity import calculate_cci\n",
    "cci_results = calculate_cci( clif.hospital_diagnosis, hierarchy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d26c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cci_results = cci_results.merge(encounter_mapping, on=\"hospitalization_id\")\n",
    "cci_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comorbidities per 1000 hospitalizations, and save results WITH statistical summaries in CSV\n",
    "\n",
    "# Step 1: Get the total number of unique hospitalizations\n",
    "total_hospitalizations = cci_results['hospitalization_id'].nunique()\n",
    "print(f\"Total hospitalizations: {total_hospitalizations:,}\")\n",
    "\n",
    "# Step 2: Define the comorbidity columns (exclude IDs and total score)\n",
    "exclude_columns = {'hospitalization_id', 'encounter_block', 'cci_score'}\n",
    "comorbidity_columns = [col for col in cci_results.columns if col not in exclude_columns]\n",
    "\n",
    "# Step 3: Calculate the count of each comorbidity (assume binary indicators)\n",
    "comorbidity_counts = cci_results[comorbidity_columns].sum()\n",
    "\n",
    "# Step 4: Compute prevalence rates\n",
    "comorbidity_per_1000 = (comorbidity_counts / total_hospitalizations) * 1000\n",
    "prevalence_percent = (comorbidity_counts.values / total_hospitalizations * 100).round(2)\n",
    "\n",
    "# Step 5: Create a summary dataframe\n",
    "comorbidity_summary = pd.DataFrame({\n",
    "    'comorbidity': comorbidity_columns,\n",
    "    'n_patients': comorbidity_counts.values,\n",
    "    'prevalence_percent': prevalence_percent,\n",
    "    'per_1000_hospitalizations': comorbidity_per_1000.values.round(1)\n",
    "})\n",
    "\n",
    "# Sort by per 1000 prevalence\n",
    "comorbidity_summary = comorbidity_summary.sort_values('per_1000_hospitalizations', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Prepare summary statistics for output\n",
    "total_comorbidities = int(comorbidity_counts.sum())\n",
    "avg_comorbidities_per_hosp = total_comorbidities / total_hospitalizations if total_hospitalizations > 0 else 0\n",
    "most_common_comorbidity = comorbidity_summary.iloc[0]['comorbidity']\n",
    "most_common_per_1000 = comorbidity_summary.iloc[0]['per_1000_hospitalizations']\n",
    "\n",
    "# Pretty-print and summary to console\n",
    "print(\"\\n=== Comorbidities per 1000 Hospitalizations ===\")\n",
    "print(comorbidity_summary.to_string(index=False))\n",
    "print(f\"\\n=== Summary Statistics ===\")\n",
    "print(f\"Total hospitalizations: {total_hospitalizations:,}\")\n",
    "print(f\"Total comorbidities across all patients: {total_comorbidities:,}\")\n",
    "print(f\"Average comorbidities per hospitalization: {avg_comorbidities_per_hosp:.2f}\")\n",
    "print(f\"Most common comorbidity: {most_common_comorbidity} ({most_common_per_1000:.1f} per 1000)\")\n",
    "\n",
    "# Step 7: Save both table and summary statistics to CSV\n",
    "\n",
    "# First, write the comorbidity table to CSV\n",
    "out_csv = '../output/final/comorbidities_per_1000_hospitalizations.csv'\n",
    "comorbidity_summary.to_csv(out_csv, index=False)\n",
    "\n",
    "# Write summary statistics to a second csv, and then append to same file as lines at the end\n",
    "import csv\n",
    "summary_stats = [\n",
    "    ['Total hospitalizations', total_hospitalizations],\n",
    "    ['Total comorbidities across all patients', total_comorbidities],\n",
    "    ['Average comorbidities per hospitalization', f\"{avg_comorbidities_per_hosp:.2f}\"],\n",
    "    ['Most common comorbidity', most_common_comorbidity],\n",
    "    ['Most common: per 1000 hospitalizations', f\"{most_common_per_1000:.1f}\"],\n",
    "]\n",
    "\n",
    "# Save the summary stats to a separate CSV for clarity (and also appending to the main comorbidity file for convenience)\n",
    "summary_csv = '../output/final/comorbidities_per_1000_hospitalizations_summary.csv'\n",
    "with open(summary_csv, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Metric', 'Value'])\n",
    "    for row in summary_stats:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"\\nComorbidity table saved to: {out_csv}\")\n",
    "print(f\"Summary statistics saved to: {summary_csv}\")\n",
    "\n",
    "# Optionally: also append to the main file after a blank line (add after main table)\n",
    "with open(out_csv, 'a', newline='') as f:\n",
    "    f.write('\\n')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([])\n",
    "    writer.writerow(['=== Summary Statistics ===', ''])\n",
    "    writer.writerow(['Metric', 'Value'])\n",
    "    for row in summary_stats:\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Step 8: Bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars = ax.barh(\n",
    "    comorbidity_summary['comorbidity'], \n",
    "    comorbidity_summary['per_1000_hospitalizations'],\n",
    "    color='#7FA8B8',\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Per 1000 Hospitalizations', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Comorbidity', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comorbidity Prevalence per 1000 Hospitalizations', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(comorbidity_summary.iterrows()):\n",
    "    ax.text(row['per_1000_hospitalizations'] + 5, i, f\"{row['per_1000_hospitalizations']:.1f}\", va='center', fontsize=9)\n",
    "\n",
    "ax.grid(axis='x', linestyle='-', alpha=0.3)\n",
    "ax.set_axisbelow(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/comorbidities_per_1000_barplot.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3d832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b0160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
