{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cc294e",
   "metadata": {},
   "source": [
    "## CLIF Table One\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "Date v1: May 13, 2025\n",
    "\n",
    "This script identifies the cohort of encounters with at least one ICU stay and then summarizes the cohort data into one table. \n",
    "\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`, `clif_patient_assessments`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **clif_patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **clif_hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`,`discharge_dttm`, `age_at_admission` | - |\n",
    "| **clif_adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category` | - |\n",
    "| **clif_vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | weight_kg |\n",
    "| **clif_labs** | `hospitalization_id`, `lab_result_dttm`, `lab_order_dttm`, `lab_category`, `lab_value_numeric` | creatinine, bilirubin_total, po2_arterial, platelet_count |\n",
    "| **clif_medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin(optional) |\n",
    "| **clif_respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`,  `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs`, `tidal_volume_set`, `pressure_control_set`, `pressure_support_set` | - |\n",
    "| **clif_patient_assessments** | `hospitalization_id`, `recorded_dttm` , `assessment_category`, `numerical_value`| `gcs_total` |\n",
    "| **clif_crrt_therapy** | `hospitalization_id`, `recorded_dttm` | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bf46d",
   "metadata": {},
   "source": [
    "# Cohort Identification\n",
    "\n",
    "\n",
    "## Inclusion \n",
    "1. Adults\n",
    "2. Patients with at least one ICU stay or those who had only emergency department or ward encounters and either died or received life support at any point. Life support is defined as the administration of any vasoactive drugs or respiratory support exceeding low-flow oxygen.\n",
    "\n",
    "Respiratory support device: 'IMV', 'NIPPV', 'CPAP', 'High Flow NC'  \n",
    "\n",
    "Vasoactive: 'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d51b416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Verification ===\n",
      "Python executable: /Users/kavenchhikara/Desktop/CLIF/CLIF-TableOne/.clif_table_one/bin/python\n",
      "Python version: 3.9.6 (default, Apr 30 2025, 02:07:17) \n",
      "[Clang 17.0.0 (clang-1700.0.13.5)]\n",
      "clifpy version: 0.2.1\n",
      "clifpy location: /Users/kavenchhikara/Desktop/CLIF/CLIF-TableOne/.clif_table_one/lib/python3.9/site-packages/clifpy/__init__.py\n",
      "\n",
      "=== Python Path Check ===\n",
      "✅ Clean environment - no local CLIFpy in path\n",
      "\n",
      "=== Working Directory ===\n",
      "Current directory: /Users/kavenchhikara/Desktop/CLIF/CLIF-TableOne/code\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import clifpy\n",
    "import os\n",
    "\n",
    "print(\"=== Environment Verification ===\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"clifpy version: {clifpy.__version__}\")\n",
    "print(f\"clifpy location: {clifpy.__file__}\")\n",
    "\n",
    "print(\"\\n=== Python Path Check ===\")\n",
    "local_clifpy_path = \"/Users/kavenchhikara/Desktop/CLIF/CLIFpy\"\n",
    "if any(local_clifpy_path in path for path in sys.path):\n",
    "    print(\"⚠️  WARNING: Local CLIFpy still in path!\")\n",
    "    for path in sys.path:\n",
    "        if local_clifpy_path in path:\n",
    "            print(f\"   Found: {path}\")\n",
    "else:\n",
    "    print(\"✅ Clean environment - no local CLIFpy in path\")\n",
    "\n",
    "print(f\"\\n=== Working Directory ===\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb9a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=� Configuration:\n",
      "   Data directory: /Users/kavenchhikara/Library/CloudStorage/Box-Box/RCLIF_data/CLIF_2018_24/WIP_2_1\n",
      "   File type: parquet\n",
      "   Timezone: US/Central\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = \"../config/config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "## import outlier json\n",
    "# with open('../config/outlier_config.json', 'r', encoding='utf-8') as f:\n",
    "#     outlier_cfg = json.load(f)\n",
    "\n",
    "print(f\"\\n=� Configuration:\")\n",
    "print(f\"   Data directory: {config['tables_path']}\")\n",
    "print(f\"   File type: {config['file_type']}\")\n",
    "print(f\"   Timezone: {config['timezone']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127edcbe",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4319503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Defining Required Data Elements\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Defining Required Data Elements\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Full patient table \n",
    "\n",
    "# Full hospitalization table \n",
    "\n",
    "# Full ADT table\n",
    "\n",
    "# Vitals\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "# Respiratory Support \n",
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set', \n",
    "    'pressure_control_set',\n",
    "    'pressure_support_set',\n",
    "    'peak_inspiratory_pressure_set',\n",
    "    'peak_inspiratory_pressure_obs',\n",
    "    'plateau_pressure_obs',\n",
    "    'minute_vent_obs'\n",
    "]\n",
    "#Labs\n",
    "# labs_required_columns = [\n",
    "#     'hospitalization_id',\n",
    "#     'lab_result_dttm',\n",
    "#     'lab_category',\n",
    "#     'lab_value',\n",
    "#     'lab_value_numeric'\n",
    "# ]\n",
    "# labs_of_interest = ['po2_arterial','pco2_arterial', 'ph_arterial','ph_venous', 'bicarbonate','so2_arterial',\n",
    "#                     'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', \n",
    "#                     'bun', 'glucose_serum', 'lactate', 'hemoglobin' ]\n",
    "\n",
    "# # Continuous administered meds\n",
    "# meds_required_columns = [\n",
    "#     'hospitalization_id',\n",
    "#     'admin_dttm',\n",
    "#     'med_name',\n",
    "#     'med_category',\n",
    "#     'med_dose',\n",
    "#     'med_dose_unit'\n",
    "# ]\n",
    "# meds_of_interest = [\n",
    "#     'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "#     'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol',\n",
    "#     'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', \n",
    "#     'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium'\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# # Full crrt table\n",
    "# crrt_required_columns = [\n",
    "#     'hospitalization_id',\n",
    "#     'recorded_dttm',\n",
    "#     'crrt_mode_category',\n",
    "#     'blood_flow_rate',\n",
    "#     'pre_filter_replacement_fluid_rate',\n",
    "#     'post_filter_replacement_fluid_rate',\n",
    "#     'dialysate_flow_rate',\n",
    "#     'ultrafiltration_out'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee461357",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7cbe42",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65fb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_hourly_sequence(group):\n",
    "#     blk = group.name  # use group name from groupby\n",
    "#     start_time = group['vent_episode_start_dttm'].iloc[0]\n",
    "#     end_time   = group['vent_end_dttm_72h'].iloc[0]\n",
    "#     hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "#     return pd.DataFrame({\n",
    "#         'hospitalization_id': blk,\n",
    "#         'recorded_dttm': hourly_timestamps\n",
    "#     })\n",
    "\n",
    "# def calculate_ibw(height_cm, sex):\n",
    "#     if pd.isna(height_cm) or pd.isna(sex):\n",
    "#         return np.nan\n",
    "#     height_inches = height_cm / 2.54\n",
    "#     sex = str(sex).lower()\n",
    "#     if sex == 'male':\n",
    "#         return 50 + 2.3 * (height_inches - 60)\n",
    "#     elif sex == 'female':\n",
    "#         return 45.5 + 2.3 * (height_inches - 60)\n",
    "#     else:\n",
    "#         return np.nan\n",
    "\n",
    "# def calculate_base_excess(ph, hco3):\n",
    "#     \"\"\"\n",
    "#     Calculate Base Excess using simplified formula\n",
    "#     BE = (HCO3 - 24.4) + (8.3 * (pH - 7.4))\n",
    "#     \"\"\"\n",
    "#     return (hco3 - 24.4) + (8.3 * (ph - 7.4))\n",
    "\n",
    "# def calculate_pf_ratio(po2, fio2):\n",
    "#     \"\"\"\n",
    "#     Vectorized calculation of P/F ratio (PaO2/FiO2)\n",
    "#     FiO2 should be as fraction (0.21-1.0), not percentage\n",
    "#     Handles pandas Series input.\n",
    "#     \"\"\"\n",
    "#     fio2 = fio2.copy()\n",
    "#     # Convert percentage to fraction if needed\n",
    "#     mask_pct = fio2 > 1\n",
    "#     fio2[mask_pct] = fio2[mask_pct] / 100\n",
    "#     # Set minimum fio2 to 0.21 (room air)\n",
    "#     fio2 = fio2.clip(lower=0.21)\n",
    "#     return po2 / fio2\n",
    "\n",
    "# def process_crrt_waterfall(\n",
    "#     crrt: pd.DataFrame,\n",
    "#     *,\n",
    "#     id_col: str = \"hospitalization_id\",\n",
    "#     gap_thresh: Union[str, pd.Timedelta] = \"2h\",\n",
    "#     infer_modes: bool = True,          # infer missing mode from numeric pattern\n",
    "#     flag_missing_bfr: bool = True,     # add QC flag if blood-flow still NaN\n",
    "#     wipe_unused: bool = True,          # null parameters not used by the mode\n",
    "#     fix_islands: bool = True,          # relabel single-row SCUF islands\n",
    "#     verbose: bool = True,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Clean + episode-aware forward-fill for the CLIF `crrt_therapy` table.\n",
    "#     Episode-aware clean-up and forward-fill of the CLIF `crrt_therapy` table.\n",
    "\n",
    "#     The function mirrors the respiratory-support “waterfall” logic but adapts it to\n",
    "#     the quirks of Continuous Renal Replacement Therapy (CRRT):\n",
    "\n",
    "#     1. **Episode detection** - a new `crrt_episode_id` starts whenever  \n",
    "#        • `crrt_mode_category` changes **OR**  \n",
    "#        • the gap between successive rows exceeds *gap_thresh* (default 2 h).\n",
    "#     2. **Numeric forward-fill inside an episode** - fills *only* the parameters\n",
    "#        that are clinically relevant for the active mode.\n",
    "#     3. **Mode-specific wiping** after filling, parameters that are **not used**\n",
    "#        in the current mode (e.g. `dialysate_flow_rate` in SCUF) are nulled so\n",
    "#        stale data never bleed across modes.\n",
    "#     4. **Deduplication & ordering** guarantees exactly **one row per\n",
    "#        `(id_col, recorded_dttm)`**, chronologically ordered.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     crrt : pd.DataFrame\n",
    "#         Raw `crrt_therapy` table **in UTC**. Must contain the schema columns\n",
    "#         defined on the CLIF website (see docstring footer).\n",
    "#     id_col : str, default ``\"hospitalization_id\"``\n",
    "#         Encounter-level identifier.\n",
    "#     gap_thresh : str or pd.Timedelta, default ``\"2h\"``\n",
    "#         Maximum tolerated gap **inside** an episode before a new episode is\n",
    "#         forced. Accepts any pandas-parsable offset string (``\"90min\"``, ``\"3h\"``,\n",
    "#         …) or a ``pd.Timedelta``.\n",
    "#     verbose : bool, default ``True``\n",
    "#         If *True* prints progress banners.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         Processed CRRT DataFrame with\n",
    "\n",
    "#         * ``crrt_episode_id`` (int32) - sequential per encounter,\n",
    "#         * forward-filled numeric parameters **within** each episode,\n",
    "#         * unused parameters blanked per mode,\n",
    "#         * unique, ordered rows ``id_col, recorded_dttm``.\n",
    "\n",
    "#     Add-ons v2.0\n",
    "#     ------------\n",
    "#     • Optional numeric-pattern inference of `crrt_mode_category`.\n",
    "#     • Flags rows that *should* have blood-flow but don't.\n",
    "#     • Optional fix for single-row modality islands (sandwiched rows).\n",
    "#     • Optional wipe vs. keep of parameters not used by the active mode.\n",
    "\n",
    "#     Key steps\n",
    "#     ----------\n",
    "#     0.  Lower-case strings, coerce numerics, **infer** mode when blank.\n",
    "#     1.  **Relabel single-row SCUF islands** (if *fix_islands*).\n",
    "#     2.  Detect `crrt_episode_id` (mode change or >gap_thresh).\n",
    "#     3.  Forward-fill numeric parameters *within* an episode.\n",
    "#     4.  QC flag → `blood_flow_missing_after_ffill` (optional).\n",
    "#     5.  Wipe / flag parameters not valid for the mode (configurable).\n",
    "#     6.  Deduplicate & order ⇒ one row per ``(id_col, recorded_dttm)``.\n",
    "#     \"\"\"\n",
    "#     p = print if verbose else (lambda *_, **__: None)\n",
    "#     gap_thresh = pd.Timedelta(gap_thresh)\n",
    "\n",
    "#     # ───────────── Phase 0 — prep, numeric coercion, optional inference\n",
    "#     p(\"✦ Phase 0: prep & numeric coercion (+optional mode inference)\")\n",
    "#     df = crrt.copy()\n",
    "\n",
    "#     df[\"crrt_mode_category\"] = df[\"crrt_mode_category\"].str.lower()\n",
    "#     # save original dialysate_flow_rate values\n",
    "#     df[\"_orig_df\"] = df[\"dialysate_flow_rate\"]\n",
    "\n",
    "#     # 0a) RAW SCUF DF‐OUT sanity check\n",
    "#     # look for rows that are already labeled “scuf”\n",
    "#     # and that have a non‐zero dialysate_flow_rate in the raw data\n",
    "#     raw_scuf = df[\"crrt_mode_category\"].str.lower() == \"scuf\"\n",
    "#     raw_df_positive = df[\"_orig_df\"].fillna(0) > 0\n",
    "\n",
    "#     n_bad = (raw_scuf & raw_df_positive).sum()\n",
    "#     if n_bad:\n",
    "#         print(f\"!!!  Found {n_bad} raw SCUF rows with dialysate_flow_rate > 0 (should be 0 or NA)\")\n",
    "#         print(\" Converting these mode category to NA, keep recorded numerical values as the ground truth\")\n",
    "#         df.loc[raw_df_positive, \"crrt_mode_category\"] = np.nan\n",
    "#     else:\n",
    "#         print(\"!!! No raw SCUF rows had dialysate_flow_rate > 0\")\n",
    "\n",
    "#     NUM_COLS = [\n",
    "#         \"blood_flow_rate\",\n",
    "#         \"pre_filter_replacement_fluid_rate\",\n",
    "#         \"post_filter_replacement_fluid_rate\",\n",
    "#         \"dialysate_flow_rate\",\n",
    "#         \"ultrafiltration_out\",\n",
    "#     ]\n",
    "#     NUM_COLS = [c for c in NUM_COLS if c in df.columns]\n",
    "#     df[NUM_COLS] = df[NUM_COLS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "#     #  any row whose original ultrafiltration_out was >0 must never be SCUF\n",
    "#     def drop_scuf_on_positive_df(df, p):\n",
    "#         bad_df  = df[\"_orig_df\"].fillna(0) > 0\n",
    "#         scuf_now = df[\"crrt_mode_category\"] == \"scuf\"\n",
    "#         n = (bad_df & scuf_now).sum()\n",
    "#         if n:\n",
    "#             p(f\"→ Removing {n:,} SCUF labels on rows with DF>0\")\n",
    "#             df.loc[bad_df & scuf_now, \"crrt_mode_category\"] = np.nan\n",
    "            \n",
    "\n",
    "#     if infer_modes:\n",
    "#         miss = df[\"crrt_mode_category\"].isna()\n",
    "#         pre  = df[\"pre_filter_replacement_fluid_rate\"].notna()\n",
    "#         post = df[\"post_filter_replacement_fluid_rate\"].notna()\n",
    "#         dial = df[\"dialysate_flow_rate\"].notna()\n",
    "#         bf   = df[\"blood_flow_rate\"].notna()\n",
    "#         uf   = df[\"ultrafiltration_out\"].notna()\n",
    "#         all_num_present = df[NUM_COLS].notna().all(axis=1)\n",
    "\n",
    "#         df.loc[miss & all_num_present,                       \"crrt_mode_category\"] = \"cvvhdf\"\n",
    "#         df.loc[miss & (~dial) & pre & post,                  \"crrt_mode_category\"] = \"cvvh\"\n",
    "#         df.loc[miss & dial & (~pre) & (~post),               \"crrt_mode_category\"] = \"cvvhd\"\n",
    "#         df.loc[miss & (~dial) & (~pre) & (~post) & bf & uf,  \"crrt_mode_category\"] = \"scuf\"\n",
    "\n",
    "#         filled = (miss & df[\"crrt_mode_category\"].notna()).sum()\n",
    "#         p(f\"  • numeric-pattern inference filled {filled:,} missing modes\")\n",
    "#         drop_scuf_on_positive_df(df, p)\n",
    "\n",
    "#     # ───────────── Phase 1 — sort and *fix islands before episodes*\n",
    "#     p(\"✦ Phase 1: sort + SCUF-island fix\")\n",
    "#     df = df.sort_values([id_col, \"recorded_dttm\"]).reset_index(drop=True)\n",
    "\n",
    "#     if fix_islands:\n",
    "#         # after sorting, BEFORE episode detection\n",
    "#         prev_mode = df.groupby(id_col)[\"crrt_mode_category\"].shift()\n",
    "#         next_mode = df.groupby(id_col)[\"crrt_mode_category\"].shift(-1)\n",
    "\n",
    "#         scuf_island = (\n",
    "#             (df[\"crrt_mode_category\"] == \"scuf\") &\n",
    "#             (prev_mode.notna()) & (next_mode.notna()) &     # ensure we have neighbours\n",
    "#             (prev_mode == next_mode)                        # both neighbours agree\n",
    "#         )\n",
    "\n",
    "#         df.loc[scuf_island, \"crrt_mode_category\"] = prev_mode[scuf_island]\n",
    "#         n_fixed = scuf_island.sum()\n",
    "#         p(f\"  • relabelled {n_fixed:,} SCUF-island rows\")\n",
    "#         drop_scuf_on_positive_df(df, p)\n",
    "\n",
    "\n",
    "#     # ───────────── Phase 2 — episode detection (now with fixed modes)\n",
    "#     p(\"✦ Phase 2: derive `crrt_episode_id`\")\n",
    "#     mode_change = (\n",
    "#         df.groupby(id_col)[\"crrt_mode_category\"]\n",
    "#           .apply(lambda s: s != s.shift())\n",
    "#           .reset_index(level=0, drop=True)\n",
    "#     )\n",
    "#     time_gap = df.groupby(id_col)[\"recorded_dttm\"].diff().gt(gap_thresh).fillna(False)\n",
    "#     df[\"crrt_episode_id\"] = ((mode_change | time_gap)\n",
    "#                               .groupby(df[id_col]).cumsum()\n",
    "#                               .astype(\"int32\"))\n",
    "\n",
    "#     # ───────────── Phase 3 — forward-fill numerics inside episodes\n",
    "#     p(\"✦ Phase 3: forward-fill numeric vars inside episodes\")\n",
    "#     tqdm.pandas(disable=not verbose, desc=\"ffill per episode\")\n",
    "#     df[NUM_COLS] = (\n",
    "#         df.groupby([id_col, \"crrt_episode_id\"], sort=False, group_keys=False)[NUM_COLS]\n",
    "#           .progress_apply(lambda g: g.ffill())\n",
    "#     )\n",
    "\n",
    "#     # QC: blood-flow still missing?\n",
    "#     if flag_missing_bfr and \"blood_flow_rate\" in NUM_COLS:\n",
    "#         need_bfr = df[\"crrt_mode_category\"].isin([\"scuf\", \"cvvh\", \"cvvhd\", \"cvvhdf\"])\n",
    "#         df[\"blood_flow_missing_after_ffill\"] = need_bfr & df[\"blood_flow_rate\"].isna()\n",
    "#         p(f\"  • blood-flow still missing where required: \"\n",
    "#           f\"{df['blood_flow_missing_after_ffill'].mean():.1%}\")\n",
    "        \n",
    "#     # Bridge tiny episodes\n",
    "    \n",
    "#     single_row_ep = (\n",
    "#         df.groupby([id_col, \"crrt_episode_id\"]).size() == 1\n",
    "#     ).reset_index(name=\"n\").query(\"n == 1\")\n",
    "#     print(\"Bridging single row episodes\")\n",
    "\n",
    "#     rows_to_bridge = df.merge(single_row_ep[[id_col, \"crrt_episode_id\"]],\n",
    "#                             on=[id_col, \"crrt_episode_id\"]).index\n",
    "    \n",
    "#     CAT_COLS = [c for c in [\"crrt_mode_category\"] if c in df.columns]\n",
    "\n",
    "#     # Combine with the numeric columns we already had\n",
    "#     BRIDGE_COLS = NUM_COLS + CAT_COLS\n",
    "\n",
    "#     # Forward-fill (and back-fill just in case the island is the first row of the encounter)\n",
    "#     df.loc[rows_to_bridge, BRIDGE_COLS] = (\n",
    "#         df.loc[rows_to_bridge, BRIDGE_COLS]\n",
    "#         .groupby(df.loc[rows_to_bridge, id_col])      # keep encounter boundaries\n",
    "#         .apply(lambda g: g.ffill())          \n",
    "#         .reset_index(level=0, drop=True)\n",
    "#     )\n",
    "#     drop_scuf_on_positive_df(df, p)\n",
    "#     # ───────────── Phase 4 — wipe / flag unused parameters\n",
    "#     p(\"✦ Phase 4: handle parameters not valid for the mode\")\n",
    "#     MODE_PARAM_MAP = {\n",
    "#         \"scuf\":   {\"blood_flow_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvh\":   {\"blood_flow_rate\", \"pre_filter_replacement_fluid_rate\",\n",
    "#                    \"post_filter_replacement_fluid_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvhd\":  {\"blood_flow_rate\", \"dialysate_flow_rate\", \"ultrafiltration_out\"},\n",
    "#         \"cvvhdf\": {\"blood_flow_rate\", \"pre_filter_replacement_fluid_rate\",\"post_filter_replacement_fluid_rate\",\n",
    "#                    \"dialysate_flow_rate\", \"ultrafiltration_out\"},\n",
    "#     }\n",
    "\n",
    "#     wiped_totals = {c: 0 for c in NUM_COLS}\n",
    "#     for mode, keep in MODE_PARAM_MAP.items():\n",
    "#         mask = df[\"crrt_mode_category\"] == mode\n",
    "#         drop_cols = list(set(NUM_COLS) - keep)\n",
    "#         if wipe_unused:\n",
    "#             for col in drop_cols:\n",
    "#                 wiped_totals[col] += df.loc[mask, col].notna().sum()\n",
    "#             df.loc[mask, drop_cols] = np.nan\n",
    "#         else:\n",
    "#             for col in drop_cols:\n",
    "#                 df.loc[mask & df[col].notna(), f\"{col}_unexpected\"] = True\n",
    "\n",
    "#     if verbose and wipe_unused:\n",
    "#         p(\"  • cells set → NA by wipe:\")\n",
    "#         for col, n in wiped_totals.items():\n",
    "#             p(f\"    {col:<35} {n:>8,}\")\n",
    "#     # ───────────── Phase 4a — SCUF‐specific sanity check\n",
    "#     if \"dialysate_flow_rate\" in df.columns:\n",
    "#         # only consider rows that were originally SCUF mode\n",
    "#         # and whose original _orig_df was non‐zero/non‐NA\n",
    "#         scuf_rows = df[\"crrt_mode_category\"] == \"scuf\"\n",
    "#         orig_bad = df[\"_orig_df\"].fillna(0) > 0\n",
    "\n",
    "#         # these are rows where the *original* data had UF>0 despite SCUF\n",
    "#         bad_orig_scuf = scuf_rows & orig_bad\n",
    "\n",
    "#         n_bad_orig = bad_orig_scuf.sum()\n",
    "#         if n_bad_orig:\n",
    "#             p(f\"!!! {n_bad_orig} rows originally labeled SCUF had DF>0 (raw data); forcing DF→NA for those\")\n",
    "#             df.loc[bad_orig_scuf, \"dialysate_flow_rate\"] = np.nan\n",
    "#         else:\n",
    "#             p(\"!!! No SCUF rows with DF>0\")\n",
    "\n",
    "#     # then drop the helper column\n",
    "#     df = df.drop(columns=\"_orig_df\")\n",
    "\n",
    "#     # ───────────── Phase 5 — deduplicate & order\n",
    "#     p(\"✦ Phase 5: deduplicate & order\")\n",
    "#     pre = len(df)\n",
    "#     df = (\n",
    "#         df.drop_duplicates(subset=[id_col, \"recorded_dttm\"])\n",
    "#           .sort_values([id_col, \"recorded_dttm\"])\n",
    "#           .reset_index(drop=True)\n",
    "#     )\n",
    "#     p(f\"  • dropped {pre - len(df):,} duplicate rows\")\n",
    "\n",
    "#     if verbose:\n",
    "#         sparse = df[NUM_COLS].isna().all(axis=1).mean()\n",
    "#         p(f\"  • rows with all NUM_COLS missing: {sparse:.1%}\")\n",
    "\n",
    "#     p(\"[OK] CRRT waterfall complete.\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffe16d",
   "metadata": {},
   "source": [
    "## Cohort identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c726691f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading CLIF Tables\n",
      "================================================================================\n",
      "ClifOrchestrator initialized.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Loading CLIF Tables\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from clifpy.clif_orchestrator import ClifOrchestrator\n",
    "\n",
    "# Initialize ClifOrchestrator\n",
    "clif = ClifOrchestrator(\n",
    "    data_directory=config['tables_path'],\n",
    "    filetype=config['file_type'],\n",
    "    timezone=config['timezone']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553499b",
   "metadata": {},
   "source": [
    "## Step0: Load Core Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2dc9ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Step 0: Load Core Tables (Patient, Hospitalization, ADT)\n",
      "================================================================================\n",
      "\n",
      "Loading 3 core tables...\n",
      "   Loading patient... ✓ (97,254 rows)\n",
      "   Loading hospitalization... ✓ (166,814 rows)\n",
      "   Loading adt... ✓ (425,353 rows)\n",
      "\n",
      "Core tables loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 0: Load Core Tables (Patient, Hospitalization, ADT)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 0: Load Core Tables (Patient, Hospitalization, ADT)\")\n",
    "print(\"=\" * 80)\n",
    "core_tables = ['patient', 'hospitalization', 'adt']\n",
    "\n",
    "print(f\"\\nLoading {len(core_tables)} core tables...\")\n",
    "for table_name in core_tables:\n",
    "    print(f\"   Loading {table_name}...\", end=\" \")\n",
    "    try:\n",
    "        clif.load_table(table_name)\n",
    "        table = getattr(clif, table_name)\n",
    "        print(f\"✓ ({len(table.df):,} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nCore tables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36bd52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_df = clif.hospitalization.df\n",
    "adt_df = clif.adt.df\n",
    "\n",
    "# Merge to get age information\n",
    "all_encounters = pd.merge(\n",
    "    hosp_df[[\"patient_id\", \"hospitalization_id\", \"admission_dttm\", \"discharge_dttm\", \n",
    "             \"age_at_admission\", \"discharge_category\"]],\n",
    "    adt_df[[\"hospitalization_id\", \"hospital_id\", \"in_dttm\", \"out_dttm\", \n",
    "            \"location_category\", \"location_type\"]],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683b68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates by ['hospitalization_id', 'in_dttm', 'out_dttm']\n",
    "dup_counts = all_encounters.duplicated(subset=['hospitalization_id', 'in_dttm', 'out_dttm']).sum()\n",
    "if dup_counts > 0:\n",
    "    print(f\"Warning: {dup_counts} duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")\n",
    "else:\n",
    "    print(\"No duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5f8fc",
   "metadata": {},
   "source": [
    "## Step1: Date & Age filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d378297e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
       "       'age_at_admission', 'discharge_category', 'hospital_id', 'in_dttm',\n",
       "       'out_dttm', 'location_category', 'location_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_encounters.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61bc74b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Step 1: Identifying Adult Patients (Age >= 18) and Admissions 2018-2024\n",
      "================================================================================\n",
      "Applying initial cohort filters...\n",
      "\n",
      "Filtering Results:\n",
      "   Total hospitalizations: 166,781\n",
      "   Adult hospitalizations (age >= 18, 2018-2024): 166,781\n",
      "   Excluded (age < 18 or outside 2018-2024): 0\n",
      "\n",
      "   Unique adult hospitalization IDs: 166,781\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Identify Adult Patients (Age >= 18) and Admissions 2018-2024\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 1: Identifying Adult Patients (Age >= 18) and Admissions 2018-2024\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Applying initial cohort filters...\")\n",
    "\n",
    "# Use only the relevant columns from all_encounters\n",
    "adult_encounters = all_encounters[\n",
    "    [\n",
    "        'patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
    "        'age_at_admission', 'discharge_category', 'hospital_id',\n",
    "        'in_dttm', 'out_dttm', 'location_category', 'location_type'\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "if config['timezone'].lower() == \"mimic\":\n",
    "    # MIMIC: only age >= 18, no admit year restriction\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['age_at_admission'] >= 18) & (adult_encounters['age_at_admission'].notna())\n",
    "    ]\n",
    "else:\n",
    "    # Other sites: age >= 18 and admission between 2018-2024 inclusive\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['age_at_admission'] >= 18) &\n",
    "        (adult_encounters['age_at_admission'].notna()) &\n",
    "        (adult_encounters['admission_dttm'].dt.year >= 2018) &\n",
    "        (adult_encounters['admission_dttm'].dt.year <= 2024)\n",
    "    ]\n",
    "\n",
    "\n",
    "print(f\"\\nFiltering Results:\")\n",
    "print(f\"   Total hospitalizations: {len(all_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Adult hospitalizations (age >= 18, 2018-2024): {len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Excluded (age < 18 or outside 2018-2024): {len(all_encounters['hospitalization_id'].unique()) - len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "\n",
    "\n",
    "strobe_counts[\"0_total_hospitalizations\"] = len(all_encounters['hospitalization_id'].unique())\n",
    "strobe_counts[\"1_adult_hospitalizations\"] = len(adult_encounters['hospitalization_id'].unique())\n",
    "# Get list of adult hospitalization IDs for filtering\n",
    "adult_hosp_ids = set(adult_encounters['hospitalization_id'].unique())\n",
    "print(f\"\\n   Unique adult hospitalization IDs: {len(adult_hosp_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66edf5",
   "metadata": {},
   "source": [
    "### Stitch hospitalizations \n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_id`, then we combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block.\n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_joined_id` from the hospitalization table, then we consider the user has already stitched similar encounters, and we will consider that as the primary id column for all table joins moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4651cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from clifpy.utils.stitching_encounters import stitch_encounters\n",
    "\n",
    "# Instead of multiple copies, work with references and clean up\n",
    "hosp_filtered = clif.hospitalization.df[clif.hospitalization.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "adt_filtered = clif.adt.df[clif.adt.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "\n",
    "hosp_stitched, adt_stitched, encounter_mapping = stitch_encounters(\n",
    "    hospitalization=hosp_filtered,\n",
    "    adt=adt_filtered,\n",
    "    time_interval=6  \n",
    ")\n",
    "\n",
    "# Direct assignment without additional copies\n",
    "clif.hospitalization.df = hosp_stitched\n",
    "clif.adt.df = adt_stitched\n",
    "\n",
    "# Store the encounter mapping in the orchestrator for later use\n",
    "clif.encounter_mapping = encounter_mapping\n",
    "\n",
    "# Clean up intermediate variables\n",
    "del hosp_filtered, adt_filtered\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb77b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encounter Stitching Results:\n",
      "   Number of unique hospitalizations before stitching: 166,781\n",
      "   Number of unique encounter blocks after stitching: 166,644\n",
      "   Number of linked hospitalization ids: 137\n",
      "\n",
      "Encounter Mapping Details:\n",
      "   Total encounter mappings created: 166,781\n",
      "   Encounter blocks with multiple hospitalizations: 136\n",
      "   Maximum hospitalizations combined into one block: 3\n"
     ]
    }
   ],
   "source": [
    "# After your stitching code, add these calculations:\n",
    "\n",
    "# Calculate stitching statistics\n",
    "strobe_counts['1b_before_stitching'] = len(adult_hosp_ids)  # Original adult hospitalizations\n",
    "strobe_counts['1b_after_stitching'] = len(hosp_stitched['encounter_block'].unique())  # Unique encounter blocks after stitching\n",
    "strobe_counts['1b_stitched_hosp_ids'] = strobe_counts['1b_before_stitching'] - strobe_counts['1b_after_stitching']  # Number of hospitalizations that were linked\n",
    "\n",
    "print(f\"\\nEncounter Stitching Results:\")\n",
    "print(f\"   Number of unique hospitalizations before stitching: {strobe_counts['1b_before_stitching']:,}\")\n",
    "print(f\"   Number of unique encounter blocks after stitching: {strobe_counts['1b_after_stitching']:,}\")\n",
    "print(f\"   Number of linked hospitalization ids: {strobe_counts['1b_stitched_hosp_ids']:,}\")\n",
    "\n",
    "# Optional: Show the encounter mapping details\n",
    "print(f\"\\nEncounter Mapping Details:\")\n",
    "print(f\"   Total encounter mappings created: {len(encounter_mapping):,}\")\n",
    "if len(encounter_mapping) > 0:\n",
    "    # Show some examples of how many original hospitalizations were combined\n",
    "    mapping_counts = encounter_mapping.groupby('encounter_block').size()\n",
    "    print(f\"   Encounter blocks with multiple hospitalizations: {(mapping_counts > 1).sum():,}\")\n",
    "    print(f\"   Maximum hospitalizations combined into one block: {mapping_counts.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c040e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df = encounter_mapping.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806ca75",
   "metadata": {},
   "source": [
    "## Step2: Location based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all_encounters with encounter_mapping to get encounter_block information\n",
    "all_encounters = pd.merge(all_encounters, encounter_mapping, on='hospitalization_id', how='left')\n",
    "\n",
    "# Precompute lowercase columns for efficient string ops\n",
    "# Avoid repeated .str.lower() calls \n",
    "all_encounters['location_category_l'] = all_encounters['location_category'].str.lower()\n",
    "all_encounters['discharge_category_l'] = all_encounters['discharge_category'].str.lower()\n",
    "\n",
    "# Boolean masks for ICU and death/hospice per row\n",
    "icu_mask_row = all_encounters['location_category_l'].str.contains('icu', na=False)\n",
    "death_mask_row = all_encounters['discharge_category_l'].isin(['expired', 'hospice'])\n",
    "\n",
    "# cohort flag: groupby('encounter_block') and reduce with .transform('any')\n",
    "all_encounters['icu_enc'] = all_encounters.groupby('encounter_block')['location_category_l'].transform(lambda x: x.str.contains('icu', na=False).any()).astype(int)\n",
    "all_encounters['death_enc'] = all_encounters.groupby('encounter_block')['discharge_category_l'].transform(lambda x: x.isin(['expired', 'hospice']).any()).astype(int)\n",
    "\n",
    "# Use numpy for efficient cohort flag\n",
    "import numpy as np\n",
    "all_encounters['cohort_enc'] = np.logical_or(all_encounters['icu_enc'], all_encounters['death_enc']).astype(int)\n",
    "\n",
    "# (Optional: If you need just the filtered encounter blocks)\n",
    "# encounter_blocks_with_cohort = all_encounters.loc[all_encounters['cohort_enc'] == 1, 'encounter_block'].unique()\n",
    "\n",
    "# Clean up \n",
    "all_encounters.drop(['location_category_l', 'discharge_category_l'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "982478d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deaths outside ICU: 2600 (1.6% of all hospitalizations)\n"
     ]
    }
   ],
   "source": [
    "# Identify encounters where death occurred\n",
    "death_encounters = all_encounters[all_encounters['death_enc'] == 1]\n",
    "# Identify those that never touched the ICU\n",
    "non_icu_deaths = death_encounters[~death_encounters['icu_enc'].astype(bool)]\n",
    "# Count the number of unique encounters with deaths outside of ICU\n",
    "num_deaths_outside_icu = non_icu_deaths['encounter_block'].nunique()\n",
    "# Calculate total deaths (unique encounter blocks with death)\n",
    "total_encounters = all_encounters['encounter_block'].nunique()\n",
    "# Calculate the percentage\n",
    "pct_deaths_outside_icu = (num_deaths_outside_icu / total_encounters * 100) if total_encounters > 0 else 0\n",
    "print(f\"Number of deaths outside ICU: {num_deaths_outside_icu} ({pct_deaths_outside_icu:.1f}% of all hospitalizations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1672f",
   "metadata": {},
   "source": [
    "## Respiratory Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db32a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Step 2: Loading Respiratory Support and Identifying IMV Patients\n",
      "================================================================================\n",
      "\n",
      "Loading respiratory_support table...\n",
      "Respiratory support loaded (6,973,889 rows)\n",
      "\n",
      "Standardizing category columns...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Load Respiratory Support and Identify IMV Patients\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 2: Loading Respiratory Support and Identifying IMV Patients\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nLoading respiratory_support table...\")\n",
    "clif.load_table('respiratory_support',\n",
    "                        columns=rst_required_columns,\n",
    "                        filters={'hospitalization_id': list(adult_hosp_ids)})\n",
    "print(f\"Respiratory support loaded ({len(clif.respiratory_support.df):,} rows)\")\n",
    "\n",
    "# Standardize category columns to lowercase\n",
    "print(f\"\\nStandardizing category columns...\")\n",
    "category_cols = [col for col in clif.respiratory_support.df.columns if col.endswith('_category')]\n",
    "for col in category_cols:\n",
    "    clif.respiratory_support.df[col] = clif.respiratory_support.df[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06229735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hospitalizations with any IMV (device_category == 'imv')\n",
    "print(f\"\\nIdentifying IMV hospitalizations...\")\n",
    "imv_hosp_ids = clif.respiratory_support.df.loc[\n",
    "    clif.respiratory_support.df['device_category'] == 'imv',\n",
    "    'hospitalization_id'\n",
    "].unique()\n",
    "print(f\"Hospitalizations with IMV: {len(imv_hosp_ids):,}\")\n",
    "strobe_counts[\"2_imv_hospitalizations\"] = len(imv_hosp_ids)\n",
    "\n",
    "# Filter to only IMV hospitalizations\n",
    "clif.respiratory_support.df = clif.respiratory_support.df[\n",
    "    clif.respiratory_support.df['hospitalization_id'].isin(imv_hosp_ids)\n",
    "].copy()\n",
    "print(f\"Respiratory support rows (IMV hospitalizations): {len(clif.respiratory_support.df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170325ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".clif_table_one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
