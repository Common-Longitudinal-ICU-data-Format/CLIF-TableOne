{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## CLIF Table One\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "Date v1: May 13, 2025\n",
    "\n",
    "This script identifies the cohort of encounters with at least one ICU stay and then summarizes the cohort data into one table. \n",
    "\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`, `clif_patient_assessments`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`,`discharge_dttm`, `age_at_admission` | - |\n",
    "| **adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category` | - |\n",
    "| **vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | weight_kg |\n",
    "| **labs** | `hospitalization_id`, `lab_result_dttm`, `lab_order_dttm`, `lab_category`, `lab_value_numeric` | creatinine, bilirubin_total, po2_arterial, platelet_count |\n",
    "| **medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin(optional) |\n",
    "| **respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`,  `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs`, `tidal_volume_set`, `pressure_control_set`, `pressure_support_set` | - |\n",
    "| **patient_assessments** | `hospitalization_id`, `recorded_dttm` , `assessment_category`, `numerical_value`| `gcs_total` |\n",
    "| **crrt_therapy** | `hospitalization_id`, `recorded_dttm` | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pyCLIF\n",
    "import pyarrow\n",
    "import sofa_score\n",
    "from datetime import timedelta\n",
    "\n",
    "## import outlier json\n",
    "with open('../config/outlier_config.json', 'r') as f:\n",
    "    outlier_cfg = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pyCLIF.load_data('clif_patient')\n",
    "hospitalization = pyCLIF.load_data('clif_hospitalization')\n",
    "adt = pyCLIF.load_data('clif_adt')\n",
    "\n",
    "# ensure id variable is of dtype character\n",
    "hospitalization['hospitalization_id']= hospitalization['hospitalization_id'].astype(str)\n",
    "patient['patient_id']= patient['patient_id'].astype(str)\n",
    "adt['hospitalization_id']= adt['hospitalization_id'].astype(str)\n",
    "\n",
    "# check for duplicates\n",
    "# patient table should be unique by patient id\n",
    "patient = pyCLIF.remove_duplicates(patient, ['patient_id'], 'patient')\n",
    "# hospitalization table should be unique by hospitalization id\n",
    "hospitalization = pyCLIF.remove_duplicates(hospitalization, ['hospitalization_id'], 'hospitalization')\n",
    "# adt table should be unique by hospitalization id and in dttm\n",
    "adt = pyCLIF.remove_duplicates(adt, ['hospitalization_id', 'hospital_id', 'in_dttm'], 'adt')\n",
    "\n",
    "hospitalization = hospitalization.sort_values(['hospitalization_id', \"admission_dttm\"])\n",
    "adt = adt.sort_values(['hospitalization_id', \"in_dttm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all _dttm variables to the same format\n",
    "patient = pyCLIF.convert_datetime_columns_to_site_tz(patient,  pyCLIF.helper['timezone'])\n",
    "hospitalization = pyCLIF.convert_datetime_columns_to_site_tz(hospitalization, pyCLIF.helper['timezone'])\n",
    "adt = pyCLIF.convert_datetime_columns_to_site_tz(adt,  pyCLIF.helper['timezone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort\n",
    "\n",
    "Filter down to adult encounters with at least one recorded ICU stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_hosp_ids = hospitalization[(hospitalization['admission_dttm'].dt.year <= 2024) & \n",
    "                                (hospitalization['age_at_admission'] >= 18)]['hospitalization_id'].unique()\n",
    "icu_hosp_ids = adt[adt['location_category'].str.lower() == 'icu']['hospitalization_id'].unique()\n",
    "# Get intersection of adult and ICU hospitalizations\n",
    "cohort_ids = np.intersect1d(adult_hosp_ids, icu_hosp_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linked Hospitalizations\n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_id`, then we combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block.\n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_joined_id` from the hospitalization table, then we consider the user has already stitched similar encounters, and we will consider that as the primary id column for all table joins moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitalization_cohort = hospitalization[hospitalization['hospitalization_id'].isin(cohort_ids)].copy()\n",
    "adt_cohort = adt[adt['hospitalization_id'].isin(cohort_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyCLIF.helper['id_column'] == 'hospitalization_id':\n",
    "    stitched_cohort = pyCLIF.stitch_encounters(hospitalization_cohort, adt_cohort, time_interval=6)\n",
    "    id_col = 'encounter_block'\n",
    "    stitched_unique = stitched_cohort[['patient_id', id_col]].drop_duplicates()\n",
    "    all_ids = stitched_cohort[['patient_id', 'hospitalization_id', id_col, 'discharge_category', 'admission_dttm', 'discharge_dttm', 'age_at_admission']].drop_duplicates()\n",
    "else:\n",
    "    id_col = pyCLIF.helper['id_column']\n",
    "    all_ids = hospitalization[['patient_id', 'hospitalization_id', id_col, 'discharge_category','admission_dttm', 'discharge_dttm', 'age_at_admission']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get start and End Years\n",
    "start_year = all_ids['admission_dttm'].dt.year.min()\n",
    "end_year = all_ids['admission_dttm'].dt.year.max()\n",
    "all_ids['is_dead'] = (all_ids['discharge_category'].str.lower().isin(['expired', 'hospice'])).astype(int)\n",
    "print(\"Start Year:\", start_year, \"\\n End Year:\", end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics\n",
    "\n",
    "Summarize the demographic info at patient level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get patient demographics for patients in our cohort\n",
    "patient_cohort = patient[patient['patient_id'].isin(all_ids['patient_id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_cohort =pyCLIF.map_race_column(patient_cohort, 'race_category')\n",
    "patient_cohort['race_new'] = patient_cohort['race_new'].fillna('Missing')\n",
    "patient_cohort['ethnicity_category'] = patient_cohort['ethnicity_category'].fillna('Missing')\n",
    "patient_cohort['sex_category'] = patient_cohort['sex_category'].fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_cohort.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hospital and ICU Admission Summary\n",
    "\n",
    "1. Get the first ICU dttm. \n",
    "2. Calculate ICU LOS and Hospital LOS for each encounter in days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_admission_summary = (\n",
    "        adt_cohort\n",
    "        .merge(all_ids[['hospitalization_id', id_col]], on='hospitalization_id')\n",
    "        .groupby(id_col)\n",
    "        .agg(\n",
    "            min_in_dttm = ('in_dttm', 'min'),\n",
    "            max_out_dttm = ('out_dttm', 'max'),\n",
    "            first_admission_location = ('location_category', 'first')\n",
    "        )\n",
    ")\n",
    "hosp_admission_summary['hospital_length_of_stay_days'] = (\n",
    "    (hosp_admission_summary['max_out_dttm'] - hosp_admission_summary['min_in_dttm']) / pd.Timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the column, not the entire df\n",
    "adt_cohort['location_category'] = (\n",
    "    adt_cohort['location_category']\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# Merge on the ID mapping\n",
    "df = (\n",
    "    adt_cohort\n",
    "    .merge(all_ids[['hospitalization_id', id_col]], on='hospitalization_id')\n",
    ")\n",
    "\n",
    "# restrict to ICU rows\n",
    "icu_df = df.query('location_category == \"icu\"')\n",
    "\n",
    "# find first ICU in time per id_col\n",
    "first_in = (\n",
    "    icu_df\n",
    "     .groupby(id_col, as_index=False)\n",
    "     .agg(first_icu_in_dttm=('in_dttm', 'min'))\n",
    ")\n",
    "\n",
    "# join back to pull the matching out_dttm\n",
    "icu_summary = (\n",
    "    first_in\n",
    "      # bring in that one row’s out_dttm\n",
    "      .merge(\n",
    "          icu_df[['hospitalization_id','in_dttm','out_dttm', id_col]],\n",
    "          left_on=[id_col, 'first_icu_in_dttm'],\n",
    "          right_on=[id_col, 'in_dttm'],\n",
    "          how='left'\n",
    "      )\n",
    "      .rename(columns={'out_dttm':'first_icu_out_dttm'})\n",
    ")\n",
    "\n",
    "# compute LOS in days (out - in)\n",
    "icu_summary['first_icu_los_days'] = (\n",
    "    (icu_summary['first_icu_out_dttm'] - icu_summary['first_icu_in_dttm'])\n",
    "    .dt.total_seconds()\n",
    "    / (3600 * 24)\n",
    ")\n",
    "\n",
    "# trim to just the columns you need\n",
    "icu_summary = icu_summary[[id_col, 'first_icu_in_dttm',\n",
    "                           'first_icu_out_dttm','first_icu_los_days']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all_ids with icu_summary and hosp_admission_summary\n",
    "final_df = (\n",
    "    all_ids[[id_col,  'discharge_category', 'admission_dttm', 'discharge_dttm', 'age_at_admission', 'is_dead']]\n",
    "    .merge(icu_summary, on=id_col, how='left')\n",
    "    .merge(hosp_admission_summary, on=id_col, how='left')\n",
    ")\n",
    "final_df['first_admission_location'] = final_df['first_admission_location'].fillna('Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMV Encounters\n",
    "\n",
    "For the most reliable results, we should run the waterfall on the respiratory support table before summarising. \n",
    "To prioritize efficiency, we have not done so for this version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'tidal_volume_set', \n",
    "    'pressure_control_set',\n",
    "    'pressure_support_set',\n",
    "]\n",
    "\n",
    "resp_support_raw = pyCLIF.load_data(\n",
    "    'clif_respiratory_support',\n",
    "    columns=rst_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist()}\n",
    ")\n",
    "\n",
    "resp_support = resp_support_raw.copy()\n",
    "resp_support = pyCLIF.convert_datetime_columns_to_site_tz(resp_support, pyCLIF.helper['timezone'])\n",
    "resp_support['device_category'] = resp_support['device_category'].str.lower()\n",
    "resp_support['mode_category'] = resp_support['mode_category'].str.lower()\n",
    "resp_support['lpm_set'] = pd.to_numeric(resp_support['lpm_set'], errors='coerce')\n",
    "resp_support['resp_rate_set'] = pd.to_numeric(resp_support['resp_rate_set'], errors='coerce')\n",
    "resp_support['peep_set'] = pd.to_numeric(resp_support['peep_set'], errors='coerce')\n",
    "resp_support['tidal_volume_set'] = pd.to_numeric(resp_support['tidal_volume_set'], errors='coerce')\n",
    "resp_support['pressure_control_set'] = pd.to_numeric(resp_support['pressure_control_set'], errors='coerce')\n",
    "resp_support['pressure_support_set'] = pd.to_numeric(resp_support['pressure_support_set'], errors='coerce')\n",
    "\n",
    "resp_support = resp_support.sort_values(['hospitalization_id', 'recorded_dttm'])\n",
    "# del resp_support_raw\n",
    "\n",
    "print(\"\\n=== Apply outlier thresholds ===\\n\")\n",
    "resp_support['fio2_set'] = pd.to_numeric(resp_support['fio2_set'], errors='coerce')\n",
    "# (Optional) If FiO2 is >1 on average => scale by /100\n",
    "fio2_mean = resp_support['fio2_set'].mean(skipna=True)\n",
    "# If the mean is greater than 1, divide 'fio2_set' by 100\n",
    "if fio2_mean and fio2_mean > 1.0:\n",
    "    # Only divide values greater than 1 to avoid re-dividing already correct values\n",
    "    resp_support.loc[resp_support['fio2_set'] > 1, 'fio2_set'] = \\\n",
    "        resp_support.loc[resp_support['fio2_set'] > 1, 'fio2_set'] / 100\n",
    "    print(\"Updated fio2_set to be between 0.21 and 1\")\n",
    "else:\n",
    "    print(\"FIO2_SET mean=\", fio2_mean, \"is within the required range\")\n",
    "\n",
    "resp_stitched = resp_support.merge(\n",
    "    all_ids[['hospitalization_id', id_col]],\n",
    "    on='hospitalization_id', how='right'\n",
    ")\n",
    "\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'fio2_set', *outlier_cfg['fio2_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'peep_set', *outlier_cfg['peep_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'lpm_set',  *outlier_cfg['lpm_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'resp_rate_set', *outlier_cfg['resp_rate_set'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vent start and end times \n",
    "\n",
    "Calculate vent start times for the first episode of invasive mechanical intubation.   \n",
    "Limitation: the vent end time might not be associated with the same intubation episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Identify IMV\n",
    "imv_mask = resp_stitched['device_category'].str.contains(\"imv\", case=False, na=False)\n",
    "resp_stitched_imv = resp_stitched[imv_mask].copy()\n",
    "# Create on_vent column for IMV records\n",
    "resp_stitched_imv['on_vent'] = 1\n",
    "# Get unique encounter IDs from resp_stitched_imv\n",
    "imv_encounters = resp_stitched_imv[id_col].unique()\n",
    "# Determine Vent Start/End for Each Hospitalization \n",
    "vent_start_end = resp_stitched_imv.groupby(id_col).agg(\n",
    "    vent_start_time=('recorded_dttm','min'),\n",
    "    vent_end_time=('recorded_dttm','max')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add on_vent flag to final_df\n",
    "final_df = final_df.merge(\n",
    "    resp_stitched_imv[[id_col, 'on_vent']].drop_duplicates(),\n",
    "    on=id_col,\n",
    "    how='left'\n",
    ")\n",
    "final_df['on_vent'] = final_df['on_vent'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Mode and Ventilator settings  \n",
    "\n",
    "* Initial mode: First mode category post intubation\n",
    "* Vent settings: Median and IQR for all non NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first non-NA mode category for each id_col\n",
    "initial_modes = resp_stitched_imv.groupby(id_col)['mode_category'].first().reset_index()\n",
    "initial_modes = initial_modes.rename(columns={'mode_category': 'initial_mode_category'})\n",
    "initial_modes['initial_mode_category'] = initial_modes['initial_mode_category'].fillna('Missing')\n",
    "\n",
    "# Merge back to final_df\n",
    "final_df = final_df.merge(initial_modes, on=id_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter resp_stitched to only those encounters and merge\n",
    "resp_stitched_final = resp_stitched[resp_stitched[id_col].isin(imv_encounters)]\n",
    "\n",
    "# 4) aggregate in one shot\n",
    "numeric_cols = [\n",
    "    'fio2_set','lpm_set','resp_rate_set','peep_set',\n",
    "    'tidal_volume_set','pressure_control_set',\n",
    "    'pressure_support_set'\n",
    "]\n",
    "\n",
    "# build named aggregation dict\n",
    "named_aggs = {\n",
    "    'mode_category': ('mode_category', 'first')\n",
    "}\n",
    "for col in numeric_cols:\n",
    "    named_aggs[f'{col}_median'] = (col, 'median')\n",
    "    named_aggs[f'{col}_q1']     = (col, lambda x: x.quantile(0.25))\n",
    "    named_aggs[f'{col}_q3']     = (col, lambda x: x.quantile(0.75))\n",
    "\n",
    "vent_stats = (\n",
    "    resp_stitched_final\n",
    "    .groupby(id_col, as_index=False)\n",
    "    .agg(**named_aggs)\n",
    ")\n",
    "\n",
    "final_df = final_df.merge(vent_stats, on=id_col, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First location of intubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get minimal ADT cohort with required columns and merge with all_ids to get id_col\n",
    "adt_cohort_subset = pd.merge(\n",
    "    adt_cohort[['hospitalization_id', 'in_dttm', 'location_category']],\n",
    "    all_ids[['hospitalization_id', id_col]],\n",
    "    on='hospitalization_id'\n",
    ")\n",
    "\n",
    "\n",
    "adt_vent = pd.merge(\n",
    "    vent_start_end[[id_col, 'vent_start_time']],\n",
    "    adt_cohort_subset,\n",
    "    on=id_col\n",
    ")\n",
    "\n",
    "adt_vent['time_diff'] = abs(adt_vent['vent_start_time'] - adt_vent['in_dttm'])\n",
    "\n",
    "# Get the closest ADT row for each encounter block\n",
    "closest_adt = (adt_vent\n",
    "    .sort_values('time_diff')\n",
    "    .groupby(id_col)\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "closest_adt = closest_adt.rename(columns={'location_category': 'first_location_imv'})\n",
    "\n",
    "final_df = final_df.merge(\n",
    "    closest_adt[[id_col, 'first_location_imv']],\n",
    "    on=id_col,\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vitals- Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['weight_kg']\n",
    "\n",
    "vitals_cohort = pyCLIF.load_data('clif_vitals',\n",
    "    columns=vitals_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(), \n",
    "             'vital_category': vitals_of_interest}\n",
    ")\n",
    "vitals_cohort = vitals_cohort.merge(all_ids[['hospitalization_id', id_col]], on='hospitalization_id', how='left')\n",
    "vitals_cohort = pyCLIF.convert_datetime_columns_to_site_tz(vitals_cohort,  pyCLIF.helper['timezone'])\n",
    "vitals_cohort['vital_value'] = pd.to_numeric(vitals_cohort['vital_value'], errors='coerce')\n",
    "vitals_cohort = vitals_cohort.sort_values([id_col, 'recorded_dttm'])\n",
    "is_weight = vitals_cohort['vital_category'] == 'weight_kg'\n",
    "min_weight, max_weight = outlier_cfg['weight_kg']\n",
    "vitals_cohort.loc[is_weight & (vitals_cohort['vital_value'] < min_weight), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_weight & (vitals_cohort['vital_value'] > max_weight), 'vital_value'] = np.nan\n",
    "vitals_cohort = vitals_cohort.dropna(subset=['vital_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average weight for each encounter block\n",
    "weight_summary = (vitals_cohort\n",
    "    .groupby(id_col)['vital_value']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'vital_value': 'weight_kg'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vasopressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin'\n",
    "]\n",
    "\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "\n",
    "meds_filters = {\n",
    "    'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(),\n",
    "    'med_category': meds_of_interest\n",
    "}\n",
    "meds = pyCLIF.load_data('clif_medication_admin_continuous', columns=meds_required_columns, filters=meds_filters)\n",
    "# ensure correct format\n",
    "meds['hospitalization_id']= meds['hospitalization_id'].astype(str)\n",
    "meds['med_dose_unit'] = meds['med_dose_unit'].str.lower()\n",
    "meds = pyCLIF.convert_datetime_columns_to_site_tz(meds,  pyCLIF.helper['timezone'])\n",
    "meds['med_dose'] = pd.to_numeric(meds['med_dose'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_stitched = meds.merge(all_ids[['hospitalization_id', id_col]], on='hospitalization_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_filtered = meds_stitched[~meds_stitched['med_dose'].isnull()].copy()\n",
    "# Standardize time units in med_dose_unit\n",
    "meds_filtered['med_dose_unit'] = (meds_filtered['med_dose_unit']\n",
    "    .str.replace('hour', 'hr')\n",
    "    .str.replace('minutes', 'min')\n",
    "    .str.replace('minute', 'min')\n",
    "    .str.replace('hours', 'hr')\n",
    ")\n",
    "meds_filtered = meds_filtered[meds_filtered['med_dose_unit'].apply(pyCLIF.has_per_hour_or_min)].copy()\n",
    "ne_df = meds_filtered.merge(weight_summary[[id_col, 'weight_kg']], on=id_col, how='left')\n",
    "ne_df[\"med_dose_converted\"] = ne_df.apply(pyCLIF.convert_dose, axis=1)\n",
    "ne_df = ne_df[ne_df.apply(pyCLIF.is_dose_within_range, axis=1, args=(outlier_cfg,))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flags for number of unique vasopressors received\n",
    "vaso_flags = pd.get_dummies(\n",
    "    ne_df.groupby(id_col)['med_category']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'med_category': 'vaso_count'})\n",
    "    .set_index(id_col)['vaso_count']\n",
    "    .apply(lambda x: f'vasopressors_{x}')\n",
    ").astype(int)\n",
    "# Create flag for if patient was ever on any pressor\n",
    "on_pressor = pd.DataFrame(\n",
    "    ne_df.groupby(id_col).size() > 0,\n",
    "    columns=['on_pressor']\n",
    ").astype(int)\n",
    "\n",
    "# Create flag for number of concurrent pressors at any time\n",
    "n_pressors = pd.DataFrame(\n",
    "    ne_df.groupby([id_col, 'admin_dttm'])['med_category']\n",
    "    .nunique()\n",
    "    .groupby(id_col)\n",
    "    .max()\n",
    "    .rename('n_pressors')\n",
    ")\n",
    "\n",
    "# Combine with existing vaso_flags\n",
    "vaso_flags = pd.concat([vaso_flags, on_pressor, n_pressors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to make sure id_col is a column, not an index\n",
    "vaso_flags = vaso_flags.reset_index()\n",
    "final_df = final_df.merge(\n",
    "    vaso_flags[[id_col, 'on_pressor', 'n_pressors']],\n",
    "    on=id_col,\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dose_stats = (\n",
    "    ne_df\n",
    "    .groupby([id_col, 'med_category'], as_index=False)['med_dose_converted']\n",
    "    .agg(\n",
    "        median = 'median',\n",
    "        q1     = lambda x: x.quantile(0.25),\n",
    "        q3     = lambda x: x.quantile(0.75)\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2) Pivot into a wide format with MultiIndex columns\n",
    "dose_wide = dose_stats.pivot(\n",
    "    index=id_col,\n",
    "    columns='med_category',\n",
    "    values=['median','q1','q3']\n",
    ")\n",
    "\n",
    "# 3) Flatten the MultiIndex to single‐level: \"<med>_<stat>\"\n",
    "dose_wide.columns = [\n",
    "    f\"{med.lower()}_{stat}\"  # e.g. \"norepinephrine_median\", \"norepinephrine_q1\", ...\n",
    "    for stat, med in dose_wide.columns\n",
    "]\n",
    "dose_wide = dose_wide.reset_index()\n",
    "\n",
    "final_df = final_df.merge(\n",
    "    dose_wide,\n",
    "    on=id_col,\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOFA Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_path= pyCLIF.helper['tables_path']\n",
    "sofa_input_df = icu_summary[[id_col, 'first_icu_in_dttm']].copy()\n",
    "sofa_input_df = sofa_input_df.rename(columns={'first_icu_in_dttm': 'start_dttm'})\n",
    "sofa_input_df['stop_dttm'] = sofa_input_df['start_dttm'] + pd.Timedelta(hours=24)\n",
    "id_mappings = all_ids[[id_col, 'hospitalization_id' ]].drop_duplicates()\n",
    "\n",
    "sofa_df = sofa_score.compute_sofa(\n",
    "            ids_w_dttm = sofa_input_df,          # id, start_dttm, end_dttm  (local time)\n",
    "            tables_path = tables_path,\n",
    "            use_hospitalization_id = False,         # or False + id_mapping (new id , hospitalization_id)\n",
    "            id_mapping = id_mappings,              # first column should be your new id_variable, second column is hospitalization id\n",
    "            helper_module = pyCLIF,               \n",
    "            output_filepath = \"../output/intermediate/sofa.parquet\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = (\n",
    "    final_df\n",
    "    .merge(sofa_df, on=id_col, how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = (\n",
    "    final_df\n",
    "    .merge(all_ids[[id_col, 'patient_id']], on=id_col, how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns\n",
    "final_df.to_parquet(\"../output/intermediate/final_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TableOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the overall table and grab its Variable order\n",
    "tbl_overall = pyCLIF.make_table_one(final_df, patient_cohort, id_col=\"encounter_block\")\n",
    "var_order = tbl_overall[\"Variable\"].tolist()         \n",
    "\n",
    "if pyCLIF.helper[\"site_name\"].lower() == 'mimic':\n",
    "    # Write table_by_year to CSV in output/final directory\n",
    "    tbl_overall.to_csv(f\"../output/final/table_one_{pyCLIF.helper['site_name']}.csv\", index=False)\n",
    "    # View\n",
    "    print(tbl_overall.to_markdown(index=False))\n",
    "else:\n",
    "    # Build a dict of Series (Overall + each year)\n",
    "    # get sorted list of all calendar years in your data\n",
    "    years = sorted(final_df['admission_dttm'].dt.year.unique())\n",
    "    # build a dict of Series, one for “Overall,” one for each year\n",
    "    results = {}\n",
    "    results = {\"Overall\": tbl_overall.set_index(\"Variable\")[\"Overall\"]}\n",
    "\n",
    "    for yr in sorted(final_df[\"admission_dttm\"].dt.year.unique()):\n",
    "        df_y = final_df[final_df[\"admission_dttm\"].dt.year == yr]\n",
    "        tbl_y = pyCLIF.make_table_one(df_y, patient_cohort, id_col=\"encounter_block\")\n",
    "        results[str(yr)] = tbl_y.set_index(\"Variable\")[\"Overall\"]\n",
    "\n",
    "    # 3) Create the wide DataFrame, using the saved var_order\n",
    "    table_by_year = (\n",
    "        pd.DataFrame(results)           # index is Variable\n",
    "        .reindex(var_order)          # enforce your original row order\n",
    "        .reset_index()               # bring Variable back as a column\n",
    "        .rename(columns={\"index\":\"Variable\"})\n",
    "    )\n",
    "    # Write table_by_year to CSV in output/final directory\n",
    "    table_by_year.to_csv(f\"../output/final/table_one_{pyCLIF.helper['site_name']}.csv\", index=False)\n",
    "    # View\n",
    "    print(table_by_year.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".clif_table_one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
