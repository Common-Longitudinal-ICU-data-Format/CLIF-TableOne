{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cc294e",
   "metadata": {},
   "source": [
    "## CLIF Table One\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "Date v1: September 8, 2025\n",
    "\n",
    "This script identifies the cohort of encounters with at least one ICU stay and then summarizes the cohort data into one table. \n",
    "\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`, `clif_patient_assessments`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **clif_patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **clif_hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`,`discharge_dttm`, `age_at_admission` | - |\n",
    "| **clif_adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category` | - |\n",
    "| **clif_vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | weight_kg |\n",
    "| **clif_labs** | `hospitalization_id`, `lab_result_dttm`, `lab_order_dttm`, `lab_category`, `lab_value_numeric` | creatinine, bilirubin_total, po2_arterial, platelet_count |\n",
    "| **clif_medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin(optional) |\n",
    "| **clif_respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`,  `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs`, `tidal_volume_set`, `pressure_control_set`, `pressure_support_set` | - |\n",
    "| **clif_patient_assessments** | `hospitalization_id`, `recorded_dttm` , `assessment_category`, `numerical_value`| `gcs_total` |\n",
    "| **clif_crrt_therapy** | `hospitalization_id`, `recorded_dttm` | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bf46d",
   "metadata": {},
   "source": [
    "## Cohort Identification\n",
    "\n",
    "\n",
    "## Inclusion \n",
    "1. Adults\n",
    "2. Patients with at least one ICU stay or those who had only emergency department or ward encounters and either died or received life support at any point. Life support is defined as the administration of any vasoactive drugs or respiratory support exceeding low-flow oxygen.\n",
    "\n",
    "Respiratory support device: 'IMV', 'NIPPV', 'CPAP', 'High Flow NC'  \n",
    "\n",
    "Vasoactive: 'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import sys\n",
    "import clifpy\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import colorsys\n",
    "import matplotlib.path\n",
    "import matplotlib.patches\n",
    "import matplotlib.patheffects\n",
    "import polars as pl\n",
    "print(\"=== Environment Verification ===\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"clifpy version: {clifpy.__version__}\")\n",
    "print(f\"clifpy location: {clifpy.__file__}\")\n",
    "\n",
    "print(\"\\n=== Python Path Check ===\")\n",
    "local_clifpy_path = \"/Users/kavenchhikara/Desktop/CLIF/CLIFpy\"\n",
    "if any(local_clifpy_path in path for path in sys.path):\n",
    "    print(\"⚠️  WARNING: Local CLIFpy still in path!\")\n",
    "    for path in sys.path:\n",
    "        if local_clifpy_path in path:\n",
    "            print(f\"   Found: {path}\")\n",
    "else:\n",
    "    print(\"✅ Clean environment - no local CLIFpy in path\")\n",
    "\n",
    "print(f\"\\n=== Working Directory ===\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../../config/config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Create the output directory for tableone results if it does not already exist\n",
    "# Create necessary output directories within output/final\n",
    "output_base_dir = Path(\"../../output/final\")\n",
    "# Add all required subdirectories, including 'tableone/figures'\n",
    "subdirs = [\n",
    "    'tableone',\n",
    "    'tableone/figures',\n",
    "    'tableone/mcide',\n",
    "    'tableone/summary_stats',\n",
    "    'reports',\n",
    "    'results',\n",
    "    'clifpy'\n",
    "]\n",
    "for subdir in subdirs:\n",
    "    dir_path = output_base_dir / subdir\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "output_dir = output_base_dir / 'tableone'\n",
    "mcide_dir = '../../output/final/tableone/mcide'\n",
    "clifpy_dir = '../../output/final/clifpy'\n",
    "summary_stats_dir = '../../output/final/tableone/summary_stats'\n",
    "\n",
    "print(f\"\\n=� Configuration:\")\n",
    "print(f\"   Data directory: {config['tables_path']}\")\n",
    "print(f\"   File type: {config['file_type']}\")\n",
    "print(f\"   Timezone: {config['timezone']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127edcbe",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4319503",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Defining Required Data Elements\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Full patient table \n",
    "\n",
    "# Full hospitalization table \n",
    "\n",
    "# Full ADT table\n",
    "\n",
    "# Vitals\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "# Respiratory Support \n",
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set', \n",
    "    'pressure_control_set',\n",
    "    'pressure_support_set',\n",
    "    'peak_inspiratory_pressure_set',\n",
    "    'peak_inspiratory_pressure_obs',\n",
    "    'plateau_pressure_obs',\n",
    "    'minute_vent_obs'\n",
    "]\n",
    "\n",
    "\n",
    "# Continuous administered meds\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol',\n",
    "    'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', \n",
    "    'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee461357",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7cbe42",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_counts(clif_table, field_names, output_dir=None):\n",
    "    \"\"\"\n",
    "    Get N (count) for all unique combinations of the specified fields from CLIF table.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clif_table : CLIF table object\n",
    "        CLIF table object with .df attribute (e.g., clif.patient, clif.crrt_therapy).\n",
    "    field_names : list of str\n",
    "        List of field names to calculate count combinations for.\n",
    "    output_dir : str, optional\n",
    "        Directory path to save CSV file. If provided, saves summary CSV.\n",
    "        Default is None (no files saved).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with all unique combinations of field_names and a column 'N' with counts.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> result = get_value_counts(clif.patient, ['sex_name','sex_category'])\n",
    "    >>> result = get_value_counts(clif.patient, ['race_name', 'race_category'], \n",
    "    ...                          output_dir='../output/final/tableone')\n",
    "    \"\"\"\n",
    "    df = clif_table.df\n",
    "    # Filter to only valid columns\n",
    "    valid_fields = [field for field in field_names if field in df.columns]\n",
    "    if not valid_fields:\n",
    "        raise ValueError(\"None of the specified fields are in the DataFrame.\")\n",
    "        \n",
    "    # Group by all specified fields and count\n",
    "    combo_counts = (\n",
    "        df.groupby(valid_fields, dropna=False)\n",
    "          .size()\n",
    "          .reset_index(name='N')\n",
    "    )\n",
    "    \n",
    "    if output_dir:\n",
    "        import os\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        out_path = f\"{output_dir}/{'_'.join(valid_fields)}_counts.csv\"\n",
    "        combo_counts.to_csv(out_path, index=False)\n",
    "    \n",
    "    return combo_counts\n",
    "\n",
    "def create_summary_table(clif_table, numeric_cols, group_by_cols=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Create summary statistics for numeric columns from CLIF table.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clif_table : CLIF table object\n",
    "        CLIF table object with .df attribute.\n",
    "    numeric_cols : str or list of str\n",
    "        Column name(s) to summarize (e.g., 'fio2_set' or ['fio2_set', 'peep_set']).\n",
    "    group_by_cols : str or list of str, optional\n",
    "        Column name(s) to group by. If None, provides overall summary.\n",
    "    output_dir : str, optional\n",
    "        Directory path to save CSV. If provided, saves summary table.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Summary with columns: [group_cols (if any), 'variable', 'N', 'missing', \n",
    "        'min', 'q25', 'median', 'q75', 'mean', 'max'].\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> summary = create_summary_table(clif.respiratory_support, 'fio2_set')\n",
    "    >>> summary = create_summary_table(clif.respiratory_support, \n",
    "    ...                                ['fio2_set', 'peep_set'], \n",
    "    ...                                group_by_cols='device_category',\n",
    "    ...                                output_dir='../output/final/tableone')\n",
    "    \"\"\"\n",
    "    df = clif_table.df\n",
    "    table_name = str(clif_table.labs).split('.')[-1].split()[0]\n",
    "    if isinstance(numeric_cols, str):\n",
    "        numeric_cols = [numeric_cols]\n",
    "    if isinstance(group_by_cols, str):\n",
    "        group_by_cols = [group_by_cols]\n",
    "    \n",
    "    summaries = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        agg_dict = {\n",
    "            col: ['count', lambda x: x.isna().sum(), 'min', \n",
    "                  lambda x: x.quantile(0.25), lambda x: x.quantile(0.50),\n",
    "                  lambda x: x.quantile(0.75), 'mean', 'max']\n",
    "        }\n",
    "        \n",
    "        if group_by_cols:\n",
    "            summary = df.groupby(group_by_cols).agg(agg_dict)\n",
    "            summary.columns = summary.columns.droplevel(0)\n",
    "            summary = summary.reset_index()\n",
    "        else:\n",
    "            summary = df.agg(agg_dict).to_frame().T\n",
    "            summary.columns = summary.columns.droplevel(0)\n",
    "        \n",
    "        summary.columns = (list(group_by_cols) if group_by_cols else []) + \\\n",
    "                         ['N', 'missing', 'min', 'q25', 'median', 'q75', 'mean', 'max']\n",
    "        summary.insert(len(group_by_cols) if group_by_cols else 0, 'variable', col)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    result = pd.concat(summaries, ignore_index=True)\n",
    "    \n",
    "    if output_dir:\n",
    "        filename = f\"{table_name}_{'_'.join(group_by_cols) if group_by_cols else 'overall'}_summary.csv\"\n",
    "        result.to_csv(f\"{output_dir}/{filename}\", index=False)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_colors(n):\n",
    "    \"\"\"Generate n visually distinct colors.\"\"\"\n",
    "    hue_partition = 1 / (n + 1)\n",
    "    colors = [colorsys.hsv_to_rgb(hue_partition * value, 0.8, 0.5)\n",
    "              for value in range(0, n)]\n",
    "    return reversed(colors[::2] + colors[1::2])\n",
    "\n",
    "\n",
    "class Sankey:\n",
    "    def __init__(self, df,\n",
    "                 plot_width=8,\n",
    "                 plot_height=8,\n",
    "                 gap=0.12,\n",
    "                 alpha=0.3,\n",
    "                 fontsize='small',\n",
    "                 order=None,\n",
    "                 mapping=None,\n",
    "                 tag=None,\n",
    "                 title=None,\n",
    "                 title_left=None,\n",
    "                 title_right=None,\n",
    "                 labels=True,\n",
    "                 block_width=0.1,\n",
    "                 block_fontsize=12,\n",
    "                 flow_color_func=None,\n",
    "                 colors=None,\n",
    "                 ax=None\n",
    "    ):\n",
    "        self.df = df\n",
    "        if ax:\n",
    "            self.plot_width = ax.get_position().width * ax.figure.get_size_inches()[0]\n",
    "            self.plot_height = ax.get_position().height * ax.figure.get_size_inches()[1]\n",
    "        else:\n",
    "            self.plot_width = plot_width\n",
    "            self.plot_height = plot_height\n",
    "        self.gap = gap\n",
    "        self.block_width = block_width\n",
    "        self.block_fontsize = block_fontsize\n",
    "        self.alpha = alpha\n",
    "        self.labels = labels\n",
    "        self.fontsize = fontsize\n",
    "        self.order = order\n",
    "        self.flow_color_func = flow_color_func\n",
    "        self.mapping_colors = {\n",
    "            'increase': '#1f721c',\n",
    "            'decrease': '#ddc90f',\n",
    "            'mistake': '#dd1616',\n",
    "            'correct': '#dddddd',\n",
    "            'novel': '#59a8d6',\n",
    "        }\n",
    "\n",
    "        self.init_figure(ax)\n",
    "        self.init_flows()\n",
    "        self.init_nodes(order)\n",
    "        self.init_widths()\n",
    "        \n",
    "        # inches per 1 item in x and y\n",
    "        self.resolution = (plot_height - gap * (len(order) - 1)) / df.shape[0]\n",
    "        \n",
    "        if colors is not None:\n",
    "            self.colors = colors\n",
    "        else:\n",
    "            self.colors = {\n",
    "                name: colour\n",
    "                for name, colour\n",
    "                in zip(self.nodes[0].keys(),\n",
    "                    get_distinct_colors(len(self.nodes[0])))\n",
    "            }\n",
    "\n",
    "        self.init_offsets()\n",
    "\n",
    "    def init_figure(self, ax):\n",
    "        if ax is None:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = plt.Axes(self.fig, [0, 0, 1, 1])\n",
    "            self.fig.add_axes(self.ax)\n",
    "        else:\n",
    "            self.fig = ax.figure\n",
    "            self.ax = ax\n",
    "\n",
    "    def init_flows(self):\n",
    "        self.flows = []\n",
    "        n_cols = self.df.columns.size\n",
    "        for i in range(n_cols - 1):\n",
    "            x, y = self.df.iloc[:, i], self.df.iloc[:, i + 1]\n",
    "            self.flows.append(collections.Counter(zip(x, y)))\n",
    "\n",
    "    def init_nodes(self, order):\n",
    "        self.nodes = []\n",
    "\n",
    "        for i in range(self.df.columns.size):\n",
    "            column = collections.OrderedDict()\n",
    "            counts = self.df.iloc[:, i].value_counts()\n",
    "            for item in order:\n",
    "                if item in counts:\n",
    "                    column[item] = counts[item]\n",
    "                else:\n",
    "                    column[item] = 0\n",
    "            self.nodes.append(column)\n",
    "\n",
    "    def init_widths(self):\n",
    "        self.left_stop = self.block_width\n",
    "        self.right_stop = self.plot_width - self.block_width\n",
    "        self.stops = []\n",
    "        n_cols = self.df.columns.size\n",
    "        self.flow_width = (self.plot_width - self.block_width * (n_cols - 2)) / (n_cols - 1)\n",
    "\n",
    "        for i in range(1, n_cols):\n",
    "            stop1 = (self.block_width * i\n",
    "                     + self.flow_width * (i - 1) + self.flow_width * 7 / 20)\n",
    "            stop2 = (self.block_width * i\n",
    "                     + self.flow_width * (i - 1) + self.flow_width * 13 / 20)\n",
    "            self.stops.append((stop1, stop2))\n",
    "\n",
    "    def init_offsets(self):\n",
    "        self.offsets = []\n",
    "\n",
    "        for col in self.nodes:\n",
    "            offset = 0\n",
    "            offsets = collections.OrderedDict()\n",
    "            for name, size in col.items():\n",
    "                offsets[name] = offset\n",
    "                offset += size * self.resolution + self.gap\n",
    "            self.offsets.append(offsets)\n",
    "\n",
    "    def draw_flow(self, x, left, right, flow, node_offsets_l, node_offsets_r):\n",
    "        P = matplotlib.path.Path\n",
    "\n",
    "        left_y = self.offsets[x][left] + node_offsets_l[left]\n",
    "        right_y = self.offsets[x + 1][right] + node_offsets_r[right]\n",
    "\n",
    "        flow *= self.resolution\n",
    "\n",
    "        node_offsets_l[left] += flow\n",
    "        node_offsets_r[right] += flow\n",
    "        \n",
    "        if self.flow_color_func is not None:\n",
    "            mapping = self.flow_color_func(left, right)\n",
    "            color = self.mapping_colors[mapping]\n",
    "        else:\n",
    "            color = self.colors[left]\n",
    "\n",
    "        left_x = self.flow_width * x + self.block_width * (x + 1)\n",
    "        right_x  = left_x + self.flow_width\n",
    "\n",
    "        path_data = [\n",
    "            (P.MOVETO, (left_x, -left_y)),\n",
    "            (P.LINETO, (left_x, -left_y - flow)),\n",
    "            (P.CURVE4, (self.stops[x][0], -left_y - flow)),\n",
    "            (P.CURVE4, (self.stops[x][1], -right_y - flow)),\n",
    "            (P.CURVE4, (right_x, -right_y - flow)),\n",
    "            (P.LINETO, (right_x, -right_y)),\n",
    "            (P.CURVE4, (self.stops[x][1], -right_y)),\n",
    "            (P.CURVE4, (self.stops[x][0], -left_y)),\n",
    "            (P.CURVE4, (left_x, -left_y)),\n",
    "            (P.CLOSEPOLY, (left_x, -left_y)),\n",
    "        ]\n",
    "        codes, verts = zip(*path_data)\n",
    "        path = P(verts, codes)\n",
    "        patch = matplotlib.patches.PathPatch(\n",
    "            path,\n",
    "            facecolor=color,\n",
    "            alpha=0.9 if flow < .02 else self.alpha,\n",
    "            edgecolor='none',\n",
    "        )\n",
    "        self.ax.add_patch(patch)\n",
    "\n",
    "    def draw_node(self, x, y, size, name):\n",
    "        if size <= 0:\n",
    "            return\n",
    "        y = -list(self.offsets[x].values())[y] - size * self.resolution\n",
    "        x = self.flow_width * x + self.block_width * x\n",
    "        color = self.colors[name]\n",
    "        patch = matplotlib.patches.Rectangle(\n",
    "            (x, y),\n",
    "            width=self.block_width,\n",
    "            height=size * self.resolution,\n",
    "            facecolor=color,\n",
    "            edgecolor='none',\n",
    "        )\n",
    "        self.ax.add_patch(patch)\n",
    "        self.ax.text(\n",
    "            x + self.block_width / 2,\n",
    "            y + size * self.resolution / 2,\n",
    "            name,\n",
    "            color=\"black\",\n",
    "            va=\"center\",\n",
    "            ha=\"center\",\n",
    "            size=self.block_fontsize,\n",
    "            path_effects=[\n",
    "                matplotlib.patheffects.Stroke(linewidth=2, foreground=\"white\"),\n",
    "                matplotlib.patheffects.Normal()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def draw(self):\n",
    "        for x, col in enumerate(self.nodes):\n",
    "            for y, (name, size) in enumerate(col.items()):\n",
    "                self.draw_node(x, y, size, name)\n",
    "\n",
    "        for x, flows in enumerate(self.flows):\n",
    "            node_offsets_l = collections.Counter()\n",
    "            node_offsets_r = collections.Counter()\n",
    "\n",
    "            for (left, right), flow in sorted(\n",
    "                flows.items(),\n",
    "                key=lambda x: (self.order.index(x[0][0]), self.order.index(x[0][1]))\n",
    "            ):\n",
    "                self.draw_flow(\n",
    "                    x,\n",
    "                    left,\n",
    "                    right,\n",
    "                    flow,\n",
    "                    node_offsets_l,\n",
    "                    node_offsets_r\n",
    "                )\n",
    "\n",
    "        self.ax.set_ylim(\n",
    "            -self.resolution * self.df.shape[0] - self.gap * (len(self.order) - 1),\n",
    "            0\n",
    "        )\n",
    "        self.ax.set_xlim(\n",
    "            0,\n",
    "            self.block_width * self.df.shape[1] + self.flow_width * (self.df.shape[1] - 1)\n",
    "        )\n",
    "        self.ax.get_xaxis().set_visible(False)\n",
    "        self.ax.get_yaxis().set_visible(False)\n",
    "        for k in self.ax.spines.keys():\n",
    "            self.ax.spines[k].set_visible(False)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Data Preparation Functions\n",
    "# ============================================================================\n",
    "\n",
    "def simplify_location_category(location):\n",
    "    \"\"\"Map location to simplified categories.\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return 'Other'\n",
    "    \n",
    "    location_lower = str(location).lower()\n",
    "    \n",
    "    if location_lower == 'icu':\n",
    "        return 'ICU'\n",
    "    elif location_lower == 'ward':\n",
    "        return 'Ward'\n",
    "    elif location_lower == 'ed':\n",
    "        return 'ED'\n",
    "    elif location_lower == 'procedural':\n",
    "        return 'Procedural'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "\n",
    "def create_outcome_df(final_tableone_df):\n",
    "    \"\"\"Create outcome dataframe with death indicator.\"\"\"\n",
    "    outcome_df = final_tableone_df[['encounter_block', 'death_enc', 'death_dttm', 'discharge_dttm']].copy()\n",
    "    outcome_df['final_outcome_dttm'] = outcome_df['death_dttm'].fillna(outcome_df['discharge_dttm'])\n",
    "    outcome_df = outcome_df[['encounter_block', 'death_enc', 'final_outcome_dttm']].drop_duplicates()\n",
    "    return outcome_df\n",
    "\n",
    "\n",
    "def prepare_sankey_wide_format(adt_cohort, encounter_blocks, outcome_df, max_locations=7):\n",
    "    \"\"\"\n",
    "    Transform ADT data into wide format for Sankey diagram.\n",
    "    Returns DataFrame where each row = encounter, each column = location position.\n",
    "    \"\"\"\n",
    "    # Filter and sort\n",
    "    adt_filtered = adt_cohort[adt_cohort['encounter_block'].isin(encounter_blocks)].copy()\n",
    "    adt_filtered = adt_filtered.sort_values(['encounter_block', 'in_dttm'])\n",
    "    \n",
    "    # Simplify locations\n",
    "    adt_filtered['location_simple'] = adt_filtered['location_category'].apply(simplify_location_category)\n",
    "    \n",
    "    # Add position/segment rank\n",
    "    adt_filtered['segment_rank'] = adt_filtered.groupby('encounter_block').cumcount() + 1\n",
    "    adt_filtered = adt_filtered[adt_filtered['segment_rank'] <= max_locations]\n",
    "    \n",
    "    # Pivot to wide format\n",
    "    sankey_df = adt_filtered.pivot(\n",
    "        index='encounter_block',\n",
    "        columns='segment_rank',\n",
    "        values='location_simple'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns to float format (1.0, 2.0, etc.) to match example\n",
    "    column_mapping = {col: float(col) for col in sankey_df.columns if col != 'encounter_block'}\n",
    "    sankey_df = sankey_df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Fill NaN with 'Discharged'\n",
    "    location_cols = [float(i) for i in range(1, max_locations + 1)]\n",
    "    for col in location_cols:\n",
    "        if col not in sankey_df.columns:\n",
    "            sankey_df[col] = 'Discharged'\n",
    "        else:\n",
    "            sankey_df[col] = sankey_df[col].fillna('Discharged')\n",
    "    \n",
    "    # Merge death information\n",
    "    sankey_df = sankey_df.merge(\n",
    "        outcome_df[['encounter_block', 'death_enc']], \n",
    "        on='encounter_block', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return sankey_df, location_cols\n",
    "\n",
    "\n",
    "def propagate_death(df, location_cols):\n",
    "    \"\"\"\n",
    "    Propagate 'Died' status across all subsequent segments.\n",
    "    Once a patient dies, all future segments show 'Died'.\n",
    "    \"\"\"\n",
    "    def propagate_row(row):\n",
    "        death_found = False\n",
    "        for col in location_cols:\n",
    "            if row[col] == 'Died':\n",
    "                death_found = True\n",
    "            if death_found:\n",
    "                row[col] = 'Died'\n",
    "        return row\n",
    "    \n",
    "    df[location_cols] = df[location_cols].apply(propagate_row, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def mark_final_outcomes(df, location_cols):\n",
    "    \"\"\"\n",
    "    Mark the final outcome for each encounter based on death_enc.\n",
    "    \"\"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        # Find last actual location (not Discharged/Died)\n",
    "        last_location_idx = None\n",
    "        for i, col in enumerate(location_cols):\n",
    "            if row[col] not in ['Discharged', 'Died']:\n",
    "                last_location_idx = i\n",
    "        \n",
    "        if last_location_idx is not None:\n",
    "            # Determine outcome\n",
    "            if row['death_enc'] == 1:\n",
    "                outcome = 'Died'\n",
    "            else:\n",
    "                outcome = 'Discharged'\n",
    "            \n",
    "            # Fill remaining positions with outcome\n",
    "            for i in range(last_location_idx + 1, len(location_cols)):\n",
    "                df.at[idx, location_cols[i]] = outcome\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sankey_diagram(adt_cohort, encounter_blocks, outcome_df, \n",
    "                         max_locations=7,\n",
    "                         title=\"Patient Flow Through Locations\",\n",
    "                         output_file=None,\n",
    "                         figsize=(16, 8)):\n",
    "    \"\"\"\n",
    "    Create Sankey diagram using custom matplotlib Sankey class.\n",
    "    \"\"\"\n",
    "    # Prepare data in wide format\n",
    "    sankey_df, location_cols = prepare_sankey_wide_format(\n",
    "        adt_cohort, encounter_blocks, outcome_df, max_locations\n",
    "    )\n",
    "    \n",
    "    # Mark final outcomes based on death_enc\n",
    "    sankey_df = mark_final_outcomes(sankey_df, location_cols)\n",
    "    \n",
    "    # Propagate death status through remaining positions\n",
    "    sankey_df = propagate_death(sankey_df, location_cols)\n",
    "    \n",
    "    # Define colors (matching your preferences)\n",
    "    # colors = {\n",
    "    #     \"ED\": '#f08080',         # Light coral (red)\n",
    "    #     \"ICU\": '#c9a0a0',        # Dusty rose\n",
    "    #     \"Ward\": '#87ceeb',       # Sky blue\n",
    "    #     'Procedural': '#d8bfd8', # Thistle (purple)\n",
    "    #     \"Other\": '#d3d3d3',      # Light gray\n",
    "    #     \"Discharged\": '#d3d3d3', # Light gray\n",
    "    #     \"Died\": '#696969'        # Dim gray (dark)\n",
    "    # }\n",
    "    colors = {\n",
    "            \"ED\": '#f08080',         # Light coral (keep)\n",
    "            \"ICU\": '#87ceeb',        # Sky blue (SWAP with Ward!)\n",
    "            \"Ward\": '#d4c5a0',       # Beige/tan (neutral, distinct)\n",
    "            'Procedural': '#c9a0c9', # Light purple (but be careful)\n",
    "            \"Other\": '#e8e8e8',      # Very light gray\n",
    "            \"Discharged\": '#b8d4b8', # Light sage green\n",
    "            \"Died\": '#696969'        # Dark gray (keep)\n",
    "        }\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=figsize, constrained_layout=True)\n",
    "    \n",
    "    # Create Sankey diagram\n",
    "    diag = Sankey(\n",
    "        sankey_df[location_cols],\n",
    "        ax=ax,\n",
    "        order=[\"ED\", \"Ward\", \"ICU\", \"Procedural\", \"Other\", \"Discharged\", \"Died\"],\n",
    "        block_width=0.15,\n",
    "        colors=colors,\n",
    "        alpha=0.4,\n",
    "        block_fontsize=10,\n",
    "        gap=0.05  # Smaller gap between node types\n",
    "    )\n",
    "    diag.draw()\n",
    "    \n",
    "    # Set title\n",
    "    ax.set_title(f\"{title}\\nN={len(encounter_blocks)} encounters\", \n",
    "                 size=18, pad=20, fontweight='bold')\n",
    "    \n",
    "    # Set x-axis ticks for location numbers\n",
    "    ax.set_xticks([\n",
    "        diag.block_width / 2 + diag.flow_width * x + diag.block_width * x \n",
    "        for x in range(len(location_cols))\n",
    "    ])\n",
    "    ax.set_xticklabels([int(col) for col in location_cols])\n",
    "    ax.set_xlabel(\"Location number\", size=16, fontweight='bold')\n",
    "    ax.get_xaxis().set_visible(True)\n",
    "    ax.tick_params(axis=\"x\", pad=10, labelsize=14)\n",
    "    \n",
    "    # Save if requested\n",
    "    if output_file:\n",
    "        fig.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"✅ Saved: {output_file}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, sankey_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffe16d",
   "metadata": {},
   "source": [
    "## Cohort identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Loading CLIF Tables\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from clifpy.clif_orchestrator import ClifOrchestrator\n",
    "\n",
    "# Initialize ClifOrchestrator\n",
    "clif = ClifOrchestrator(\n",
    "    data_directory=config['tables_path'],\n",
    "    filetype=config['file_type'],\n",
    "    timezone=config['timezone'],\n",
    "    output_directory='../output/final/clifpy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553499b",
   "metadata": {},
   "source": [
    "## Step0: Load Core Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc9ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 0: Load Core Tables (Patient, Hospitalization, ADT)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 0: Load Core Tables (Patient, Hospitalization, ADT)\")\n",
    "print(\"=\" * 80)\n",
    "core_tables = ['patient', 'hospitalization', 'adt']\n",
    "\n",
    "print(f\"\\nLoading {len(core_tables)} core tables...\")\n",
    "for table_name in core_tables:\n",
    "    print(f\"   Loading {table_name}...\", end=\" \")\n",
    "    try:\n",
    "        clif.load_table(table_name)\n",
    "        table = getattr(clif, table_name)\n",
    "        print(f\"✓ ({len(table.df):,} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nCore tables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_df = clif.hospitalization.df\n",
    "adt_df = clif.adt.df\n",
    "\n",
    "# Merge to get age information\n",
    "all_encounters = pd.merge(\n",
    "    hosp_df[[\"patient_id\", \"hospitalization_id\", \"admission_dttm\", \"discharge_dttm\", \n",
    "             \"age_at_admission\", \"discharge_category\", \"admission_type_category\"]],\n",
    "    adt_df[[\"hospitalization_id\", \"hospital_id\", \"in_dttm\", \"out_dttm\", \n",
    "            \"location_category\", \"location_type\"]],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_value_counts(clif.adt, ['location_name', 'location_category'], output_dir=mcide_dir)\n",
    "get_value_counts(clif.hospitalization, ['discharge_name', 'discharge_category'], output_dir=mcide_dir)\n",
    "get_value_counts(clif.hospitalization, ['admission_type_name', 'admission_type_category'], output_dir=mcide_dir)\n",
    "get_value_counts(clif.patient, ['race_name', 'race_category'], output_dir=mcide_dir)\n",
    "get_value_counts(clif.patient, ['ethnicity_name', 'ethnicity_category'], output_dir=mcide_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates by ['hospitalization_id', 'in_dttm', 'out_dttm']\n",
    "dup_counts = all_encounters.duplicated(subset=['hospitalization_id', 'in_dttm', 'out_dttm']).sum()\n",
    "if dup_counts > 0:\n",
    "    print(f\"Warning: {dup_counts} duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")\n",
    "else:\n",
    "    print(\"No duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5f8fc",
   "metadata": {},
   "source": [
    "## Step1: Date & Age filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Identify Adult Patients (Age >= 18) and Admissions 2018-2024\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 1: Identifying Adult Patients (Age >= 18) and Admissions 2018-2024\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Applying initial cohort filters...\")\n",
    "\n",
    "# Use only the relevant columns from all_encounters\n",
    "adult_encounters = all_encounters[\n",
    "    [\n",
    "        'patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
    "        'age_at_admission', 'discharge_category', 'admission_type_category' ,'hospital_id',\n",
    "        'in_dttm', 'out_dttm', 'location_category', 'location_type'\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "if config['timezone'].lower() == \"mimic\":\n",
    "    # MIMIC: only age >= 18, no admit year restriction\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['age_at_admission'] >= 18) & (adult_encounters['age_at_admission'].notna())\n",
    "    ]\n",
    "else:\n",
    "    # Other sites: age >= 18 and admission between 2018-2024 inclusive\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['age_at_admission'] >= 18) &\n",
    "        (adult_encounters['age_at_admission'].notna()) &\n",
    "        (adult_encounters['admission_dttm'].dt.year >= 2018) &\n",
    "        (adult_encounters['admission_dttm'].dt.year <= 2024)\n",
    "    ]\n",
    "\n",
    "print(f\"\\nFiltering Results:\")\n",
    "print(f\"   Total hospitalizations: {len(all_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Adult hospitalizations (age >= 18, 2018-2024): {len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Excluded (age < 18 or outside 2018-2024): {len(all_encounters['hospitalization_id'].unique()) - len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "\n",
    "\n",
    "strobe_counts[\"0_total_hospitalizations\"] = len(all_encounters['hospitalization_id'].unique())\n",
    "strobe_counts[\"1_adult_hospitalizations\"] = len(adult_encounters['hospitalization_id'].unique())\n",
    "# Get list of adult hospitalization IDs for filtering\n",
    "adult_hosp_ids = set(adult_encounters['hospitalization_id'].unique())\n",
    "print(f\"\\n   Unique adult hospitalization IDs: {len(adult_hosp_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66edf5",
   "metadata": {},
   "source": [
    "### Stitch hospitalizations \n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_id`, then we combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block.\n",
    "\n",
    "If the `id_col` supplied by user is `hospitalization_joined_id` from the hospitalization table, then we consider the user has already stitched similar encounters, and we will consider that as the primary id column for all table joins moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4651cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clifpy.utils.stitching_encounters import stitch_encounters\n",
    "\n",
    "# stitch hospitalizations\n",
    "hosp_filtered = clif.hospitalization.df[clif.hospitalization.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "adt_filtered = clif.adt.df[clif.adt.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "\n",
    "hosp_stitched, adt_stitched, encounter_mapping = stitch_encounters(\n",
    "    hospitalization=hosp_filtered,\n",
    "    adt=adt_filtered,\n",
    "    time_interval=6  \n",
    ")\n",
    "\n",
    "# Direct assignment without additional copies\n",
    "clif.hospitalization.df = hosp_stitched\n",
    "clif.adt.df = adt_stitched\n",
    "\n",
    "# Store the encounter mapping in the orchestrator for later use\n",
    "clif.encounter_mapping = encounter_mapping\n",
    "\n",
    "# Clean up intermediate variables\n",
    "del hosp_filtered, adt_filtered\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After your stitching code, add these calculations:\n",
    "\n",
    "# Calculate stitching statistics\n",
    "strobe_counts['1b_before_stitching'] = len(adult_hosp_ids)  # Original adult hospitalizations\n",
    "strobe_counts['1b_after_stitching'] = len(hosp_stitched['encounter_block'].unique())  # Unique encounter blocks after stitching\n",
    "strobe_counts['1b_stitched_hosp_ids'] = strobe_counts['1b_before_stitching'] - strobe_counts['1b_after_stitching']  # Number of hospitalizations that were linked\n",
    "\n",
    "print(f\"\\nEncounter Stitching Results:\")\n",
    "print(f\"   Number of unique hospitalizations before stitching: {strobe_counts['1b_before_stitching']:,}\")\n",
    "print(f\"   Number of unique encounter blocks after stitching: {strobe_counts['1b_after_stitching']:,}\")\n",
    "print(f\"   Number of linked hospitalization ids: {strobe_counts['1b_stitched_hosp_ids']:,}\")\n",
    "\n",
    "# Optional: Show the encounter mapping details\n",
    "print(f\"\\nEncounter Mapping Details:\")\n",
    "print(f\"   Total encounter mappings created: {len(encounter_mapping):,}\")\n",
    "if len(encounter_mapping) > 0:\n",
    "    # Show some examples of how many original hospitalizations were combined\n",
    "    mapping_counts = encounter_mapping.groupby('encounter_block').size()\n",
    "    print(f\"   Encounter blocks with multiple hospitalizations: {(mapping_counts > 1).sum():,}\")\n",
    "    print(f\"   Maximum hospitalizations combined into one block: {mapping_counts.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806ca75",
   "metadata": {},
   "source": [
    "# ADT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all_encounters with encounter_mapping to get encounter_block information\n",
    "all_encounters = pd.merge(all_encounters, encounter_mapping, on='hospitalization_id', how='left')\n",
    "\n",
    "# Convert location_category and discharge_category to lowercase in place (vectorized)\n",
    "all_encounters['location_category'] = all_encounters['location_category'].str.lower()\n",
    "all_encounters['discharge_category'] = all_encounters['discharge_category'].str.lower()\n",
    "all_encounters['admission_type_category'] = all_encounters['admission_type_category'].str.lower()\n",
    "\n",
    "# Create vectorized ICU and death masks\n",
    "icu_mask = all_encounters['location_category'].str.contains('icu', na=False)\n",
    "death_mask = all_encounters['discharge_category'].isin(['expired', 'hospice'])\n",
    "\n",
    "# Vectorized: For each encounter_block, does any row have ICU or death? (much faster)\n",
    "# Use groupby('encounter_block')[mask].transform('any') to vectorize\n",
    "all_encounters['icu_enc'] = icu_mask.groupby(all_encounters['encounter_block']).transform('any').astype(int)\n",
    "all_encounters['death_enc'] = death_mask.groupby(all_encounters['encounter_block']).transform('any').astype(int)\n",
    "\n",
    "# Cohort flag using logical OR (vectorized)\n",
    "all_encounters['cohort_enc'] = (all_encounters['icu_enc'] | all_encounters['death_enc']).astype(int)\n",
    "\n",
    "# Store hospitalization_ids for cohort_enc==1 in a list (as before)\n",
    "cohort_enc_hospitalization_ids = all_encounters.loc[all_encounters['cohort_enc'] == 1, 'hospitalization_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df638c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_locations = all_encounters.groupby('encounter_block').agg({\n",
    "        'death_enc': 'max',\n",
    "        'icu_enc': 'max',  # Did they ever touch ICU?\n",
    "        'location_category': lambda x: set(x.dropna().str.lower())  # Set of all locations visited\n",
    "    }).reset_index()\n",
    "\n",
    "encounter_locations['has_procedural_or_ld'] = encounter_locations['location_category'].apply(\n",
    "          lambda locs: any(loc in {'procedural', 'l&d'} for loc in locs)\n",
    "      )\n",
    "\n",
    "# Step 3: Flag as procedural/L&D only if: \n",
    "# - MUST have procedural or L&D\n",
    "# - CANNOT have ICU (icu_enc == 0)\n",
    "encounter_locations['is_procedural_ld_only'] = (\n",
    "    (encounter_locations['icu_enc'] == 0) &\n",
    "    (encounter_locations['has_procedural_or_ld'] == True)\n",
    ").astype(int)\n",
    "\n",
    "# Join has_procedural_or_ld (and is_procedural_ld_only if desired) with all_encounters by encounter_block\n",
    "all_encounters = all_encounters.merge(\n",
    "    encounter_locations[['encounter_block', 'is_procedural_ld_only']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982478d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify encounters where death occurred\n",
    "death_encounters = all_encounters[all_encounters['death_enc'] == 1]\n",
    "# Identify those that never touched the ICU\n",
    "non_icu_deaths = death_encounters[~death_encounters['icu_enc'].astype(bool)]\n",
    "# Count the number of unique encounters with deaths outside of ICU\n",
    "num_deaths_outside_icu = non_icu_deaths['encounter_block'].nunique()\n",
    "# Calculate total deaths (unique encounter blocks with death)\n",
    "total_encounters = all_encounters['encounter_block'].nunique()\n",
    "# Calculate the percentage\n",
    "pct_deaths_outside_icu = (num_deaths_outside_icu / total_encounters * 100) if total_encounters > 0 else 0\n",
    "print(f\"Number of deaths outside ICU: {num_deaths_outside_icu} ({pct_deaths_outside_icu:.1f}% of all hospitalizations)\")\n",
    "\n",
    "# Add ICU encounters to strobe counts as 1_icu_encounters\n",
    "num_icu_encounters = all_encounters[all_encounters['icu_enc'] == 1]['encounter_block'].nunique()\n",
    "if 'strobe_counts' not in globals():\n",
    "    strobe_counts = {}\n",
    "strobe_counts['1_icu_encounters'] = num_icu_encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df96416",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cohort = all_encounters[\n",
    "    all_encounters['hospitalization_id'].isin(cohort_enc_hospitalization_ids)\n",
    "][['encounter_block', 'icu_enc', 'death_enc', 'cohort_enc']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1672f",
   "metadata": {},
   "source": [
    "# Respiratory Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Load Respiratory Support and Identify Patients on Advanced Respiratory support \n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" Loading Respiratory Support and Identifying IMV Patients\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nLoading respiratory_support table...\")\n",
    "clif.load_table('respiratory_support',\n",
    "                        columns=rst_required_columns,\n",
    "                        filters={'hospitalization_id': list(adult_hosp_ids)})\n",
    "print(f\"Respiratory support loaded ({len(clif.respiratory_support.df):,} rows)\")\n",
    "\n",
    "get_value_counts(clif.respiratory_support, ['device_name', 'device_category'], output_dir=mcide_dir)\n",
    "get_value_counts(clif.respiratory_support, ['mode_name', 'mode_category'], output_dir=mcide_dir)\n",
    "get_value_counts(clif.respiratory_support, ['device_name', 'device_category', 'mode_name', 'mode_category'], output_dir=mcide_dir)\n",
    "\n",
    "# Standardize category columns to lowercase\n",
    "print(f\"\\nStandardizing category columns...\")\n",
    "category_cols = [col for col in clif.respiratory_support.df.columns if col.endswith('_category')]\n",
    "for col in category_cols:\n",
    "    clif.respiratory_support.df[col] = clif.respiratory_support.df[col].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06229735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hospitalizations on advanced mechanical support\n",
    "print(f\"\\nIdentifying hospitalizations with advanced respiratory support devices...\")\n",
    "device_types = ['imv', 'nippv', 'cpap', 'high flow nc']\n",
    "clif.respiratory_support.df = pd.merge(clif.respiratory_support.df, encounter_mapping, \n",
    "                                        on='hospitalization_id', how='left')\n",
    "advanced_support_hosp_ids = clif.respiratory_support.df.loc[\n",
    "    clif.respiratory_support.df['device_category'].str.lower().isin([d.lower() for d in device_types]),\n",
    "    'encounter_block'\n",
    "].unique()\n",
    "print(f\"Hospitalizations with any advanced resp. device ({', '.join(device_types).upper()}): {len(advanced_support_hosp_ids):,}\")\n",
    "strobe_counts[\"2_advanced_resp_support_hospitalizations\"] = len(advanced_support_hosp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with encounter mapping\n",
    "resp_df = pd.merge(\n",
    "    clif.respiratory_support.df,\n",
    "    encounter_mapping,\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Standardize device_category\n",
    "resp_df['device_category'] = resp_df['device_category'].str.lower()\n",
    "\n",
    "# Filter to advanced support devices\n",
    "resp_advanced = resp_df[\n",
    "    resp_df['device_category'].isin([d.lower() for d in device_types])\n",
    "].copy()\n",
    "\n",
    "print(f\"   Advanced respiratory support records: {len(resp_advanced):,}\")\n",
    "\n",
    "# Merge with ADT to get location at time of device usage\n",
    "print(\"\\n4. Merging respiratory support with ADT locations...\")\n",
    "\n",
    "# Prepare ADT for merge_asof\n",
    "adt_for_merge = clif.adt.df[['hospitalization_id', 'in_dttm', 'out_dttm', 'location_category']].copy()\n",
    "adt_for_merge['location_category'] = adt_for_merge['location_category'].str.lower()\n",
    "\n",
    "# ---- Ensure merge_asof sorted order for both left and right sides ----\n",
    "# Pandas merge_asof requires both frames to be sorted by BOTH the 'by' key and the merge 'on' key\n",
    "# We'll sort and then groupby to ensure each group is strictly sorted for merge_asof\n",
    "\n",
    "# Remove rows with null merge keys to avoid ValueError\n",
    "resp_advanced_clean = resp_advanced.dropna(subset=['hospitalization_id', 'recorded_dttm']).copy()\n",
    "adt_for_merge_clean = adt_for_merge.dropna(subset=['hospitalization_id', 'in_dttm']).copy()\n",
    "\n",
    "# Sort required for merge_asof (by and on keys), must be strictly monotonic IN EACH group\n",
    "resp_advanced_clean = resp_advanced_clean.sort_values(['hospitalization_id', 'recorded_dttm']).reset_index(drop=True)\n",
    "adt_for_merge_clean = adt_for_merge_clean.sort_values(['hospitalization_id', 'in_dttm']).reset_index(drop=True)\n",
    "\n",
    "# ---- Extra check: ensure that each hospitalization_id group is sorted by time ----\n",
    "# (not strictly necessary since we already sorted above, but this is robust)\n",
    "def check_sorted_per_group(df, by, on):\n",
    "    grp = df.groupby(by)\n",
    "    for _, group in grp:\n",
    "        if not group[on].is_monotonic_increasing:\n",
    "            raise ValueError(f\"Group sorting check failed for '{by}' by '{on}'.\")\n",
    "check_sorted_per_group(resp_advanced_clean, 'hospitalization_id', 'recorded_dttm')\n",
    "check_sorted_per_group(adt_for_merge_clean, 'hospitalization_id', 'in_dttm')\n",
    "\n",
    "# Merge to get location at time of respiratory support (backward looks for the most recent previous ADT record)\n",
    "resp_with_location = pd.merge_asof(\n",
    "    resp_advanced_clean,\n",
    "    adt_for_merge_clean,\n",
    "    left_on='recorded_dttm',\n",
    "    right_on='in_dttm',\n",
    "    by='hospitalization_id',\n",
    "    direction='backward',\n",
    "    suffixes=('', '_adt')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214c7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e77873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with advanced_support_hosp_ids and 'high_support_en' == 1\n",
    "advanced_support_df = pd.DataFrame({\n",
    "    'encounter_block': advanced_support_hosp_ids,\n",
    "    'high_support_enc': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a full join (outer merge) of final_cohort and advanced_support_df on 'encounter_block'\n",
    "final_cohort = final_cohort.merge(\n",
    "    advanced_support_df,\n",
    "    on='encounter_block',\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170325ca",
   "metadata": {},
   "source": [
    "# Vasoactives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efe800",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading medication_admin_continuous table...\")\n",
    "clif.load_table(\n",
    "    'medication_admin_continuous',\n",
    "    columns=meds_required_columns,\n",
    "    filters={\n",
    "        'hospitalization_id': list(adult_hosp_ids),\n",
    "        'med_category': meds_of_interest\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c276d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hospitalizations on advanced mechanical support\n",
    "print(f\"\\nIdentifying hospitalizations with advanced respiratory support devices...\")\n",
    "vasoactive_meds = ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin','dopamine', 'angiotensin']\n",
    "clif.medication_admin_continuous.df= pd.merge(clif.medication_admin_continuous.df, encounter_mapping, \n",
    "                                        on='hospitalization_id', how='left')\n",
    "vasoactive_hosp_ids = clif.medication_admin_continuous.df.loc[\n",
    "    clif.medication_admin_continuous.df['med_category'].str.lower().isin([d.lower() for d in vasoactive_meds]),\n",
    "    'encounter_block'\n",
    "].unique()\n",
    "print(f\"Hospitalizations with any vasoactives. device ({', '.join(vasoactive_meds).upper()}): {len(vasoactive_hosp_ids):,}\")\n",
    "strobe_counts[\"3_vasoactive_hospitalizations\"] = len(vasoactive_hosp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e522d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with advanced_support_hosp_ids and 'high_support_en' == 1\n",
    "vasoactives_df = pd.DataFrame({\n",
    "    'encounter_block': vasoactive_hosp_ids,\n",
    "    'vaso_support_enc': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join vasoactives_df with final cohort on hospitalization_id\n",
    "final_cohort = final_cohort.merge(\n",
    "    vasoactives_df,\n",
    "    on='encounter_block',\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1473ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing high_support_en means not on advanced support\n",
    "final_cohort['vaso_support_enc'] = final_cohort['vaso_support_enc'].fillna(0).astype(int)\n",
    "# Missing high_support_en means not on advanced support\n",
    "final_cohort['high_support_enc'] = final_cohort['high_support_enc'].fillna(0).astype(int)\n",
    "# Missing icu_enc means not ICU\n",
    "final_cohort['icu_enc'] = final_cohort['icu_enc'].fillna(0).astype(int)\n",
    "# Define the criteria for other critically ill\n",
    "final_cohort['other_critically_ill'] = (\n",
    "    (final_cohort[['icu_enc', 'vaso_support_enc', 'high_support_enc']].sum(axis=1) == 0)\n",
    ").astype(int)\n",
    "# Calculate the count\n",
    "strobe_counts['4_other_critically_ill'] = final_cohort.loc[final_cohort['other_critically_ill'] == 1, \n",
    "                                                            'encounter_block'].nunique()\n",
    "strobe_counts['5_all_critically_ill'] = final_cohort['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec68a76",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa9305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "strobe_counts_df = pd.DataFrame(list(strobe_counts.items()), columns=['count_name', 'count_value'])\n",
    "strobe_counts_df.to_csv('../../output/final/tableone/strobe_counts.csv', index=False)\n",
    "# Calculate mortality rates\n",
    "mortality_rates = {\n",
    "    'ICU Hospitalizations': final_cohort.loc[final_cohort['icu_enc'] == 1, 'death_enc'].mean() * 100,\n",
    "    'Advanced Respiratory Support': final_cohort.loc[final_cohort['high_support_enc'] == 1, 'death_enc'].mean() * 100,\n",
    "    'Vasoactive Hospitalizations': final_cohort.loc[final_cohort['vaso_support_enc'] == 1, 'death_enc'].mean() * 100,\n",
    "    'Other Critically Ill': final_cohort.loc[final_cohort['other_critically_ill'] == 1, 'death_enc'].mean() * 100,\n",
    "    'All Critically Ill Adults': final_cohort['death_enc'].mean() * 100,\n",
    "}\n",
    "mortality_rates_df = pd.DataFrame(list(mortality_rates.items()), columns=['count_name', 'count_value'])\n",
    "mortality_rates_df.to_csv('../../output/final/tableone/mortality_rates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdaffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df = encounter_mapping.copy()\n",
    "cohort_df = cohort_df[cohort_df['encounter_block'].isin(final_cohort['encounter_block'])]\n",
    "\n",
    "# Merge cohort_df with all_encounters on hospitalization_id\n",
    "cohort_df = cohort_df.merge(\n",
    "    all_encounters,\n",
    "    on=['hospitalization_id', 'encounter_block'],\n",
    "    how='left',\n",
    "    suffixes=('', '_allenc')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "def create_consort_diagram(strobe_counts, mortality_rates):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "    ax.set_xlim(-1, 13)\n",
    "    ax.set_ylim(0, 14)\n",
    "    ax.axis('off')\n",
    "\n",
    "    box_style = \"round,pad=0.1\"\n",
    "    boxes = {}\n",
    "\n",
    "    def create_box(x, y, width, height, text, box_id=None, fontsize=10, fontweight='normal'):\n",
    "        box = FancyBboxPatch(\n",
    "            (x - width/2, y - height/2), width, height,\n",
    "            boxstyle=box_style, facecolor='white', edgecolor='black', linewidth=1.5\n",
    "        )\n",
    "        ax.add_patch(box)\n",
    "        ax.text(x, y, text, ha='center', va='center', fontsize=fontsize, fontweight=fontweight, wrap=True)\n",
    "        \n",
    "        return {\n",
    "            'x': x, 'y': y, 'width': width, 'height': height,\n",
    "            'left': x - width/2, 'right': x + width/2,\n",
    "            'top': y + height/2, 'bottom': y - height/2\n",
    "        }\n",
    "\n",
    "    def create_arrow(from_box, to_box):\n",
    "        x1, y1 = from_box['x'], from_box['bottom'] - 0.1\n",
    "        x2, y2 = to_box['x'], to_box['top'] + 0.1\n",
    "        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "\n",
    "    ax.text(5, 13, 'Cohort', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Define and arrange the boxes\n",
    "    box1 = create_box(5, 12, 3, 0.7, \n",
    "                      f\"Total Hospitalizations\\nn = {strobe_counts['0_total_hospitalizations']:,}\",\n",
    "                      'total', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box2 = create_box(5, 10.5, 3, 0.7,\n",
    "                      f\"Stitched Adult Hospitalizations\\nn = {strobe_counts['1b_after_stitching']:,}\",\n",
    "                      'adult', fontsize=11, fontweight='bold')\n",
    "    create_arrow(box1, box2)\n",
    "\n",
    "    # Define ICU, respiratory support, vasoactive, and other critically ill categories\n",
    "    box3_icu = create_box(1, 8, 3, 0.9,\n",
    "                          f\"ICU Hospitalizations\\nn = {strobe_counts['1_icu_encounters']:,}\\nMortality: {mortality_rates['ICU Hospitalizations']:.2f}%\",\n",
    "                          'icu', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box3_resp = create_box(4.5, 8, 3, 0.9,\n",
    "                           f\"Advanced Respiratory Support\\nn = {strobe_counts['2_advanced_resp_support_hospitalizations']:,}\\nMortality: {mortality_rates['Advanced Respiratory Support']:.2f}%\",\n",
    "                           'resp_support', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box3_vaso = create_box(8, 8, 3, 0.9,\n",
    "                           f\"Vasoactive Hospitalizations\\nn = {strobe_counts['3_vasoactive_hospitalizations']:,}\\nMortality: {mortality_rates['Vasoactive Hospitalizations']:.2f}%\",\n",
    "                           'vasoactive', fontsize=11, fontweight='bold')\n",
    "\n",
    "    box3_other = create_box(11.3, 8, 3, 0.9,\n",
    "                            f\"Other Critically Ill\\nn = {strobe_counts['4_other_critically_ill']:,}\\nMortality: {mortality_rates['Other Critically Ill']:.2f}%\",\n",
    "                            'other', fontsize=11, fontweight='bold')\n",
    "\n",
    "    create_arrow(box2, box3_icu)\n",
    "    create_arrow(box2, box3_resp)\n",
    "    create_arrow(box2, box3_vaso)\n",
    "    create_arrow(box2, box3_other)\n",
    "\n",
    "    # Add a final box for \"All Critically Ill Adults\"\n",
    "    box_final = create_box(5.7, 4.5, 5.2, 1.1,\n",
    "        f\"All Critically Ill Adults\\nn = {final_cohort['encounter_block'].nunique():,}\\nMortality: {mortality_rates['All Critically Ill Adults']:.2f}%\",\n",
    "        'all_critically_ill', fontsize=13, fontweight='bold')\n",
    "\n",
    "    # Do NOT draw arrows from the four groups to the all critically ill adults box\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../../output/final/tableone/consort_flow_diagram.png', dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf13e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from upsetplot import UpSet, from_indicators\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='upsetplot')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../../output/final/tableone', exist_ok=True)\n",
    "\n",
    "# Prepare final_cohort data for UpSet and venn plots\n",
    "summary_df = final_cohort[['encounter_block', 'icu_enc', 'death_enc', 'high_support_enc', 'vaso_support_enc']].drop_duplicates()\n",
    "\n",
    "# Rename columns\n",
    "summary_df = summary_df.rename(columns={\n",
    "    'icu_enc': 'ICU Hospitalizations',\n",
    "    'death_enc': 'Died',\n",
    "    'high_support_enc': 'Advanced O2 Support',\n",
    "    'vaso_support_enc': 'Vasoactive Support'\n",
    "})\n",
    "\n",
    "# Convert to boolean for UpSet and venn\n",
    "summary_df['ICU Hospitalizations'] = summary_df['ICU Hospitalizations'].fillna(0).astype(bool)\n",
    "summary_df['Died'] = summary_df['Died'].fillna(0).astype(bool)\n",
    "summary_df['Advanced O2 Support'] = summary_df['Advanced O2 Support'].fillna(0).astype(bool)\n",
    "summary_df['Vasoactive Support'] = summary_df['Vasoactive Support'].fillna(0).astype(bool)\n",
    "summary_df.to_csv('../../output/final/tableone/upset_data.csv', index=False)\n",
    "\n",
    "# ========== UpSet Plot ==========\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "upset_data = from_indicators(\n",
    "    ['ICU Hospitalizations', 'Died', 'Advanced O2 Support', 'Vasoactive Support'], \n",
    "    data=summary_df.set_index('encounter_block')\n",
    ")\n",
    "\n",
    "upset = UpSet(upset_data, \n",
    "              subset_size='count',\n",
    "              show_counts=True,\n",
    "              sort_by='cardinality',\n",
    "              element_size=50,\n",
    "              with_lines=True)\n",
    "\n",
    "upset.plot(fig=fig)\n",
    "\n",
    "plt.subplots_adjust(left=0.2, bottom=0.2, right=0.95, top=0.85, hspace=0.3, wspace=0.3)\n",
    "plt.suptitle('Clinical Cohort Intersections', fontsize=16, y=0.95)\n",
    "\n",
    "# Adjust font sizes for better readability\n",
    "for ax in fig.get_axes():\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                 ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "\n",
    "plt.savefig('../../output/final/tableone/cohort_intersect_upset_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ========== Venn Diagrams ==========\n",
    "\n",
    "# ========== 4-way Venn with venny4py ==========\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generating 4-way Venn Diagram\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create dictionary of sets for venny4py\n",
    "sets_dict = {\n",
    "    'ICU Hospitalizations': set(summary_df.loc[summary_df['ICU Hospitalizations'], 'encounter_block']),\n",
    "    'Died': set(summary_df.loc[summary_df['Died'], 'encounter_block']),\n",
    "    'Advanced O2 Support': set(summary_df.loc[summary_df['Advanced O2 Support'], 'encounter_block']),\n",
    "    'Vasoactive Support': set(summary_df.loc[summary_df['Vasoactive Support'], 'encounter_block'])\n",
    "}\n",
    "\n",
    "# Create 4-way Venn diagram\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "from venny4py.venny4py import venny4py\n",
    "venny4py(sets=sets_dict, dpi=300)\n",
    "plt.suptitle('4-way Venn Diagram', fontsize=16, y=0.98)\n",
    "plt.savefig('../../output/final/tableone/venn_all_4_groups.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✅ Saved: ../output/final/tableone/venn_all_4_groups.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ee2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the diagram\n",
    "create_consort_diagram(strobe_counts, mortality_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e6e15",
   "metadata": {},
   "source": [
    "# Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b69828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter patient table to cohort\n",
    "patient_df = clif.patient.df.copy()\n",
    "patient_df = patient_df[['patient_id', 'race_category', 'ethnicity_category',\n",
    "                         'sex_category', 'death_dttm']]\n",
    "\n",
    "# Filter patient_df to those in cohort\n",
    "patient_df = patient_df[patient_df['patient_id'].isin(cohort_df['patient_id'])]\n",
    "# Merge patient_df with cohort_df on patient_id\n",
    "cohort_df = patient_df.merge(cohort_df, on='patient_id', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6bbe8",
   "metadata": {},
   "source": [
    "# Final cohort df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a351afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tableone_df = cohort_df[['patient_id', 'hospitalization_id', 'encounter_block', 'admission_dttm', \n",
    "                                'discharge_dttm', 'age_at_admission', 'discharge_category', 'admission_type_category',\n",
    "                                'race_category', 'ethnicity_category', 'sex_category', 'death_dttm', \n",
    "                                'icu_enc', 'death_enc', 'cohort_enc']].drop_duplicates()\n",
    "\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    final_cohort[['encounter_block','high_support_enc', 'vaso_support_enc', 'other_critically_ill']],\n",
    "    on = 'encounter_block',\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# Merge hospital_id onto final_tableone_df by hospitalization_id\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    cohort_df[['hospitalization_id', 'hospital_id']].drop_duplicates(),\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "adt_cohort = cohort_df[['patient_id', 'hospitalization_id', 'encounter_block',\n",
    "                       'hospital_id', 'in_dttm', 'out_dttm', 'location_category',\n",
    "                       'location_type']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hosp_ids = final_tableone_df['hospitalization_id'].unique().tolist()\n",
    "final_patient_ids = final_tableone_df['patient_id'].unique().tolist()\n",
    "final_enc_blocks = final_tableone_df['encounter_block'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56264d9",
   "metadata": {},
   "source": [
    "# Cohort Group Sankey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = create_outcome_df(final_tableone_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING MATPLOTLIB SANKEY DIAGRAMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example 1: ICU encounters\n",
    "# icu_blocks = final_tableone_df[final_tableone_df['icu_enc'] == 1]['encounter_block'].sample(\n",
    "#     min(300, final_tableone_df[final_tableone_df['icu_enc'] == 1].shape[0]), \n",
    "#     random_state=42\n",
    "# ).tolist()\n",
    "# fig1, df1 = create_sankey_diagram(\n",
    "#     adt_cohort=adt_cohort,\n",
    "#     encounter_blocks=icu_blocks,\n",
    "#     outcome_df=outcome_df,\n",
    "#     max_locations=7,\n",
    "#     title=\"ICU Patient Flow and Outcomes (Sample 300 encounters)\",\n",
    "#     output_file=f\"{output_dir}/figures/sankey_matplotlib_icu.png\",\n",
    "#     figsize=(16, 8)\n",
    "# )\n",
    "# plt.show()\n",
    "icu_blocks_all = final_tableone_df[final_tableone_df['icu_enc'] == 1]['encounter_block'].tolist()\n",
    "fig1, df1 = create_sankey_diagram(\n",
    "    adt_cohort=adt_cohort,\n",
    "    encounter_blocks=icu_blocks_all,\n",
    "    outcome_df=outcome_df,\n",
    "    max_locations=7,\n",
    "    title=\"ICU Patient Flow and Outcomes\",\n",
    "    output_file=f\"{output_dir}/figures/sankey_matplotlib_icu.png\",\n",
    "    figsize=(16, 8)\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other critically ill\n",
    "other_enc = final_tableone_df[final_tableone_df['other_critically_ill'] == 1]['encounter_block'].tolist()\n",
    "\n",
    "print(f\"\\n Other critically ill  (N={len(other_enc)})\")\n",
    "fig1, df1 = create_sankey_diagram(\n",
    "    adt_cohort=adt_cohort,\n",
    "    encounter_blocks=other_enc,\n",
    "    outcome_df=outcome_df,\n",
    "    max_locations=8,\n",
    "    title=\"Other critically ill patients Patient Flow and Outcomes\",\n",
    "    output_file=f\"{output_dir}/figures/sankey_matplotlib_others.png\",\n",
    "    figsize=(16, 8)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a1ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other critically ill\n",
    "adv_o2_sup = final_tableone_df[final_tableone_df['high_support_enc'] == 1]['encounter_block'].tolist()\n",
    "fig1, df1 = create_sankey_diagram(\n",
    "    adt_cohort=adt_cohort,\n",
    "    encounter_blocks=adv_o2_sup,\n",
    "    outcome_df=outcome_df,\n",
    "    max_locations=8,\n",
    "    title=\"Patients receiving advanced o2 support\",\n",
    "    output_file=f\"{output_dir}/figures/sankey_matplotlib_high_o2_support.png\",\n",
    "    figsize=(16, 8)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other critically ill\n",
    "vaso_sup = final_tableone_df[final_tableone_df['vaso_support_enc'] == 1]['encounter_block'].tolist()\n",
    "fig1, df1 = create_sankey_diagram(\n",
    "    adt_cohort=adt_cohort,\n",
    "    encounter_blocks=vaso_sup,\n",
    "    outcome_df=outcome_df,\n",
    "    max_locations=8,\n",
    "    title=\"Patients receiving Vasoactives\",\n",
    "    output_file=f\"{output_dir}/figures/sankey_matplotlib_vaso_support.png\",\n",
    "    figsize=(16, 8)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73310bee",
   "metadata": {},
   "source": [
    "# Hospital and ICU Admission Summary\n",
    "\n",
    "1. Get the first ICU dttm for ICU encounters \n",
    "2. Calculate ICU LOS and Hospital LOS for each encounter in days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974cdb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_admission_summary = (\n",
    "        adt_cohort\n",
    "        .groupby('encounter_block')\n",
    "        .agg(\n",
    "            min_in_dttm = ('in_dttm', 'min'),\n",
    "            max_out_dttm = ('out_dttm', 'max'),\n",
    "            first_admission_location = ('location_category', 'first')\n",
    "        )\n",
    ")\n",
    "hosp_admission_summary['hospital_length_of_stay_days'] = (\n",
    "    (hosp_admission_summary['max_out_dttm'] - hosp_admission_summary['min_in_dttm']) / pd.Timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the column, not the entire df\n",
    "adt_cohort['location_category'] = (\n",
    "    adt_cohort['location_category']\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# restrict to ICU rows\n",
    "icu_df = adt_cohort.query('location_category == \"icu\"')\n",
    "\n",
    "# find first ICU in time per 'encounter_block'\n",
    "first_in = (\n",
    "    icu_df\n",
    "     .groupby('encounter_block', as_index=False)\n",
    "     .agg(first_icu_in_dttm=('in_dttm', 'min'))\n",
    ")\n",
    "\n",
    "# join back to pull the matching out_dttm\n",
    "icu_summary = (\n",
    "    first_in\n",
    "      # bring in that one row’s out_dttm\n",
    "      .merge(\n",
    "          icu_df[['hospitalization_id','in_dttm','out_dttm', 'encounter_block']],\n",
    "          left_on=['encounter_block', 'first_icu_in_dttm'],\n",
    "          right_on=['encounter_block', 'in_dttm'],\n",
    "          how='left'\n",
    "      )\n",
    "      .rename(columns={'out_dttm':'first_icu_out_dttm'})\n",
    ")\n",
    "\n",
    "# compute LOS in days (out - in)\n",
    "icu_summary['first_icu_los_days'] = (\n",
    "    (icu_summary['first_icu_out_dttm'] - icu_summary['first_icu_in_dttm'])\n",
    "    .dt.total_seconds()\n",
    "    / (3600 * 24)\n",
    ")\n",
    "\n",
    "# trim to just the columns you need\n",
    "icu_summary = icu_summary[['encounter_block', 'first_icu_in_dttm',\n",
    "                           'first_icu_out_dttm','first_icu_los_days']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all_ids with icu_summary and hosp_admission_summary\n",
    "final_tableone_df = (\n",
    "    final_tableone_df\n",
    "    .merge(icu_summary, on='encounter_block', how='left')\n",
    "    .merge(hosp_admission_summary, on='encounter_block', how='left')\n",
    ")\n",
    "final_tableone_df['first_admission_location'] = final_tableone_df['first_admission_location'].fillna('Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c36e54",
   "metadata": {},
   "source": [
    "# Code Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Load Code Status\n",
    "# ----------------------------------------------------------------------------\n",
    "print(f\"\\nLoading code_status table...\")\n",
    "clif.load_table(\n",
    "    'code_status'\n",
    ")\n",
    "\n",
    "get_value_counts(clif.code_status, ['code_status_name', 'code_status_category'], output_dir=mcide_dir)\n",
    "print(f\"   code_status loaded: {len(clif.code_status.df):,} rows\")\n",
    "print(f\"   Unique code_status categories: {clif.code_status.df['code_status_category'].nunique()}\")\n",
    "print(f\"   Unique code_status patients: {clif.code_status.df['patient_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c39cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the last code_status_category for each patient_id\n",
    "code_status_latest = (\n",
    "    clif.code_status.df.sort_values(['patient_id', 'start_dttm'])\n",
    "    .groupby('patient_id', as_index=False)\n",
    "    .last()[['patient_id', 'code_status_category']]\n",
    "    .rename(columns={'code_status_category': 'last_code_status_category'})\n",
    ")\n",
    "\n",
    "# Merge with final_tableone_df on patient_id\n",
    "final_tableone_df = final_tableone_df.merge(code_status_latest, on='patient_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Prepare Aggregated Data for Code Status Visualizations\n",
    "# ============================================================================\n",
    "\n",
    "encounter_flags = ['icu_enc', 'high_support_enc', 'vaso_support_enc', 'other_critically_ill']\n",
    "flag_labels = ['ICU Encounters', 'Advanced Respiratory Support', 'Vasoactive Support', 'Other Critically Ill']\n",
    "\n",
    "# Initialize containers\n",
    "code_status_counts = {}\n",
    "code_status_percentages = {}\n",
    "missingness_info = {}\n",
    "\n",
    "# Collect aggregated data for each encounter type\n",
    "for flag, label in zip(encounter_flags, flag_labels):\n",
    "    subset = final_tableone_df[final_tableone_df[flag] == 1]\n",
    "    \n",
    "    # Total encounters for this type\n",
    "    total_encounters = len(subset)\n",
    "    \n",
    "    # Count missing values\n",
    "    n_missing = subset['last_code_status_category'].isna().sum()\n",
    "    \n",
    "    # Get value counts (including handling of NaN)\n",
    "    counts = subset['last_code_status_category'].value_counts(dropna=False)\n",
    "    \n",
    "    # Store counts\n",
    "    code_status_counts[label] = counts\n",
    "    \n",
    "    # Calculate percentages\n",
    "    percentages = (counts / total_encounters * 100).round(2)\n",
    "    code_status_percentages[label] = percentages\n",
    "    \n",
    "    # Store missingness information\n",
    "    missingness_info[label] = {\n",
    "        'total_encounters': total_encounters,\n",
    "        'n_missing': n_missing,\n",
    "        'pct_missing': round(n_missing / total_encounters * 100, 2) if total_encounters > 0 else 0\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# Create DataFrames for Export\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Counts DataFrame\n",
    "df_counts = pd.DataFrame(code_status_counts).fillna(0).astype(int)\n",
    "df_counts.index.name = 'code_status_category'\n",
    "\n",
    "# Handle NaN index (if exists)\n",
    "if df_counts.index.isna().any():\n",
    "    df_counts.index = df_counts.index.fillna('Missing')\n",
    "\n",
    "# 2. Percentages DataFrame\n",
    "df_percentages = pd.DataFrame(code_status_percentages).fillna(0)\n",
    "df_percentages.index.name = 'code_status_category'\n",
    "\n",
    "if df_percentages.index.isna().any():\n",
    "    df_percentages.index = df_percentages.index.fillna('Missing')\n",
    "\n",
    "# 3. Missingness Summary DataFrame\n",
    "df_missingness = pd.DataFrame(missingness_info).T\n",
    "df_missingness.index.name = 'encounter_type'\n",
    "\n",
    "# ============================================================================\n",
    "# Add Summary Statistics\n",
    "# ============================================================================\n",
    "\n",
    "# Add row totals to counts\n",
    "df_counts['Total'] = df_counts.sum(axis=1)\n",
    "\n",
    "# Add column totals to counts\n",
    "df_counts.loc['Total'] = df_counts.sum(axis=0)\n",
    "\n",
    "# Add summary to percentages (column sums should be ~100%)\n",
    "df_percentages.loc['Total'] = df_percentages.sum(axis=0)\n",
    "\n",
    "# ============================================================================\n",
    "# Save to CSV Files\n",
    "# ============================================================================\n",
    "\n",
    "output_dir = '../output/final/tableone/'\n",
    "\n",
    "# Save counts\n",
    "df_counts.to_csv(f'{output_dir}code_status_counts_by_encounter_type.csv')\n",
    "print(f\"✅ Saved: {output_dir}code_status_counts_by_encounter_type.csv\")\n",
    "\n",
    "# Save percentages\n",
    "df_percentages.to_csv(f'{output_dir}code_status_percentages_by_encounter_type.csv')\n",
    "print(f\"✅ Saved: {output_dir}code_status_percentages_by_encounter_type.csv\")\n",
    "\n",
    "# Save missingness summary\n",
    "df_missingness.to_csv(f'{output_dir}code_status_missingness_summary.csv')\n",
    "print(f\"✅ Saved: {output_dir}code_status_missingness_summary.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create Combined Summary File (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "# Create a comprehensive summary with counts and percentages\n",
    "combined_summary = []\n",
    "\n",
    "for col in df_counts.columns[:-1]:  # Exclude 'Total' column\n",
    "    for idx in df_counts.index[:-1]:  # Exclude 'Total' row\n",
    "        count = df_counts.loc[idx, col]\n",
    "        pct = df_percentages.loc[idx, col]\n",
    "        combined_summary.append({\n",
    "            'encounter_type': col,\n",
    "            'code_status': idx,\n",
    "            'count': count,\n",
    "            'percentage': pct\n",
    "        })\n",
    "\n",
    "df_combined = pd.DataFrame(combined_summary)\n",
    "df_combined.to_csv(f'{output_dir}code_status_combined_summary.csv', index=False)\n",
    "print(f\"✅ Saved: {output_dir}code_status_combined_summary.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVED AGGREGATED DATA (NO PATIENT-LEVEL INFORMATION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"1. {output_dir}code_status_counts_by_encounter_type.csv\")\n",
    "print(f\"2. {output_dir}code_status_percentages_by_encounter_type.csv\")\n",
    "print(f\"3. {output_dir}code_status_missingness_summary.csv\")\n",
    "print(f\"4. {output_dir}code_status_combined_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load the saved aggregated data (can be done in a separate session)\n",
    "# ============================================================================\n",
    "\n",
    "# Remove 'Total' rows/columns for visualization, but only if 'Total' exists\n",
    "def drop_total_and_missing(axis_df):\n",
    "    # Remove 'Total' and 'Missing' rows/cols if they exist\n",
    "    for axis in [0, 1]:\n",
    "        if 'Total' in axis_df.axes[axis]:\n",
    "            axis_df = axis_df.drop('Total', axis=axis)\n",
    "        if 'Missing' in axis_df.axes[axis]:\n",
    "            axis_df = axis_df.drop('Missing', axis=axis)\n",
    "    return axis_df\n",
    "\n",
    "def drop_total_keep_missing(axis_df):\n",
    "    # Remove only 'Total' rows/cols, but keep 'Missing'\n",
    "    for axis in [0, 1]:\n",
    "        if 'Total' in axis_df.axes[axis]:\n",
    "            axis_df = axis_df.drop('Total', axis=axis)\n",
    "    return axis_df\n",
    "\n",
    "# For counts, we'll keep the \"Missing\" row if present for plotting, for percentages we'll recompute without it\n",
    "df_counts_viz = drop_total_keep_missing(df_counts)\n",
    "\n",
    "# Recalculate percentages excluding 'Missing' category from denominator\n",
    "def recalc_percentages_exclude_missing(df_counts):\n",
    "    # Only keep rows that are not 'Total' (already handled), and not 'Missing'\n",
    "    code_status_rows = [row for row in df_counts.index if row != 'Missing']\n",
    "    # For each column, divide counts by sum excluding 'Missing'\n",
    "    df_pct = df_counts.loc[code_status_rows].div(df_counts.loc[code_status_rows].sum(axis=0), axis=1) * 100\n",
    "    return df_pct\n",
    "\n",
    "df_pct_viz = recalc_percentages_exclude_missing(df_counts_viz)\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION 1: Stacked Bar Chart with Missingness Indicator\n",
    "# ============================================================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Define colors (use different color for Missing)\n",
    "status_categories = [row for row in df_counts_viz.index if row != 'Missing']\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "# If 'Missing' exists, assign it a distinct color at the end for plotting counts\n",
    "if 'Missing' in df_counts_viz.index:\n",
    "    status_categories_full = status_categories + ['Missing']\n",
    "    colors_full = colors + ['#808080']  # Gray for missing\n",
    "else:\n",
    "    status_categories_full = status_categories\n",
    "    colors_full = colors\n",
    "\n",
    "# Plot 1: Absolute counts (stacked bars including Missing if present)\n",
    "df_counts_viz.T[status_categories_full].plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    ax=ax1,\n",
    "    color=colors_full[:len(status_categories_full)],\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax1.set_title('Code Status Distribution by Encounter Type\\n(Absolute Counts)', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.set_xlabel('Encounter Type', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax1.legend(title='Code Status', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add missingness annotations\n",
    "for i, col in enumerate(df_counts_viz.columns):\n",
    "    if col in df_missingness.index:\n",
    "        miss_pct = df_missingness.loc[col, 'pct_missing']\n",
    "        if miss_pct > 0:\n",
    "            ax1.text(\n",
    "                i, ax1.get_ylim()[1] * 0.95, f'{miss_pct:.1f}% missing',\n",
    "                ha='center', va='top', fontsize=9, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "            )\n",
    "\n",
    "# Plot 2: Percentages (excluding Missing from numerator/denominator)\n",
    "df_pct_viz.T[status_categories].plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    ax=ax2,\n",
    "    color=colors[:len(status_categories)],\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax2.set_title('Code Status Distribution by Encounter Type\\n(Percentages, Excl. Missing)', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax2.set_xlabel('Encounter Type', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Percentage', fontsize=12, fontweight='bold')\n",
    "ax2.legend(title='Code Status', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}code_status_stacked_bar_with_missingness_excl_missing_cat.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc45679",
   "metadata": {},
   "source": [
    "# IMV encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only IMV hospitalizations\n",
    "clif.respiratory_support.df = clif.respiratory_support.df[\n",
    "    clif.respiratory_support.df['hospitalization_id'].isin(final_hosp_ids)\n",
    "].copy()\n",
    "print(f\"Respiratory support rows (IMV hospitalizations): {len(clif.respiratory_support.df):,}\")\n",
    "clif.respiratory_support.df = clif.respiratory_support.df.sort_values(['hospitalization_id', 'recorded_dttm'])\n",
    "\n",
    "# Standardize category columns to lowercase\n",
    "print(f\"\\nStandardizing category columns...\")\n",
    "category_cols = [col for col in clif.respiratory_support.df.columns if col.endswith('_category')]\n",
    "for col in category_cols:\n",
    "    clif.respiratory_support.df[col] = clif.respiratory_support.df[col].str.lower()\n",
    "\n",
    "from clifpy.utils import apply_outlier_handling\n",
    "apply_outlier_handling(clif.respiratory_support)\n",
    "\n",
    "# Merge with encounter_block mapping\n",
    "resp_stitched = clif.respiratory_support.df\n",
    "\n",
    "#  Identify IMV rows\n",
    "imv_mask = resp_stitched['device_category'].str.contains(\"imv\", case=False, na=False)\n",
    "resp_stitched_imv = resp_stitched[imv_mask].copy()\n",
    "\n",
    "# Create on_vent column for IMV records\n",
    "resp_stitched_imv['on_vent'] = 1\n",
    "\n",
    "# Get unique encounter IDs from resp_stitched_imv\n",
    "imv_encounters = resp_stitched_imv['encounter_block'].unique()\n",
    "\n",
    "print(f\"Number of IMV encounters: {len(imv_encounters):,}\")\n",
    "strobe_counts[\"IMV encounters\"] = len(imv_encounters)\n",
    "# Determine Vent Start/End for Each Encounter\n",
    "vent_start_end = resp_stitched_imv.groupby('encounter_block').agg(\n",
    "    vent_start_time=('recorded_dttm', 'min'),\n",
    "    vent_end_time=('recorded_dttm', 'max')\n",
    ").reset_index()\n",
    "\n",
    "#  Add on_vent flag to final_cohort\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    resp_stitched_imv[['encounter_block', 'on_vent']].drop_duplicates(),\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "final_tableone_df['on_vent'] = final_tableone_df['on_vent'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply waterfall processing to fill sparse data\n",
    "# print(f\"\\nApplying waterfall processing to respiratory support data. This can take 2-20 mins based on your system specs and cohort size...\")\n",
    "# clif.respiratory_support = clif.respiratory_support.waterfall(verbose=True)\n",
    "# print(f\"\\n Waterfall complete: {len(clif.respiratory_support.df):,} rows for {clif.respiratory_support.df['hospitalization_id'].nunique():,} unique hospitalizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ced3b",
   "metadata": {},
   "source": [
    "## Initial Mode category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a710cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get Initial Mode Category (first mode after vent start)\n",
    "# Subset resp_stitched to only those encounters on IMV\n",
    "resp_imv = resp_stitched[resp_stitched['encounter_block'].isin(imv_encounters)].copy()\n",
    "\n",
    "# Merge in the vent_start_time\n",
    "resp_imv = resp_imv.merge(\n",
    "    vent_start_end[['encounter_block', 'vent_start_time']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Filter to only rows at or after vent start\n",
    "resp_post_start = resp_imv[\n",
    "    resp_imv['recorded_dttm'] >= resp_imv['vent_start_time']\n",
    "]\n",
    "\n",
    "# Group and take first non-NA mode_category per encounter\n",
    "initial_modes = (\n",
    "    resp_post_start\n",
    "    .sort_values(['encounter_block', 'recorded_dttm'])\n",
    "    .groupby('encounter_block', as_index=False)['mode_category']\n",
    "    .first()\n",
    "    .rename(columns={'mode_category': 'initial_mode_category'})\n",
    ")\n",
    "\n",
    "# Fill any entirely-missing groups with \"Missing\"\n",
    "initial_modes['initial_mode_category'] = initial_modes['initial_mode_category'].fillna('Missing')\n",
    "\n",
    "# Merge back onto final_cohort\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    initial_modes,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# If some encounters never went on vent, fill those too\n",
    "final_tableone_df['initial_mode_category'] = final_tableone_df['initial_mode_category'].fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9857f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calculate Ventilator Settings Statistics (Median and IQR)\n",
    "# Filter resp_stitched to only those encounters on IMV\n",
    "# resp_stitched_final = resp_stitched[resp_stitched['encounter_block'].isin(imv_encounters)]\n",
    "\n",
    "# # Define numeric columns to aggregate\n",
    "# numeric_cols = [\n",
    "#     'fio2_set', 'lpm_set', 'resp_rate_set', 'peep_set',\n",
    "#     'tidal_volume_set', 'pressure_control_set', 'pressure_support_set'\n",
    "# ]\n",
    "\n",
    "# # Build named aggregation dict\n",
    "# named_aggs = {}\n",
    "# for col in numeric_cols:\n",
    "#     named_aggs[f'{col}_median'] = (col, 'median')\n",
    "#     named_aggs[f'{col}_q1'] = (col, lambda x: x.quantile(0.25))\n",
    "#     named_aggs[f'{col}_q3'] = (col, lambda x: x.quantile(0.75))\n",
    "\n",
    "# # Aggregate ventilator settings\n",
    "# vent_stats = (\n",
    "#     resp_stitched_final\n",
    "#     .groupby('encounter_block', as_index=False)\n",
    "#     .agg(**named_aggs)\n",
    "# )\n",
    "\n",
    "# # Merge vent stats back to final_cohort\n",
    "# final_tableone_df = final_tableone_df.merge(vent_stats, on='encounter_block', how='left')\n",
    "\n",
    "#  Find First Location at IMV Start (closest ADT location to vent start)\n",
    "# Get minimal ADT cohort with required columns and merge with encounter_block\n",
    "print(\"Find First Location at IMV Start (closest ADT location to vent start)\")\n",
    "\n",
    "# Merge with vent start times\n",
    "adt_vent = pd.merge(\n",
    "    vent_start_end[['encounter_block', 'vent_start_time']],\n",
    "    adt_cohort,\n",
    "    on='encounter_block'\n",
    ")\n",
    "\n",
    "# Calculate time difference between vent start and ADT in_dttm\n",
    "adt_vent['time_diff'] = abs(adt_vent['vent_start_time'] - adt_vent['in_dttm'])\n",
    "\n",
    "# Get the closest ADT row for each encounter block\n",
    "closest_adt = (\n",
    "    adt_vent\n",
    "    .sort_values('time_diff')\n",
    "    .groupby('encounter_block')\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "closest_adt = closest_adt.rename(columns={'location_category': 'first_location_imv'})\n",
    "\n",
    "# Merge back to final_cohort\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    closest_adt[['encounter_block', 'first_location_imv']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\n=== IMV Encounter Summary Complete ===\")\n",
    "print(f\"Total encounters: {len(final_tableone_df):,}\")\n",
    "print(f\"Encounters on IMV: {final_tableone_df['on_vent'].sum():,}\")\n",
    "print(f\"Initial mode categories:\\n{final_tableone_df['initial_mode_category'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92944d7",
   "metadata": {},
   "source": [
    "## IMV- First 24 hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. Add vent_start_dttm to final_tableone_df\n",
    "# ============================================================================\n",
    "\n",
    "# Merge vent_start_time from vent_start_end into final_tableone_df\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    vent_start_end[['encounter_block', 'vent_start_time']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename to vent_start_dttm for clarity\n",
    "final_tableone_df = final_tableone_df.rename(columns={'vent_start_time': 'vent_start_dttm'})\n",
    "\n",
    "print(f\"\\n✅ Added vent_start_dttm to final_tableone_df\")\n",
    "print(f\"   Encounters with vent_start_dttm: {final_tableone_df['vent_start_dttm'].notna().sum():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Prepare IMV data with time from vent start\n",
    "# ============================================================================\n",
    "\n",
    "# Get IMV encounters\n",
    "resp_imv = resp_stitched[resp_stitched['encounter_block'].isin(imv_encounters)].copy()\n",
    "\n",
    "# Merge vent_start_dttm\n",
    "resp_imv = resp_imv.merge(\n",
    "    final_tableone_df[['encounter_block', 'vent_start_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate hours from vent start (time 0)\n",
    "resp_imv['hours_from_vent_start'] = (\n",
    "    (resp_imv['recorded_dttm'] - resp_imv['vent_start_dttm']).dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# Filter to only records at or after vent start\n",
    "resp_imv_post_start = resp_imv[resp_imv['hours_from_vent_start'] >= 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tableone_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUPER OPTIMIZED: Calculate Statistics (10-20x faster!)\n",
    "# ============================================================================\n",
    "# Define all ventilator settings\n",
    "vent_settings = [\n",
    "    'fio2_set', 'lpm_set', 'tidal_volume_set', 'resp_rate_set',\n",
    "    'pressure_control_set', 'pressure_support_set', 'peep_set'\n",
    "]\n",
    "\n",
    "# Check which columns exist\n",
    "existing_settings = [col for col in vent_settings if col in resp_imv.columns]\n",
    "\n",
    "print(f\"\\nSettings to calculate: {len(existing_settings)}, vent settings\")\n",
    "\n",
    "# ✅ SUPER OPTIMIZATION: Calculate each stat separately (pandas can optimize better)\n",
    "results = []\n",
    "\n",
    "# Median and mean/std (fast built-ins)\n",
    "medians = resp_imv.groupby('encounter_block')[existing_settings].median()\n",
    "medians.columns = [f'{col}_median' for col in medians.columns]\n",
    "\n",
    "means = resp_imv.groupby('encounter_block')[existing_settings].mean()\n",
    "means.columns = [f'{col}_mean' for col in means.columns]\n",
    "\n",
    "stds = resp_imv.groupby('encounter_block')[existing_settings].std()\n",
    "stds.columns = [f'{col}_std' for col in stds.columns]\n",
    "\n",
    "# Quantiles (still need to use slower method, but only once each)\n",
    "q1 = resp_imv.groupby('encounter_block')[existing_settings].quantile(0.25)\n",
    "q1.columns = [f'{col}_q1' for col in q1.columns]\n",
    "\n",
    "q3 = resp_imv.groupby('encounter_block')[existing_settings].quantile(0.75)\n",
    "q3.columns = [f'{col}_q3' for col in q3.columns]\n",
    "\n",
    "# Combine all statistics\n",
    "vent_settings_stats = pd.concat([medians, q1, q3, means, stds], axis=1).reset_index()\n",
    "\n",
    "print(f\"✅ Calculated statistics for {len(existing_settings)} settings\")\n",
    "print(f\"   Total encounters: {len(vent_settings_stats):,}\")\n",
    "\n",
    "# Merge back\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    vent_settings_stats,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40062eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. Filter for specific mode categories\n",
    "# ============================================================================\n",
    "\n",
    "# Define mode categories of interest\n",
    "volume_control_modes = ['assist control-volume control', 'pressure-regulated volume control']\n",
    "pressure_control_mode = ['pressure control']\n",
    "\n",
    "# Filter data\n",
    "volume_mode_data = resp_imv_post_start[\n",
    "    resp_imv_post_start['mode_category'].isin(volume_control_modes)\n",
    "].copy()\n",
    "\n",
    "pressure_mode_data = resp_imv_post_start[\n",
    "    resp_imv_post_start['mode_category'].isin(pressure_control_mode)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\n📊 Mode Category Breakdown:\")\n",
    "print(f\"   Volume Control modes: {len(volume_mode_data):,} records\")\n",
    "for mode in volume_control_modes:\n",
    "    count = (volume_mode_data['mode_category'] == mode).sum()\n",
    "    print(f\"      - {mode}: {count:,}\")\n",
    "print(f\"   Pressure Control mode: {len(pressure_mode_data):,} records\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Plot Median/IQR Tidal Volume for Volume Control Modes\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate median and IQR tidal volume by hour for volume modes\n",
    "volume_mode_data['hour_bin'] = volume_mode_data['hours_from_vent_start'].round(0).astype(int)\n",
    "\n",
    "# Filter to first 168 hours (7 days)\n",
    "volume_mode_data_7d = volume_mode_data[volume_mode_data['hour_bin'] <= 168].copy()\n",
    "\n",
    "# Group by hour and calculate stats\n",
    "tv_stats = volume_mode_data_7d.groupby('hour_bin')['tidal_volume_set'].agg([\n",
    "    ('median', 'median'),\n",
    "    ('q25', lambda x: x.quantile(0.25)),\n",
    "    ('q75', lambda x: x.quantile(0.75)),\n",
    "    ('count', 'count')\n",
    "]).reset_index()\n",
    "\n",
    "# Filter hours with at least 10 measurements\n",
    "tv_stats = tv_stats[tv_stats['count'] >= 10]\n",
    "\n",
    "# Save CSV for Tidal Volume data\n",
    "tv_csv_path = '../output/final/tableone/tidal_volume_volume_control_modes.csv'\n",
    "tv_stats.to_csv(tv_csv_path, index=False)\n",
    "print(f\"✅ Saved CSV: {tv_csv_path}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(tv_stats['hour_bin'], tv_stats['median'], 'o-', color='#2E86AB', linewidth=2, markersize=4, label='Median')\n",
    "ax.fill_between(tv_stats['hour_bin'], tv_stats['q25'], tv_stats['q75'], \n",
    "                alpha=0.3, color='#2E86AB', label='IQR (25th-75th percentile)')\n",
    "\n",
    "ax.set_xlabel('Hours from Ventilation Start', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Tidal Volume Set (mL)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Tidal Volume Over Time: Volume Control Modes\\n(Assist Control-Volume Control & Pressure-Regulated Volume Control)', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 168)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/tableone/tidal_volume_volume_control_modes.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\n✅ Saved: ../output/final/tableone/tidal_volume_volume_control_modes.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Plot Median/IQR Pressure Control for Pressure Control Mode\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate stats for pressure control\n",
    "pressure_mode_data['hour_bin'] = pressure_mode_data['hours_from_vent_start'].round(0).astype(int)\n",
    "pressure_mode_data_7d = pressure_mode_data[pressure_mode_data['hour_bin'] <= 168].copy()\n",
    "\n",
    "pc_stats = pressure_mode_data_7d.groupby('hour_bin')['pressure_control_set'].agg([\n",
    "    ('median', 'median'),\n",
    "    ('q25', lambda x: x.quantile(0.25)),\n",
    "    ('q75', lambda x: x.quantile(0.75)),\n",
    "    ('count', 'count')\n",
    "]).reset_index()\n",
    "\n",
    "pc_stats = pc_stats[pc_stats['count'] >= 10]\n",
    "\n",
    "# Save CSV for Pressure Control data\n",
    "pc_csv_path = '../output/final/tableone/pressure_control_pressure_control_mode.csv'\n",
    "pc_stats.to_csv(pc_csv_path, index=False)\n",
    "print(f\"✅ Saved CSV: {pc_csv_path}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(pc_stats['hour_bin'], pc_stats['median'], 'o-', color='#A23B72', linewidth=2, markersize=4, label='Median')\n",
    "ax.fill_between(pc_stats['hour_bin'], pc_stats['q25'], pc_stats['q75'], \n",
    "                alpha=0.3, color='#A23B72', label='IQR (25th-75th percentile)')\n",
    "\n",
    "ax.set_xlabel('Hours from Ventilation Start', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Pressure Control Set (cmH₂O)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Pressure Control Over Time: Pressure Control Mode', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 168)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/tableone/pressure_control_pressure_control_mode.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✅ Saved: ../output/final/tableone/pressure_control_pressure_control_mode.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828538f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIMIZED: Ventilator Settings Table by Device and Mode Category - FIXED\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Ventilator settings of interest\n",
    "vent_settings = [\n",
    "    'fio2_set',\n",
    "    'tidal_volume_set',\n",
    "    'resp_rate_set',\n",
    "    'pressure_control_set',\n",
    "    'peep_set',\n",
    "    'pressure_support_set'\n",
    "]\n",
    "\n",
    "# ✅ OPTIMIZATION: Use groupby instead of nested loops (10-50x faster!)\n",
    "# Filter to combinations with at least 10 observations\n",
    "group_counts = resp_imv_post_start.groupby(['device_category', 'mode_category']).size()\n",
    "valid_groups = group_counts[group_counts >= 10].index\n",
    "\n",
    "# Filter data to only valid groups\n",
    "resp_valid = resp_imv_post_start[\n",
    "    resp_imv_post_start.set_index(['device_category', 'mode_category']).index.isin(valid_groups)\n",
    "].copy()\n",
    "\n",
    "print(f\"Calculating statistics for {len(valid_groups)} device-mode combinations...\")\n",
    "\n",
    "# Calculate median, Q1, Q3\n",
    "medians = resp_valid.groupby(['device_category', 'mode_category'])[vent_settings].median()\n",
    "q1 = resp_valid.groupby(['device_category', 'mode_category'])[vent_settings].quantile(0.25)\n",
    "q3 = resp_valid.groupby(['device_category', 'mode_category'])[vent_settings].quantile(0.75)\n",
    "\n",
    "# ✅ FIX: Reset index for all before combining\n",
    "medians_reset = medians.reset_index()\n",
    "q1_reset = q1.reset_index()\n",
    "q3_reset = q3.reset_index()\n",
    "\n",
    "# Create settings summary starting with device and mode columns\n",
    "settings_summary = medians_reset[['device_category', 'mode_category']].copy()\n",
    "\n",
    "# ✅ Format as \"median (q1-q3)\" using .values (no index issues)\n",
    "for setting in vent_settings:\n",
    "    if setting in medians.columns:\n",
    "        # Use .values to avoid index alignment issues\n",
    "        settings_summary[setting] = (\n",
    "            medians_reset[setting].round(1).astype(str) + ' (' +\n",
    "            q1_reset[setting].round(1).astype(str) + '-' +\n",
    "            q3_reset[setting].round(1).astype(str) + ')'\n",
    "        )\n",
    "\n",
    "# Rename columns\n",
    "settings_summary = settings_summary.rename(columns={\n",
    "    'mode_category': 'ventilator_setting',\n",
    "    'fio2_set': 'FiO2 Set',\n",
    "    'tidal_volume_set': 'Tidal Volume Set',\n",
    "    'resp_rate_set': 'Resp Rate Set',\n",
    "    'pressure_control_set': 'Pressure Control Set',\n",
    "    'peep_set': 'PEEP Set',\n",
    "    'pressure_support_set': 'Pressure Support Set'\n",
    "})\n",
    "\n",
    "# Sort by device and mode\n",
    "settings_summary = settings_summary.sort_values(['device_category', 'ventilator_setting'])\n",
    "\n",
    "# Display and save\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VENTILATOR SETTINGS BY DEVICE AND MODE CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "print(settings_summary.to_string(index=False))\n",
    "\n",
    "settings_summary.to_csv('../output/final/tableone/ventilator_settings_by_device_mode.csv', index=False)\n",
    "print(f\"\\n✅ Saved: ../output/final/tableone/ventilator_settings_by_device_mode.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# BONUS: Also create counts table (same optimization)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Observation Counts Table\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count non-null observations for each setting (vectorized)\n",
    "counts_summary = resp_valid.groupby(['device_category', 'mode_category'])[vent_settings].count().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "counts_summary = counts_summary.rename(columns={\n",
    "    'mode_category': 'ventilator_setting',\n",
    "    'fio2_set': 'FiO2 Set (N)',\n",
    "    'tidal_volume_set': 'Tidal Volume Set (N)',\n",
    "    'resp_rate_set': 'Resp Rate Set (N)',\n",
    "    'pressure_control_set': 'Pressure Control Set (N)',\n",
    "    'peep_set': 'PEEP Set (N)',\n",
    "    'pressure_support_set': 'Pressure Support Set (N)'\n",
    "})\n",
    "\n",
    "# Sort\n",
    "counts_summary = counts_summary.sort_values(['device_category', 'ventilator_setting'])\n",
    "\n",
    "print(\"\\nObservation Counts by Device and Mode:\")\n",
    "print(counts_summary.to_string(index=False))\n",
    "\n",
    "counts_summary.to_csv('../output/final/tableone/ventilator_settings_counts_by_device_mode.csv', index=False)\n",
    "print(f\"\\n✅ Saved: ../output/final/tableone/ventilator_settings_counts_by_device_mode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f30cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VENTILATOR MODE PROPORTIONS - FIRST 24 HOURS OF IMV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Filter to First 24 Hours of IMV\n",
    "# ============================================================================\n",
    "\n",
    "# Use the IMV data with hours from vent start that we already created\n",
    "imv_first_24h = resp_imv_post_start[\n",
    "    (resp_imv_post_start['hours_from_vent_start'] >= 0) &\n",
    "    (resp_imv_post_start['hours_from_vent_start'] <= 24)\n",
    "].copy()\n",
    "resp_support = clifpy.tables.RespiratorySupport(data=imv_first_24h, output_directory=clifpy_dir)\n",
    "resp_support = resp_support.waterfall(verbose=True)\n",
    "imv_first_24h = resp_support.df.copy()\n",
    "\n",
    "print(f\"\\n📊 Data Summary:\")\n",
    "print(f\"   Total IMV records in first 24h: {len(imv_first_24h):,}\")\n",
    "print(f\"   Unique encounters: {imv_first_24h['encounter_block'].nunique():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Map Mode Categories to Simplified Groups\n",
    "# ============================================================================\n",
    "\n",
    "# Define mode category mapping (based on your image)\n",
    "mode_mapping = {\n",
    "    'assist control-volume control': 'Assist Control-Volume Control',\n",
    "    'pressure-regulated volume control': 'Pressure-Regulated Volume Control',\n",
    "    'simv': 'SIMV',\n",
    "    'pressure support/cpap': 'Pressure Support/CPAP',\n",
    "    'pressure support': 'Pressure Support/CPAP',\n",
    "    'cpap': 'Pressure Support/CPAP',\n",
    "    'pressure control': 'Pressure Control',\n",
    "}\n",
    "\n",
    "# Apply mapping, anything not mapped goes to \"Other\"\n",
    "imv_first_24h['mode_group'] = imv_first_24h['mode_category'].str.lower().map(mode_mapping)\n",
    "imv_first_24h['mode_group'] = imv_first_24h['mode_group'].fillna('Other')\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Calculate Proportions\n",
    "# ============================================================================\n",
    "\n",
    "# Count observations per mode\n",
    "mode_counts = imv_first_24h['mode_group'].value_counts()\n",
    "total_obs = len(imv_first_24h)\n",
    "\n",
    "# Calculate proportions\n",
    "mode_proportions = (mode_counts / total_obs).sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\n📊 Mode Category Counts:\")\n",
    "for mode, count in mode_counts.items():\n",
    "    proportion = count / total_obs\n",
    "    print(f\"   {mode}: {count:,} ({proportion:.1%})\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Create DataFrame for Plotting\n",
    "# ============================================================================\n",
    "\n",
    "plot_data = pd.DataFrame({\n",
    "    'Mode': mode_proportions.index,\n",
    "    'Proportion': mode_proportions.values,\n",
    "    'Count': mode_counts[mode_proportions.index].values\n",
    "})\n",
    "\n",
    "print(\"\\nPlot Data:\")\n",
    "print(plot_data.to_string(index=False))\n",
    "\n",
    "# Save the data\n",
    "plot_data.to_csv('../output/final/tableone/mode_proportions_first_24h.csv', index=False)\n",
    "print(f\"\\n✅ Saved: ../output/final/tableone/mode_proportions_first_24h.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "# Define colors for each mode (matching the image)\n",
    "color_map = {\n",
    "    'Assist Control-Volume Control': '#66c2a5',  # Green\n",
    "    'Pressure-Regulated Volume Control': '#fc8d62',  # Orange\n",
    "    'SIMV': '#3288bd',  # Blue\n",
    "    'Pressure Support/CPAP': '#9e9ac8',  # Purple\n",
    "    'Pressure Control': '#fee08b',  # Yellow\n",
    "    'Other': '#e41a8c'  # Pink/Magenta\n",
    "}\n",
    "\n",
    "# Create vertical stacked bar\n",
    "bottom = 0\n",
    "\n",
    "for idx, row in plot_data.iterrows():\n",
    "    mode = row['Mode']\n",
    "    proportion = row['Proportion']\n",
    "    count = row['Count']\n",
    "    color = color_map.get(mode, '#cccccc')\n",
    "    \n",
    "    ax.bar(0, proportion, bottom=bottom, width=0.5, \n",
    "          color=color, edgecolor='white', linewidth=2)\n",
    "    \n",
    "    # Add text label\n",
    "    if proportion > 0.03:\n",
    "        ax.text(0, bottom + proportion/2, f\"{proportion:.1%}\\n(n={count:,})\", \n",
    "               ha='center', va='center', fontsize=9, fontweight='bold',\n",
    "               color='white' if proportion > 0.15 else 'black')\n",
    "    \n",
    "    bottom += proportion\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlim(-0.5, 0.5)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel('Proportion of Mode Category', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([0])\n",
    "ax.set_xticklabels(['Dataset\\n(All IMV Encounters)'], fontsize=11)\n",
    "ax.set_title('Proportions of Different Ventilator Modes\\nUsed in First 24 Hours of IMV', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Create legend\n",
    "legend_elements = [plt.Rectangle((0,0),1,1, fc=color_map.get(mode, '#cccccc'), \n",
    "                                edgecolor='white', linewidth=2, label=mode)\n",
    "                  for mode in plot_data['Mode']]\n",
    "ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1.02, 1), \n",
    "         fontsize=10, title='Mode Category', title_fontsize=11)\n",
    "\n",
    "# Add grid\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/tableone/mode_proportions_first_24h_vertical.png', \n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✅ Saved: ../output/final/tableone/mode_proportions_first_24h_vertical.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tableone_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499569f",
   "metadata": {},
   "source": [
    "# Meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bdfb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading medication_admin_continuous table...\")\n",
    "clif.load_table(\n",
    "    'medication_admin_continuous',\n",
    "    columns=meds_required_columns,\n",
    "    filters={\n",
    "        'hospitalization_id': final_hosp_ids\n",
    "    }\n",
    ")\n",
    "clif.medication_admin_continuous.df= pd.merge(clif.medication_admin_continuous.df, encounter_mapping, \n",
    "                                        on='hospitalization_id', how='left')\n",
    "\n",
    "\n",
    "print(f\"   Medications loaded: {len(clif.medication_admin_continuous.df):,} rows\")\n",
    "print(f\"   Unique medication categories: {clif.medication_admin_continuous.df['med_category'].nunique()}\")\n",
    "print(f\"   Unique medication_admin_continuous hospitalizations: {clif.medication_admin_continuous.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "meds_df = clif.medication_admin_continuous.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31316c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Medication Flags and Vasopressor Statistics at Encounter Block Level\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING MEDICATION FLAGS AND VASOPRESSOR STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Medication categories to track\n",
    "med_categories = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin', 'dopamine',\n",
    "    'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', 'fentanyl',\n",
    "    'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium'\n",
    "]\n",
    "\n",
    "# Vasopressors that need conversion and dose statistics\n",
    "vasopressors = ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin', 'dopamine']\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Create Binary Flags (0/1) for Each Medication\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n📊 Creating binary flags for medication exposure...\")\n",
    "\n",
    "# Get unique encounter_block - med_category combinations\n",
    "med_encounters = meds_df[meds_df['med_category'].isin(med_categories)].groupby(\n",
    "    ['encounter_block', 'med_category']\n",
    ").size().reset_index(name='count')\n",
    "\n",
    "# Pivot to create binary columns (1 if medication given, 0 if not)\n",
    "med_flags = med_encounters.pivot(\n",
    "    index='encounter_block',\n",
    "    columns='med_category',\n",
    "    values='count'\n",
    ").notna().astype(int)\n",
    "\n",
    "# Rename columns with suffix\n",
    "med_flags.columns = [f'{col}_flag' for col in med_flags.columns]\n",
    "med_flags = med_flags.reset_index()\n",
    "\n",
    "print(f\"✅ Created binary flags for {len(med_flags.columns)-1} medications\")\n",
    "print(f\"   Encounters with medication data: {len(med_flags):,}\")\n",
    "\n",
    "# Show summary\n",
    "for col in med_flags.columns:\n",
    "    if col != 'encounter_block':\n",
    "        count = med_flags[col].sum()\n",
    "        pct = 100 * count / len(med_flags)\n",
    "        print(f\"   {col}: {count:,} encounters ({pct:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Convert Vasopressor Units to mcg/kg/min\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Converting Vasopressor Units\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define preferred units for vasopressors\n",
    "preferred_units = {\n",
    "    'norepinephrine': 'mcg/kg/min',\n",
    "    'epinephrine': 'mcg/kg/min',\n",
    "    'phenylephrine': 'mcg/kg/min',\n",
    "    'vasopressin': 'mcg/kg/min',\n",
    "    'dopamine': 'mcg/kg/min'\n",
    "}\n",
    "\n",
    "print(f\"Converting {len(preferred_units)} vasopressors to mcg/kg/min...\")\n",
    "\n",
    "# Convert units (uses clifpy orchestrator)\n",
    "clif.convert_dose_units_for_continuous_meds(\n",
    "    preferred_units=preferred_units,\n",
    "    override=True, \n",
    "    save_to_table=True\n",
    ")\n",
    "\n",
    "# Get converted data\n",
    "meds_converted = clif.medication_admin_continuous.df_converted.copy()\n",
    "\n",
    "# Check conversion results\n",
    "conversion_counts = clif.medication_admin_continuous.conversion_counts\n",
    "\n",
    "print(\"\\n=== Conversion Summary ===\")\n",
    "success_count = conversion_counts[conversion_counts['_convert_status'] == 'success']['count'].sum()\n",
    "total_count = conversion_counts['count'].sum()\n",
    "print(f\"Successful conversions: {success_count:,} / {total_count:,} ({100*success_count/total_count:.1f}%)\")\n",
    "\n",
    "# Show any failed conversions\n",
    "failed_conversions = conversion_counts[conversion_counts['_convert_status'] != 'success']\n",
    "if len(failed_conversions) > 0:\n",
    "    print(f\"\\n⚠️ Found {len(failed_conversions)} conversion issues:\")\n",
    "    print(failed_conversions[['med_category', '_clean_unit', '_convert_status', 'count']].to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Calculate Median and IQR for Vasopressors (Optimized)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Calculating Vasopressor Dose Statistics\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter to vasopressors only and successfully converted doses\n",
    "vaso_df = meds_converted[\n",
    "    (meds_converted['med_category'].isin(vasopressors)) &\n",
    "    (meds_converted['_convert_status'] == 'success')\n",
    "].copy()\n",
    "\n",
    "print(f\"Vasopressor records for analysis: {len(vaso_df):,}\")\n",
    "\n",
    "# Check which vasopressors exist in data\n",
    "existing_vasos = [v for v in vasopressors if v in vaso_df['med_category'].unique()]\n",
    "print(f\"Vasopressors found: {existing_vasos}\")\n",
    "\n",
    "# Calculate statistics for each vasopressor separately (more efficient)\n",
    "vaso_stats_list = []\n",
    "\n",
    "for vaso in existing_vasos:\n",
    "    vaso_subset = vaso_df[vaso_df['med_category'] == vaso]\n",
    "    \n",
    "    # Calculate median, Q1, Q3 (vectorized)\n",
    "    stats = vaso_subset.groupby('encounter_block')['med_dose'].agg([\n",
    "        ('median', 'median'),\n",
    "        ('q1', lambda x: x.quantile(0.25)),\n",
    "        ('q3', lambda x: x.quantile(0.75))\n",
    "    ])\n",
    "    \n",
    "    # Rename columns with medication prefix\n",
    "    stats.columns = [f'{vaso}_{col}' for col in stats.columns]\n",
    "    stats = stats.reset_index()\n",
    "    \n",
    "    vaso_stats_list.append(stats)\n",
    "    \n",
    "    print(f\"   {vaso}: {len(stats):,} encounters with dose data\")\n",
    "\n",
    "# Merge all vasopressor statistics\n",
    "if vaso_stats_list:\n",
    "    vaso_stats = vaso_stats_list[0]\n",
    "    for stats in vaso_stats_list[1:]:\n",
    "        vaso_stats = vaso_stats.merge(stats, on='encounter_block', how='outer')\n",
    "    \n",
    "    print(f\"\\n✅ Calculated dose statistics for {len(existing_vasos)} vasopressors\")\n",
    "    print(f\"   Total encounters with vasopressor data: {len(vaso_stats):,}\")\n",
    "else:\n",
    "    vaso_stats = pd.DataFrame({'encounter_block': []})\n",
    "    print(\"\\n⚠️ No vasopressor data found for statistics\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Merge Everything to final_tableone_df\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Merging to final_tableone_df\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "initial_cols = len(final_tableone_df.columns)\n",
    "\n",
    "# Merge medication flags\n",
    "final_tableone_df = final_tableone_df.merge(\n",
    "    med_flags,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN with 0 for medication flags (encounters without that medication)\n",
    "flag_cols = [col for col in med_flags.columns if col.endswith('_flag') and col != 'encounter_block']\n",
    "for col in flag_cols:\n",
    "    final_tableone_df[col] = final_tableone_df[col].fillna(0).astype(int)\n",
    "\n",
    "print(f\"✅ Added {len(flag_cols)} medication flag columns\")\n",
    "\n",
    "# Merge vasopressor statistics\n",
    "if len(vaso_stats) > 0:\n",
    "    final_tableone_df = final_tableone_df.merge(\n",
    "        vaso_stats,\n",
    "        on='encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    vaso_stat_cols = [col for col in vaso_stats.columns if col != 'encounter_block']\n",
    "    print(f\"✅ Added {len(vaso_stat_cols)} vasopressor dose statistic columns\")\n",
    "\n",
    "new_cols = len(final_tableone_df.columns) - initial_cols\n",
    "\n",
    "print(f\"\\n✅ Total new columns added: {new_cols}\")\n",
    "print(f\"   Final tableone columns: {len(final_tableone_df.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Summary Report\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MEDICATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📋 Medication Flag Columns Added:\")\n",
    "for col in sorted(flag_cols):\n",
    "    med_name = col.replace('_flag', '')\n",
    "    count = final_tableone_df[col].sum()\n",
    "    pct = 100 * count / len(final_tableone_df)\n",
    "    print(f\"   {med_name:30s}: {count:6,} encounters ({pct:5.1f}%)\")\n",
    "\n",
    "if len(vaso_stats) > 0:\n",
    "    print(\"\\n📋 Vasopressor Dose Statistics Columns Added:\")\n",
    "    for vaso in existing_vasos:\n",
    "        median_col = f'{vaso}_median'\n",
    "        if median_col in final_tableone_df.columns:\n",
    "            count = final_tableone_df[median_col].notna().sum()\n",
    "            print(f\"   {vaso}:\")\n",
    "            print(f\"      - {median_col}\")\n",
    "            print(f\"      - {vaso}_q1\")\n",
    "            print(f\"      - {vaso}_q3\")\n",
    "            print(f\"      ({count:,} encounters with dose data)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ MEDICATION PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cca29",
   "metadata": {},
   "source": [
    "## First 24 hrs of ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Define medication groups\n",
    "med_groups = {\n",
    "    'vasoactive': ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin', 'dopamine'],\n",
    "    'sedative': ['propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', 'fentanyl'],\n",
    "    'paralytic': ['vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium']\n",
    "}\n",
    "\n",
    "all_meds = [med for meds in med_groups.values() for med in meds]\n",
    "\n",
    "# Merge and calculate hours from ICU (vectorized)\n",
    "meds_merged = meds_df.merge(\n",
    "    final_tableone_df[['encounter_block', 'first_icu_in_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "meds_merged['hours_from_icu'] = (\n",
    "    pd.to_datetime(meds_merged['admin_dttm']) - pd.to_datetime(meds_merged['first_icu_in_dttm'])\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Filter and bin (vectorized), handle non-finite for hour_bin, avoid IntCastingNaNError\n",
    "meds_merged['med_lower'] = meds_merged['med_category'].str.lower()\n",
    "finite_mask = np.isfinite(meds_merged['hours_from_icu'])\n",
    "meds_merged['hour_bin'] = np.nan\n",
    "meds_merged.loc[finite_mask, 'hour_bin'] = np.floor(meds_merged.loc[finite_mask, 'hours_from_icu'])\n",
    "meds_merged['hour_bin'] = meds_merged['hour_bin'].astype('Int64')\n",
    "\n",
    "meds_7d = meds_merged[\n",
    "    (meds_merged['med_lower'].isin(all_meds)) &\n",
    "    (meds_merged['hour_bin'].notna()) &\n",
    "    (meds_merged['hour_bin'] >= 0) &\n",
    "    (meds_merged['hour_bin'] <= 167)\n",
    "]\n",
    "\n",
    "total_icu_encounters = final_tableone_df[final_tableone_df['icu_enc'] == 1]['encounter_block'].nunique()\n",
    "\n",
    "# ============================================================================\n",
    "#  hourly counts and percentages\n",
    "# ============================================================================\n",
    "\n",
    "pivot = (\n",
    "    meds_7d\n",
    "    .groupby(['hour_bin', 'med_lower'])['encounter_block']\n",
    "    .nunique()\n",
    "    .unstack(fill_value=0)\n",
    "    .reindex(index=np.arange(168), columns=all_meds, fill_value=0)\n",
    ")\n",
    "\n",
    "pct_pivot = (pivot / total_icu_encounters * 100) if total_icu_encounters > 0 else pivot * 0\n",
    "\n",
    "hourly_df = pd.DataFrame({'hour': np.arange(168)})\n",
    "hourly_df = pd.concat([\n",
    "    hourly_df,\n",
    "    pivot.add_suffix('_n'),\n",
    "    pct_pivot.add_suffix('_pct')\n",
    "], axis=1)\n",
    "\n",
    "hourly_df.to_csv('../output/final/tableone/medications_hourly_data.csv', index=False)\n",
    "print(f\"✅ Saved: ../output/final/tableone/medications_hourly_data.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# Plotly plotting functions (interactive area plots)\n",
    "# ============================================================================\n",
    "\n",
    "colors = {\n",
    "    'norepinephrine': '#1f77b4', 'epinephrine': '#ff7f0e', 'phenylephrine': '#2ca02c',\n",
    "    'vasopressin': '#d62728', 'dopamine': '#9467bd',\n",
    "    'propofol': '#e377c2', 'midazolam': '#7f7f7f', 'lorazepam': '#bcbd22', 'dexmedetomidine': '#17becf',\n",
    "    'vecuronium': '#8c564b', 'rocuronium': '#f7b6d2', 'cisatracurium': '#c49c94', 'pancuronium': '#dbdb8d'\n",
    "}\n",
    "\n",
    "def hex_to_rgba(hex_color, alpha=0.2):\n",
    "    \"\"\"Convert hex RGB color like '#1f77b4' to 'rgba(R,G,B,A)' string.\"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    if len(hex_color) == 6:\n",
    "        r = int(hex_color[0:2], 16)\n",
    "        g = int(hex_color[2:4], 16)\n",
    "        b = int(hex_color[4:6], 16)\n",
    "        return f'rgba({r},{g},{b},{alpha})'\n",
    "    # Fallback to gray if something is wrong\n",
    "    return f'rgba(180,180,180,{alpha})'\n",
    "\n",
    "def plotly_medication_group(group_name, meds, hourly_df, output_path_html):\n",
    "    fig = go.Figure()\n",
    "    hours = hourly_df['hour'].values\n",
    "\n",
    "    for med in meds:\n",
    "        pct_col = f\"{med}_pct\"\n",
    "        if pct_col in hourly_df.columns:\n",
    "            color = colors.get(med, '#333')\n",
    "            fillcolor = (\n",
    "                hex_to_rgba(color, 0.2)\n",
    "                if color.startswith(\"#\") and len(color) == 7\n",
    "                else \"rgba(180,180,180,0.15)\"\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=hours,\n",
    "                    y=hourly_df[pct_col],\n",
    "                    mode='lines',\n",
    "                    name=med.capitalize(),\n",
    "                    line=dict(color=color, width=3),\n",
    "                    fill='tozeroy',\n",
    "                    fillcolor=fillcolor,\n",
    "                    opacity=0.8,\n",
    "                    hovertemplate=f\"{med.capitalize()}<br>Hour: %{{x}}<br>% ICU: %{{y:.2f}}<extra></extra>\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"{group_name.capitalize()} Medication Use in First 7 Days of ICU\",\n",
    "        xaxis_title=\"Hours from ICU Admission\",\n",
    "        yaxis_title=\"% of ICU Encounters\",\n",
    "        xaxis=dict(range=[0, 168]),\n",
    "        yaxis=dict(range=[0, None]),\n",
    "        legend=dict(title=\"Medication\", font=dict(size=12)),\n",
    "        template=\"simple_white\",\n",
    "        font=dict(size=14),\n",
    "        margin=dict(l=50, r=20, t=70, b=50)\n",
    "    )\n",
    "\n",
    "    # Save interactive plot as HTML\n",
    "    pio.write_html(fig, output_path_html)\n",
    "    fig.show()\n",
    "\n",
    "# Generate all 3 interactive plots (save as HTML)\n",
    "plotly_medication_group(\n",
    "    'vasoactive', med_groups['vasoactive'], hourly_df, \n",
    "    '../output/final/tableone/vasoactive_area_curve_7d.html'\n",
    ")\n",
    "plotly_medication_group(\n",
    "    'sedative', med_groups['sedative'], hourly_df, \n",
    "    '../output/final/tableone/sedative_area_curve_7d.html'\n",
    ")\n",
    "plotly_medication_group(\n",
    "    'paralytic', med_groups['paralytic'], hourly_df,\n",
    "    '../output/final/tableone/paralytic_area_curve_7d.html'\n",
    ")\n",
    "\n",
    "print(\"\\n✅ All medication plots (plotly) created and saved as HTML!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Define medication groups\n",
    "med_groups = {\n",
    "    'vasoactive': ['norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin', 'dopamine'],\n",
    "    'sedative': ['propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', 'fentanyl'],\n",
    "    'paralytic': ['vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium']\n",
    "}\n",
    "\n",
    "# Lower-cased mapping for safety\n",
    "med_to_group = {med: group for group, meds in med_groups.items() for med in meds}\n",
    "all_meds = [med for meds in med_groups.values() for med in meds]\n",
    "\n",
    "# Merge and preprocess (same as before)\n",
    "meds_merged = meds_df.merge(\n",
    "    final_tableone_df[['encounter_block', 'first_icu_in_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "meds_merged['hours_from_icu'] = (\n",
    "    pd.to_datetime(meds_merged['admin_dttm']) - pd.to_datetime(meds_merged['first_icu_in_dttm'])\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "meds_merged['med_lower'] = meds_merged['med_category'].str.lower()\n",
    "finite_mask = np.isfinite(meds_merged['hours_from_icu'])\n",
    "meds_merged['hour_bin'] = np.nan\n",
    "meds_merged.loc[finite_mask, 'hour_bin'] = np.floor(meds_merged.loc[finite_mask, 'hours_from_icu'])\n",
    "meds_merged['hour_bin'] = meds_merged['hour_bin'].astype('Int64')\n",
    "\n",
    "meds_7d = meds_merged[\n",
    "    (meds_merged['med_lower'].isin(all_meds)) &\n",
    "    (meds_merged['hour_bin'].notna()) &\n",
    "    (meds_merged['hour_bin'] >= 0) &\n",
    "    (meds_merged['hour_bin'] <= 167)\n",
    "].copy()\n",
    "\n",
    "# =======================\n",
    "# Line plot: median dose by hour since ICU admission (per med group)\n",
    "# =======================\n",
    "\n",
    "colors = {\n",
    "    'norepinephrine': '#1f77b4', 'epinephrine': '#ff7f0e', 'phenylephrine': '#2ca02c',\n",
    "    'vasopressin': '#d62728', 'dopamine': '#9467bd',\n",
    "    'propofol': '#e377c2', 'midazolam': '#7f7f7f', 'lorazepam': '#bcbd22', 'dexmedetomidine': '#17becf',\n",
    "    'vecuronium': '#8c564b', 'rocuronium': '#f7b6d2', 'cisatracurium': '#c49c94', 'pancuronium': '#dbdb8d'\n",
    "}\n",
    "\n",
    "def plot_median_dose_line_by_hour(group_name, meds, meds_7d, output_path_html):\n",
    "    fig = go.Figure()\n",
    "    for med in meds:\n",
    "        med_data = meds_7d[meds_7d['med_lower'] == med]\n",
    "        # For each hour, compute the median dose (across all encounters)\n",
    "        hourly_median = (\n",
    "            med_data.groupby('hour_bin')['med_dose']\n",
    "            .median()\n",
    "            .reset_index()\n",
    "            .sort_values('hour_bin')\n",
    "        )\n",
    "        color = colors.get(med, '#333')\n",
    "        if not hourly_median.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=hourly_median['hour_bin'],\n",
    "                    y=hourly_median['med_dose'],\n",
    "                    mode='lines+markers',\n",
    "                    name=med.capitalize(),\n",
    "                    line=dict(color=color, width=3),\n",
    "                    marker=dict(size=6),\n",
    "                    hovertemplate=f\"{med.capitalize()}<br>Hour: %{{x}}<br>Median Dose: %{{y:.2f}}<extra></extra>\"\n",
    "                )\n",
    "            )\n",
    "    fig.update_layout(\n",
    "        title=f\"Median {group_name.capitalize()} Dose by Hour Since ICU Admission\",\n",
    "        xaxis_title=\"Hours from ICU Admission\",\n",
    "        yaxis_title=\"Median Dose\",\n",
    "        legend=dict(title=\"Medication\", font=dict(size=12)),\n",
    "        template=\"simple_white\",\n",
    "        font=dict(size=14),\n",
    "        margin=dict(l=50, r=20, t=70, b=50),\n",
    "        xaxis=dict(range=[0, 168])\n",
    "    )\n",
    "    pio.write_html(fig, output_path_html)\n",
    "    fig.show()\n",
    "\n",
    "# Generate and save plots for each medication group (lines: median dose over time)\n",
    "plot_median_dose_line_by_hour(\n",
    "    'vasoactive', med_groups['vasoactive'], meds_7d,\n",
    "    '../output/final/tableone/vasoactive_median_dose_by_hour.html'\n",
    ")\n",
    "plot_median_dose_line_by_hour(\n",
    "    'sedative', med_groups['sedative'], meds_7d,\n",
    "    '../output/final/tableone/sedative_median_dose_by_hour.html'\n",
    ")\n",
    "plot_median_dose_line_by_hour(\n",
    "    'paralytic', med_groups['paralytic'], meds_7d,\n",
    "    '../output/final/tableone/paralytic_median_dose_by_hour.html'\n",
    ")\n",
    "\n",
    "print(\"\\n✅ All median dose line plots by hour (plotly) created and saved as HTML!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98283a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  summary statistics\n",
    "# ============================================================================\n",
    "\n",
    "# Group by medication once, calculate all stats on med_dose\n",
    "summary_agg = (\n",
    "    meds_7d\n",
    "    .groupby('med_lower')['med_dose']\n",
    "    .agg(['count', 'median', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)])\n",
    "    .rename(columns={'count': 'n_admin', '<lambda_0>': 'q1_dose', '<lambda_1>': 'q3_dose'})\n",
    ")\n",
    "\n",
    "# Count unique encounters per medication\n",
    "encounter_counts = meds_7d.groupby('med_lower')['encounter_block'].nunique()\n",
    "\n",
    "# Get most common dose unit per medication\n",
    "dose_units = meds_7d.groupby('med_lower')['med_dose_unit'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else '')\n",
    "\n",
    "# Combine and add group labels\n",
    "summary_df = pd.DataFrame({\n",
    "    'medication': summary_agg.index,\n",
    "    'n_encounters': encounter_counts.values,\n",
    "    'pct_encounters': (encounter_counts / total_icu_encounters * 100).values,\n",
    "    'median_dose': summary_agg['median'].values,\n",
    "    'q1_dose': summary_agg['q1_dose'].values,\n",
    "    'q3_dose': summary_agg['q3_dose'].values,\n",
    "    'dose_unit': dose_units.values\n",
    "})\n",
    "\n",
    "# Add group labels (vectorized with map)\n",
    "med_to_group = {med: group for group, meds in med_groups.items() for med in meds}\n",
    "summary_df['group'] = summary_df['medication'].map(med_to_group)\n",
    "summary_df = summary_df[['group', 'medication', 'n_encounters', 'pct_encounters', \n",
    "                         'median_dose', 'q1_dose', 'q3_dose', 'dose_unit']]\n",
    "\n",
    "summary_df.to_csv('../output/final/tableone/medications_summary_stats.csv', index=False)\n",
    "print(f\"✅ Saved: ../output/final/tableone/medications_summary_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad75da",
   "metadata": {},
   "source": [
    "# Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0feee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Load Labs\n",
    "# ----------------------------------------------------------------------------\n",
    "print(f\"\\nLoading labs table...\")\n",
    "clif.load_table(\n",
    "    'labs',\n",
    "    columns=['hospitalization_id', 'lab_result_dttm', 'lab_category', 'lab_value_numeric', 'reference_unit'],\n",
    "    filters={\n",
    "        'hospitalization_id': final_hosp_ids\n",
    "    }\n",
    ")\n",
    "clif.labs.df= pd.merge(clif.labs.df, encounter_mapping, \n",
    "                                        on='hospitalization_id', how='left')\n",
    "\n",
    "\n",
    "print(\"Applying outlier handling to Labs data...\")\n",
    "print(\"=\" * 50)\n",
    "get_value_counts(clif.labs, ['lab_name', 'lab_category', 'lab_loinc_code'], output_dir=mcide_dir)\n",
    "# get_value_counts(clif.labs, ['lab_specimen_name', 'lab_specimen_category'], output_dir=mcide_dir)\n",
    "apply_outlier_handling(clif.labs)\n",
    "create_summary_table(clif.labs, 'lab_value_numeric',group_by_cols='lab_category',\n",
    "                    output_dir=summary_stats_dir)\n",
    "create_summary_table(clif.labs, 'lab_value_numeric',group_by_cols=['lab_category', 'reference_unit'],\n",
    "                    output_dir=summary_stats_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e50ea",
   "metadata": {},
   "source": [
    "# Comorbidity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading vitals table...\")\n",
    "clif.load_table(\n",
    "    'hospital_diagnosis',\n",
    "    filters={\n",
    "        'hospitalization_id': final_hosp_ids\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clifpy.utils.comorbidity import calculate_cci\n",
    "cci_results = calculate_cci( clif.hospital_diagnosis, hierarchy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d26c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cci_results = (\n",
    "    cci_results.merge(encounter_mapping, on=\"hospitalization_id\")\n",
    "    .drop(columns=[\"hospitalization_id\"])\n",
    "    .drop_duplicates()\n",
    ")\n",
    "# Join with final_tableone_df on encounter_block\n",
    "final_tableone_df = final_tableone_df.merge(cci_results, on=\"encounter_block\", how=\"left\")\n",
    "final_tableone_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comorbidities per 1000 hospitalizations, and save results WITH statistical summaries in CSV\n",
    "# Step 1: Get the total number of unique hospitalizations\n",
    "total_hospitalizations = cci_results['encounter_block'].nunique()\n",
    "print(f\"Total hospitalizations: {total_hospitalizations:,}\")\n",
    "\n",
    "# Step 2: Define the comorbidity columns (exclude IDs and total score)\n",
    "exclude_columns = {'hospitalization_id', 'encounter_block', 'cci_score'}\n",
    "comorbidity_columns = [col for col in cci_results.columns if col not in exclude_columns]\n",
    "\n",
    "# Step 3: Calculate the count of each comorbidity (assume binary indicators)\n",
    "comorbidity_counts = cci_results[comorbidity_columns].sum()\n",
    "\n",
    "# Step 4: Compute prevalence rates\n",
    "comorbidity_per_1000 = (comorbidity_counts / total_hospitalizations) * 1000\n",
    "prevalence_percent = (comorbidity_counts.values / total_hospitalizations * 100).round(2)\n",
    "\n",
    "# Step 5: Create a summary dataframe\n",
    "comorbidity_summary = pd.DataFrame({\n",
    "    'comorbidity': comorbidity_columns,\n",
    "    'n_patients': comorbidity_counts.values,\n",
    "    'prevalence_percent': prevalence_percent,\n",
    "    'per_1000_hospitalizations': comorbidity_per_1000.values.round(1)\n",
    "})\n",
    "\n",
    "# Sort by per 1000 prevalence\n",
    "comorbidity_summary = comorbidity_summary.sort_values('per_1000_hospitalizations', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Prepare summary statistics for output\n",
    "total_comorbidities = int(comorbidity_counts.sum())\n",
    "avg_comorbidities_per_hosp = total_comorbidities / total_hospitalizations if total_hospitalizations > 0 else 0\n",
    "most_common_comorbidity = comorbidity_summary.iloc[0]['comorbidity']\n",
    "most_common_per_1000 = comorbidity_summary.iloc[0]['per_1000_hospitalizations']\n",
    "\n",
    "# Step 7: Save both table and summary statistics to CSV\n",
    "\n",
    "# First, write the comorbidity table to CSV\n",
    "out_csv = '../output/final/tableone/comorbidities_per_1000_hospitalizations.csv'\n",
    "comorbidity_summary.to_csv(out_csv, index=False)\n",
    "\n",
    "# Write summary statistics to a second csv, and then append to same file as lines at the end\n",
    "\n",
    "summary_stats = [\n",
    "    ['Total hospitalizations', total_hospitalizations],\n",
    "    ['Total comorbidities across all patients', total_comorbidities],\n",
    "    ['Average comorbidities per hospitalization', f\"{avg_comorbidities_per_hosp:.2f}\"],\n",
    "    ['Most common comorbidity', most_common_comorbidity],\n",
    "    ['Most common: per 1000 hospitalizations', f\"{most_common_per_1000:.1f}\"],\n",
    "]\n",
    "\n",
    "# Save the summary stats to a separate CSV for clarity (and also appending to the main comorbidity file for convenience)\n",
    "summary_csv = '../output/final/tableone/comorbidities_per_1000_hospitalizations_summary.csv'\n",
    "with open(summary_csv, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Metric', 'Value'])\n",
    "    for row in summary_stats:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"\\nComorbidity table saved to: {out_csv}\")\n",
    "print(f\"Summary statistics saved to: {summary_csv}\")\n",
    "\n",
    "# Step 8: Bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars = ax.barh(\n",
    "    comorbidity_summary['comorbidity'], \n",
    "    comorbidity_summary['per_1000_hospitalizations'],\n",
    "    color='#7FA8B8',\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Per 1000 Hospitalizations', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Comorbidity', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comorbidity Prevalence per 1000 Hospitalizations', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(comorbidity_summary.iterrows()):\n",
    "    ax.text(row['per_1000_hospitalizations'] + 5, i, f\"{row['per_1000_hospitalizations']:.1f}\", va='center', fontsize=9)\n",
    "\n",
    "ax.grid(axis='x', linestyle='-', alpha=0.3)\n",
    "ax.set_axisbelow(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/tableone/comorbidities_per_1000_barplot.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25671f",
   "metadata": {},
   "source": [
    "# ECMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68700042",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading ECMO table...\")\n",
    "try:\n",
    "    clif.load_table(\n",
    "        'ecmo_mcs',\n",
    "        filters={\n",
    "            'hospitalization_id': final_hosp_ids\n",
    "        }\n",
    "    )\n",
    "    clif.ecmo_mcs.df = pd.merge(\n",
    "        clif.ecmo_mcs.df,\n",
    "        encounter_mapping,\n",
    "        on='hospitalization_id',\n",
    "        how='left'\n",
    "    )\n",
    "    # Add on_ecmo = 1 for all encounter blocks in this df\n",
    "    clif.ecmo_mcs.df['on_ecmo'] = 1\n",
    "    # Keep only encounter_block and on_ecmo\n",
    "    ecmo_df = clif.ecmo_mcs.df[['encounter_block', 'on_ecmo']].drop_duplicates()\n",
    "    # Join with final_tableone_df on encounter_block (left join)\n",
    "    final_tableone_df = final_tableone_df.merge(ecmo_df, on='encounter_block', how='left')\n",
    "    # Optionally fill NaN with 0 if you want on_ecmo=0 for non-ECMO\n",
    "    final_tableone_df['on_ecmo'] = final_tableone_df['on_ecmo'].fillna(0).astype(int)\n",
    "    get_value_counts(clif.ecmo_mcs, ['device_name', 'device_category'], output_dir=mcide_dir)\n",
    "    create_summary_table(clif.ecmo_mcs, \n",
    "                            ['device_rate', 'sweep', 'fdO2','flow'],\n",
    "                            group_by_cols='device_category',\n",
    "                            output_dir=summary_stats_dir)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Warning: Failed to load the ECMO table: {e}. Proceeding without ECMO data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46478d",
   "metadata": {},
   "source": [
    "# CRRT Therapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading crrt_therapy table...\")\n",
    "try:\n",
    "    clif.load_table(\n",
    "        'crrt_therapy',\n",
    "        filters={\n",
    "            'hospitalization_id': final_hosp_ids\n",
    "        }\n",
    "    )\n",
    "    clif.crrt_therapy.df = pd.merge(\n",
    "        clif.crrt_therapy.df,\n",
    "        encounter_mapping,\n",
    "        on='hospitalization_id',\n",
    "        how='left'\n",
    "    )\n",
    "    clif.crrt_therapy.df['on_crrt'] = 1\n",
    "    # Keep only encounter_block and on_crrt\n",
    "    on_crrt_df = clif.crrt_therapy.df[['encounter_block', 'on_crrt']].drop_duplicates()\n",
    "    # Join with final_tableone_df on encounter_block (left join)\n",
    "    final_tableone_df = final_tableone_df.merge(on_crrt_df, on='encounter_block', how='left')\n",
    "    # Before proceeding, ensure 'on_crrt' column exists after the merge; if not, create it\n",
    "    if 'on_crrt' not in final_tableone_df.columns:\n",
    "        final_tableone_df['on_crrt'] = 0\n",
    "    else:\n",
    "        final_tableone_df['on_crrt'] = final_tableone_df['on_crrt'].fillna(0).astype(int)\n",
    "    get_value_counts(clif.crrt_therapy, ['crrt_mode_name', 'crrt_mode_category'], output_dir=mcide_dir)\n",
    "    create_summary_table(clif.crrt_therapy, \n",
    "                            ['blood_flow_rate', 'pre_filter_replacement_fluid_rate', 'post_filter_replacement_fluid_rate',\n",
    "                                'dialysate_flow_rate', 'ultrafiltration_out'],\n",
    "                            group_by_cols='crrt_mode_category',\n",
    "                            output_dir=summary_stats_dir)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Warning: Failed to load the ECMO table: {e}. Proceeding without ECMO data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3aa0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37787609",
   "metadata": {},
   "source": [
    "# Patient Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6000752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading patient_assessments table...\")\n",
    "try:\n",
    "    clif.load_table(\n",
    "        'patient_assessments',\n",
    "        filters={\n",
    "            'hospitalization_id': final_hosp_ids\n",
    "        }\n",
    "    )\n",
    "    clif.patient_assessments.df = pd.merge(\n",
    "        clif.patient_assessments.df,\n",
    "        encounter_mapping,\n",
    "        on='hospitalization_id',\n",
    "        how='left'\n",
    "    )\n",
    "    get_value_counts(clif.patient_assessments, ['assessment_name', 'assessment_category'], output_dir=mcide_dir)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Warning: Failed to load the ECMO table: {e}. Proceeding without ECMO data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75af746",
   "metadata": {},
   "source": [
    "# Number of clinical events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "clif.get_loaded_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clinical_events(clif_table, composite_key_fields, verbose=True):\n",
    "    \"\"\"\n",
    "    Count unique clinical events in a CLIF table based on composite key.\n",
    "    \n",
    "    A clinical event is defined as a unique combination of the composite key fields,\n",
    "    with all fields non-null.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    clif_table : CLIFTable object\n",
    "        The CLIF table object (e.g., clif.vitals, clif.labs, clif.medication_admin_continuous)\n",
    "    composite_key_fields : list of str\n",
    "        List of column names that form the composite key.\n",
    "        Example: ['hospitalization_id', 'recorded_dttm', 'vital_category']\n",
    "    verbose : bool, default=True\n",
    "        If True, print summary statistics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        - 'n_events': Number of unique non-null clinical events\n",
    "        - 'n_total_rows': Total rows in table\n",
    "        - 'n_rows_with_nulls': Rows with null values in composite key\n",
    "        - 'n_valid_rows': Rows with all composite key fields non-null\n",
    "        - 'pct_valid': Percentage of rows that are valid\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> # Count unique vital sign measurements\n",
    "    >>> result = count_clinical_events(\n",
    "    ...     clif.vitals, \n",
    "    ...     ['hospitalization_id', 'recorded_dttm', 'vital_category']\n",
    "    ... )\n",
    "    >>> print(f\"Unique vital measurements: {result['n_events']:,}\")\n",
    "    \"\"\"\n",
    "    # Get dataframe\n",
    "    df = clif_table.df\n",
    "    table_name = clif_table.__class__.__name__\n",
    "    \n",
    "    # Check that all composite key fields exist\n",
    "    missing_fields = [f for f in composite_key_fields if f not in df.columns]\n",
    "    if missing_fields:\n",
    "        raise ValueError(f\"Fields not found in {table_name}: {missing_fields}\")\n",
    "    \n",
    "    # Total rows\n",
    "    n_total = len(df)\n",
    "    \n",
    "    # Drop rows where ANY composite key field is null\n",
    "    df_valid = df.dropna(subset=composite_key_fields)\n",
    "    n_valid = len(df_valid)\n",
    "    n_with_nulls = n_total - n_valid\n",
    "    \n",
    "    # Count unique combinations of composite key fields (clinical events)\n",
    "    n_events = df_valid[composite_key_fields].drop_duplicates().shape[0]\n",
    "    \n",
    "    # Calculate percentage\n",
    "    pct_valid = 100 * n_valid / n_total if n_total > 0 else 0\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'n_events': n_events,\n",
    "        'n_total_rows': n_total,\n",
    "        'n_rows_with_nulls': n_with_nulls,\n",
    "        'n_valid_rows': n_valid,\n",
    "        'pct_valid': pct_valid\n",
    "    }\n",
    "    \n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Clinical Events Count: {table_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Composite key: {composite_key_fields}\")\n",
    "        print(f\"\\nTotal rows in table:        {n_total:>12,}\")\n",
    "        print(f\"Rows with null in key:      {n_with_nulls:>12,} ({100*n_with_nulls/n_total:.1f}%)\")\n",
    "        print(f\"Valid rows (non-null key):  {n_valid:>12,} ({pct_valid:.1f}%)\")\n",
    "        print(f\"Unique clinical events:     {n_events:>12,}\")\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    return results\n",
    "    \n",
    "def count_all_clinical_events(clif_obj):\n",
    "    \"\"\"\n",
    "    Count clinical events for all common CLIF tables.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    clif_obj : CLIF object\n",
    "        The main CLIF object containing all tables\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Summary of clinical events for all tables\n",
    "    \"\"\"\n",
    "    tables_config = {\n",
    "        'vitals': ['hospitalization_id', 'recorded_dttm', 'vital_category'],\n",
    "        'labs': ['hospitalization_id', 'lab_result_dttm', 'lab_category'],\n",
    "        'medication_admin_continuous': ['hospitalization_id', 'admin_dttm', 'med_category'],\n",
    "        'respiratory_support': ['hospitalization_id', 'recorded_dttm'],\n",
    "        'adt': ['hospitalization_id', 'in_dttm'],\n",
    "        'patient_assessments': ['hospitalization_id', 'recorded_dttm', 'assessment_category'],\n",
    "        'ecmo_mcs': ['hospitalization_id', 'recorded_dttm', 'device_category'],\n",
    "        'crrt_therapy' : ['hospitalization_id', 'recorded_dttm', 'crrt_mode_category'],\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for table_name, composite_key in tables_config.items():\n",
    "        if hasattr(clif_obj, table_name):\n",
    "            table = getattr(clif_obj, table_name)\n",
    "            if hasattr(table, 'df') and len(table.df) > 0:\n",
    "                result = count_clinical_events(table, composite_key, verbose=False)\n",
    "                results.append({\n",
    "                    'table': table_name,\n",
    "                    'composite_key': str(composite_key),\n",
    "                    'n_events': result['n_events'],\n",
    "                    'n_total_rows': result['n_total_rows'],\n",
    "                    'pct_valid': result['pct_valid']\n",
    "                })\n",
    "    \n",
    "    summary_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLINICAL EVENTS SUMMARY - ALL TABLES\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Run batch summary\n",
    "events_summary = count_all_clinical_events(clif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6606e40",
   "metadata": {},
   "source": [
    "# SOFA calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d066a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing ClifOrchestrator for SOFA computation...\")\n",
    "co = ClifOrchestrator(\n",
    "    data_directory=config['tables_path'],\n",
    "    filetype=config['file_type'],\n",
    "    timezone=config['timezone'],\n",
    "    output_directory=clifpy_dir\n",
    ")\n",
    "print(\"✅ ClifOrchestrator initialized\")\n",
    "\n",
    "# Filter to icu_enc == 1\n",
    "sofa_cohort_df = final_tableone_df[final_tableone_df['icu_enc'] == 1][['hospitalization_id', 'encounter_block', 'icu_enc', 'first_icu_in_dttm']]\n",
    "sofa_cohort_df['start_time'] = sofa_cohort_df['first_icu_in_dttm']\n",
    "sofa_cohort_df['end_time'] = sofa_cohort_df['start_time'] + pd.Timedelta(hours=24)\n",
    "sofa_cohort_df= sofa_cohort_df[['hospitalization_id', 'encounter_block', 'start_time', 'end_time']]\n",
    "sofa_cohort_ids = cohort_df['hospitalization_id'].astype(str).unique().tolist()\n",
    "\n",
    "\n",
    "# Load required tables for SOFA computation with cohort filtering\n",
    "print(\"Loading required tables for SOFA computation...\")\n",
    "print(\"SOFA requires: Labs (creatinine, platelet_count, po2_arterial, bilirubin_total)\")\n",
    "print(\"               Vitals (map, spo2)\")\n",
    "print(\"               Assessments (gcs_total)\")\n",
    "print(\"               Medications (norepinephrine, epinephrine, dopamine, dobutamine)\")\n",
    "print(\"               Respiratory (device_category, fio2_set)\")\n",
    "\n",
    "# Define columns to load for each table (optimize memory usage)\n",
    "sofa_columns = {\n",
    "    'labs': ['hospitalization_id', 'lab_result_dttm', 'lab_category', 'lab_value', 'lab_value_numeric'],\n",
    "    'vitals': ['hospitalization_id', 'recorded_dttm', 'vital_category', 'vital_value'],\n",
    "    'patient_assessments': ['hospitalization_id', 'recorded_dttm', 'assessment_category', 'numerical_value', 'categorical_value'],\n",
    "    'medication_admin_continuous': ['hospitalization_id', 'admin_dttm', 'med_category', 'med_dose', 'med_dose_unit'],\n",
    "    'respiratory_support':  ['hospitalization_id', 'recorded_dttm', 'device_category', 'mode_category', 'fio2_set','lpm_set', 'tidal_volume_set', 'resp_rate_set' ]\n",
    "}\n",
    "\n",
    "sofa_tables = ['labs', 'vitals', 'patient_assessments', 'medication_admin_continuous', 'respiratory_support']\n",
    "\n",
    "for table_name in sofa_tables:\n",
    "    table_cols = sofa_columns.get(table_name)\n",
    "    print(f\"Loading {table_name} with {len(table_cols) if table_cols else 'all'} columns and {len(sofa_cohort_ids)} hospitalization filters...\")\n",
    "    co.load_table(\n",
    "        table_name,\n",
    "        filters={'hospitalization_id': sofa_cohort_ids},\n",
    "        columns=table_cols\n",
    "    )\n",
    "\n",
    "co.encounter_mapping = encounter_mapping[encounter_mapping['hospitalization_id'].isin(cohort_df['hospitalization_id'])]\n",
    "print(\"✅ All required tables loaded for SOFA computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert medication units to mcg/kg/min for SOFA computation\n",
    "print(\"Converting medication units to mcg/kg/min for SOFA...\")\n",
    "\n",
    "# Define preferred units for SOFA medications\n",
    "preferred_units = {\n",
    "    'norepinephrine': 'mcg/kg/min',\n",
    "    'epinephrine': 'mcg/kg/min',\n",
    "    'dopamine': 'mcg/kg/min',\n",
    "    'dobutamine': 'mcg/kg/min'\n",
    "}\n",
    "\n",
    "print(f\"Converting {len(preferred_units)} medications: {list(preferred_units.keys())}\")\n",
    "\n",
    "# Convert units (uses vitals table for weight data)\n",
    "co.convert_dose_units_for_continuous_meds(\n",
    "    preferred_units=preferred_units,\n",
    "    override = True, \n",
    "    save_to_table=True  # Saves to co.medication_admin_continuous.df_converted\n",
    ")\n",
    "\n",
    "# Check conversion results\n",
    "conversion_counts = co.medication_admin_continuous.conversion_counts\n",
    "\n",
    "print(\"\\n=== Conversion Summary ===\")\n",
    "print(f\"Total conversion records: {len(conversion_counts):,}\")\n",
    "\n",
    "# Check for conversion failures\n",
    "success_count = conversion_counts[conversion_counts['_convert_status'] == 'success']['count'].sum()\n",
    "total_count = conversion_counts['count'].sum()\n",
    "\n",
    "print(f\"Successful conversions: {success_count:,} / {total_count:,} ({100*success_count/total_count:.1f}%)\")\n",
    "\n",
    "# Show any failed conversions\n",
    "failed_conversions = conversion_counts[conversion_counts['_convert_status'] != 'success']\n",
    "if len(failed_conversions) > 0:\n",
    "    print(f\"\\n⚠️ Found {len(failed_conversions)} conversion issues:\")\n",
    "    for _, row in failed_conversions.head(10).iterrows():\n",
    "        print(f\"  {row['med_category']}: {row['_clean_unit']} → {row['_convert_status']} ({row['count']} records)\")\n",
    "else:\n",
    "    print(\"✅ All conversions successful!\")\n",
    "print(\"\\n✅ Medication unit conversion completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e719af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing SOFA\")\n",
    "sofa_scores = co.compute_sofa_scores(\n",
    "    cohort_df=sofa_cohort_df,\n",
    "    id_name='encounter_block'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_scores = sofa_scores.merge(\n",
    "    final_tableone_df[['encounter_block', 'death_enc']], \n",
    "    how='left', \n",
    "    on='encounter_block'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_scores = sofa_scores.merge(\n",
    "    final_tableone_df[['encounter_block', 'death_enc']], \n",
    "    how='left', \n",
    "    on='encounter_block'\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#  Prepare the data\n",
    "# Group by SOFA score and calculate mortality rate and counts\n",
    "sofa_mortality = sofa_scores.groupby('sofa_total').agg({\n",
    "    'death_enc': ['mean', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "sofa_mortality.columns = ['sofa_score', 'mortality_rate', 'count']\n",
    "sofa_mortality['mortality_rate'] = sofa_mortality['mortality_rate'] * 100  # Convert to percentage\n",
    "\n",
    "# Step 2: Calculate confidence intervals (optional, for error bars)\n",
    "# Using Wilson score interval for binomial proportions\n",
    "def wilson_ci(successes, n, confidence=0.95):\n",
    "    from scipy import stats\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    p_hat = successes / n\n",
    "    denominator = 1 + z**2 / n\n",
    "    center = (p_hat + z**2 / (2*n)) / denominator\n",
    "    margin = z * np.sqrt((p_hat * (1 - p_hat) + z**2 / (4*n)) / n) / denominator\n",
    "    return center * 100, margin * 100\n",
    "\n",
    "# Calculate number of deaths per score\n",
    "sofa_mortality['deaths'] = (sofa_mortality['mortality_rate'] / 100) * sofa_mortality['count']\n",
    "\n",
    "# Calculate confidence intervals\n",
    "ci_data = [wilson_ci(deaths, n) if n > 0 else (0, 0) \n",
    "           for deaths, n in zip(sofa_mortality['deaths'], sofa_mortality['count'])]\n",
    "sofa_mortality['ci_center'] = [x[0] for x in ci_data]\n",
    "sofa_mortality['ci_margin'] = [x[1] for x in ci_data]\n",
    "\n",
    "# Step 3: Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Create bar chart\n",
    "bars = ax.bar(sofa_mortality['sofa_score'], \n",
    "              sofa_mortality['mortality_rate'],\n",
    "              color='#7FA8B8',  # Steel blue color similar to the image\n",
    "              edgecolor='black',\n",
    "              linewidth=0.5,\n",
    "              alpha=0.9)\n",
    "\n",
    "# Add error bars\n",
    "ax.errorbar(sofa_mortality['sofa_score'], \n",
    "            sofa_mortality['mortality_rate'],\n",
    "            yerr=sofa_mortality['ci_margin'],\n",
    "            fmt='none',\n",
    "            ecolor='black',\n",
    "            capsize=3,\n",
    "            capthick=1,\n",
    "            alpha=0.7)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('SOFA Score', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Mortality, %', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Mortality by SOFA Score (First 24hr of ICU admission)', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Add grid for readability\n",
    "ax.yaxis.grid(True, linestyle='-', alpha=0.3, color='gray')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Set x-axis ticks to show all SOFA scores\n",
    "ax.set_xticks(range(int(sofa_mortality['sofa_score'].min()), \n",
    "                    int(sofa_mortality['sofa_score'].max()) + 1))\n",
    "\n",
    "# Add count labels below x-axis\n",
    "counts_text = '\\n'.join([\n",
    "    'No. of patients per score',\n",
    "    '  '.join([f'{int(count)}' for count in sofa_mortality['count']])\n",
    "])\n",
    "\n",
    "# Create a second table-like annotation below the plot\n",
    "fig.text(0.1, -0.05, 'No. of patients per score', \n",
    "         ha='left', fontsize=10, weight='bold')\n",
    "\n",
    "# Add individual counts\n",
    "x_positions = np.linspace(0.15, 0.9, len(sofa_mortality))\n",
    "for i, (score, count) in enumerate(zip(sofa_mortality['sofa_score'], sofa_mortality['count'])):\n",
    "    if i < len(x_positions):\n",
    "        fig.text(x_positions[i], -0.08, f'{int(count)}', \n",
    "                ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)  # Make room for patient counts\n",
    "\n",
    "# Save the figure\n",
    "# plt.savefig('../output/final/tableone/sofa_mortality_histogram.png', \n",
    "#             dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Prepare data for CSV export\n",
    "# Calculate lower and upper confidence interval bounds\n",
    "sofa_mortality['ci_lower'] = sofa_mortality['mortality_rate'] - sofa_mortality['ci_margin']\n",
    "sofa_mortality['ci_upper'] = sofa_mortality['mortality_rate'] + sofa_mortality['ci_margin']\n",
    "\n",
    "# Ensure CI bounds are within valid range [0, 100]\n",
    "sofa_mortality['ci_lower'] = sofa_mortality['ci_lower'].clip(lower=0)\n",
    "sofa_mortality['ci_upper'] = sofa_mortality['ci_upper'].clip(upper=100)\n",
    "sofa_mortality['total_encounters'] = sofa_scores['encounter_block'].nunique()\n",
    "# Create export dataframe with all relevant columns\n",
    "sofa_export = sofa_mortality[[\n",
    "    'sofa_score', \n",
    "    'total_encounters',\n",
    "    'count',\n",
    "    'deaths',\n",
    "    'mortality_rate', \n",
    "    'ci_lower',\n",
    "    'ci_upper',\n",
    "    'ci_margin'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "sofa_export.columns = [\n",
    "    'sofa_score',\n",
    "    'total_encounters',\n",
    "    'n_encounters',\n",
    "    'n_deaths',\n",
    "    'mortality_rate_percent',\n",
    "    'ci_lower_95',\n",
    "    'ci_upper_95',\n",
    "    'ci_margin_95'\n",
    "]\n",
    "\n",
    "# Round numeric columns for readability\n",
    "sofa_export['n_encounters'] = sofa_export['n_encounters'].astype(int)\n",
    "sofa_export['total_encounters'] = sofa_export['total_encounters'].astype(int)\n",
    "sofa_export['n_deaths'] = sofa_export['n_deaths'].round(0).astype(int)\n",
    "sofa_export['mortality_rate_percent'] = sofa_export['mortality_rate_percent'].round(2)\n",
    "sofa_export['ci_lower_95'] = sofa_export['ci_lower_95'].round(2)\n",
    "sofa_export['ci_upper_95'] = sofa_export['ci_upper_95'].round(2)\n",
    "sofa_export['ci_margin_95'] = sofa_export['ci_margin_95'].round(2)\n",
    "\n",
    "# Save to CSV\n",
    "# output_path = '../output/final/tableone/sofa_mortality_summary.csv'\n",
    "# sofa_export.to_csv(output_path, index=False)\n",
    "\n",
    "# print(f\"\\n=== SOFA Mortality Summary Saved ===\")\n",
    "# print(f\"File saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84f047",
   "metadata": {},
   "source": [
    "# SOFA POLARS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the local compute_sofa_polars function, supporting reload via importlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Add parent directory to path if not already there\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent  # Go up from code/archives to code/\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import and reload compute_sofa_polars from sofa_polars using importlib\n",
    "import sofa_polars\n",
    "importlib.reload(sofa_polars)\n",
    "compute_sofa_polars = sofa_polars.compute_sofa_polars\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0427da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare SOFA cohort for Polars computation\n",
    "print(\"Preparing SOFA cohort for Polars computation...\")\n",
    "sofa_cohort_pandas = final_tableone_df[final_tableone_df['icu_enc'] == 1][\n",
    "    ['hospitalization_id', 'encounter_block', 'first_icu_in_dttm']\n",
    "].copy()\n",
    "\n",
    "sofa_cohort_pandas['start_dttm'] = sofa_cohort_pandas['first_icu_in_dttm']\n",
    "sofa_cohort_pandas['end_dttm'] = sofa_cohort_pandas['start_dttm'] + pd.Timedelta(hours=24)\n",
    "\n",
    "# Convert to Polars\n",
    "sofa_cohort_pl = pl.from_pandas(\n",
    "    sofa_cohort_pandas[['hospitalization_id', 'encounter_block', 'start_dttm', 'end_dttm']]\n",
    ")\n",
    "\n",
    "print(f\"Cohort shape: {sofa_cohort_pl.shape}\")\n",
    "print(f\"Cohort columns: {sofa_cohort_pl.columns}\")\n",
    "\n",
    "# Compute SOFA scores using Polars\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Computing SOFA scores with Polars...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "sofa_scores_pl = compute_sofa_polars(\n",
    "    data_directory=config['tables_path'],\n",
    "    cohort_df=sofa_cohort_pl,\n",
    "    filetype=config['file_type'],\n",
    "    id_name='encounter_block',\n",
    "    extremal_type='worst',\n",
    "    fill_na_scores_with_zero=True,\n",
    "    remove_outliers=True,\n",
    "    timezone=config['timezone']\n",
    ")\n",
    "\n",
    "# Convert back to Pandas if needed for compatibility\n",
    "sofa_scores_pandas = sofa_scores_pl.to_pandas()\n",
    "\n",
    "print(f\"\\n✅ SOFA computation complete!\")\n",
    "print(f\"Result shape: {sofa_scores_pandas.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(sofa_scores_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced6bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95828a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SOFA scores between Polars and clifpy implementations\n",
    "import numpy as np\n",
    "\n",
    "# Merge the two dataframes on encounter_block\n",
    "comparison_df = sofa_scores_pandas[['encounter_block', 'sofa_total']].merge(\n",
    "    sofa_scores[['encounter_block', 'sofa_total']], \n",
    "    on='encounter_block', \n",
    "    how='outer',\n",
    "    suffixes=('_polars', '_clifpy')\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SOFA SCORE COMPARISON: Polars vs clifpy\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nTotal encounter_blocks:\")\n",
    "print(f\"  Polars:  {len(sofa_scores_pandas):,}\")\n",
    "print(f\"  clifpy:  {len(sofa_scores):,}\")\n",
    "print(f\"  Merged:  {len(comparison_df):,}\")\n",
    "\n",
    "# Check for missing matches\n",
    "only_polars = comparison_df['sofa_total_clifpy'].isna().sum()\n",
    "only_clifpy = comparison_df['sofa_total_polars'].isna().sum()\n",
    "both_present = (~comparison_df['sofa_total_polars'].isna() & ~comparison_df['sofa_total_clifpy'].isna()).sum()\n",
    "\n",
    "print(f\"\\nMatching:\")\n",
    "print(f\"  Both present:     {both_present:,}\")\n",
    "print(f\"  Only in Polars:   {only_polars:,}\")\n",
    "print(f\"  Only in clifpy:   {only_clifpy:,}\")\n",
    "\n",
    "# For records present in both, compare the values\n",
    "comparison_both = comparison_df[\n",
    "    ~comparison_df['sofa_total_polars'].isna() & \n",
    "    ~comparison_df['sofa_total_clifpy'].isna()\n",
    "].copy()\n",
    "\n",
    "if len(comparison_both) > 0:\n",
    "    # Calculate differences\n",
    "    comparison_both['difference'] = comparison_both['sofa_total_polars'] - comparison_both['sofa_total_clifpy']\n",
    "    comparison_both['abs_difference'] = comparison_both['difference'].abs()\n",
    "    \n",
    "    # Identify exact matches vs differences\n",
    "    exact_matches = (comparison_both['difference'] == 0).sum()\n",
    "    any_difference = (comparison_both['difference'] != 0).sum()\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"COMPARISON OF {len(comparison_both):,} MATCHING RECORDS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nExact matches:    {exact_matches:,} ({100*exact_matches/len(comparison_both):.2f}%)\")\n",
    "    print(f\"Any difference:   {any_difference:,} ({100*any_difference/len(comparison_both):.2f}%)\")\n",
    "    \n",
    "    if any_difference > 0:\n",
    "        print(f\"\\nDifference statistics:\")\n",
    "        print(f\"  Mean difference:     {comparison_both['difference'].mean():.4f}\")\n",
    "        print(f\"  Median difference:   {comparison_both['difference'].median():.4f}\")\n",
    "        print(f\"  Std deviation:       {comparison_both['difference'].std():.4f}\")\n",
    "        print(f\"  Max difference:      {comparison_both['difference'].max():.4f}\")\n",
    "        print(f\"  Min difference:      {comparison_both['difference'].min():.4f}\")\n",
    "        print(f\"  Mean abs difference: {comparison_both['abs_difference'].mean():.4f}\")\n",
    "        \n",
    "        print(f\"\\nDistribution of differences:\")\n",
    "        print(comparison_both['difference'].value_counts().sort_index().head(20))\n",
    "        \n",
    "        print(f\"\\nTop 10 largest positive differences (Polars > clifpy):\")\n",
    "        print(comparison_both.nlargest(10, 'difference')[\n",
    "            ['encounter_block', 'sofa_total_polars', 'sofa_total_clifpy', 'difference']\n",
    "        ])\n",
    "        \n",
    "        print(f\"\\nTop 10 largest negative differences (clifpy > Polars):\")\n",
    "        print(comparison_both.nsmallest(10, 'difference')[\n",
    "            ['encounter_block', 'sofa_total_polars', 'sofa_total_clifpy', 'difference']\n",
    "        ])\n",
    "        \n",
    "        # Breakdown by magnitude of difference\n",
    "        print(f\"\\nBreakdown by magnitude of difference:\")\n",
    "        print(f\"  Difference = 0:     {(comparison_both['abs_difference'] == 0).sum():,}\")\n",
    "        print(f\"  Difference < 1:     {(comparison_both['abs_difference'] < 1).sum():,}\")\n",
    "        print(f\"  Difference 1-2:     {((comparison_both['abs_difference'] >= 1) & (comparison_both['abs_difference'] < 2)).sum():,}\")\n",
    "        print(f\"  Difference 2-5:     {((comparison_both['abs_difference'] >= 2) & (comparison_both['abs_difference'] < 5)).sum():,}\")\n",
    "        print(f\"  Difference >= 5:    {(comparison_both['abs_difference'] >= 5).sum():,}\")\n",
    "        \n",
    "        # Save detailed comparison to file\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"Saving detailed comparison...\")\n",
    "        comparison_both.sort_values('abs_difference', ascending=False).to_csv(\n",
    "            'sofa_comparison_polars_vs_clifpy.csv', index=False\n",
    "        )\n",
    "        print(\"✅ Saved to: sofa_comparison_polars_vs_clifpy.csv\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No matching records found to compare!\")\n",
    "\n",
    "# Scatter plot comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(comparison_both) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(comparison_both['sofa_total_clifpy'], comparison_both['sofa_total_polars'], \n",
    "                    alpha=0.5, s=10)\n",
    "    axes[0].plot([0, 24], [0, 24], 'r--', label='Perfect agreement')\n",
    "    axes[0].set_xlabel('SOFA Total (clifpy)')\n",
    "    axes[0].set_ylabel('SOFA Total (Polars)')\n",
    "    axes[0].set_title('SOFA Score Comparison: Polars vs clifpy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Difference distribution\n",
    "    axes[1].hist(comparison_both['difference'], bins=50, edgecolor='black')\n",
    "    axes[1].axvline(0, color='red', linestyle='--', label='No difference')\n",
    "    axes[1].set_xlabel('Difference (Polars - clifpy)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Distribution of Differences')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sofa_comparison_plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n✅ Saved plot to: sofa_comparison_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038faf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_scores = sofa_scores_pandas.merge(\n",
    "    final_tableone_df[['encounter_block', 'death_enc']], \n",
    "    how='left', \n",
    "    on='encounter_block'\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#  Prepare the data\n",
    "# Group by SOFA score and calculate mortality rate and counts\n",
    "sofa_mortality = sofa_scores.groupby('sofa_total').agg({\n",
    "    'death_enc': ['mean', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "sofa_mortality.columns = ['sofa_score', 'mortality_rate', 'count']\n",
    "sofa_mortality['mortality_rate'] = sofa_mortality['mortality_rate'] * 100  # Convert to percentage\n",
    "\n",
    "# Step 2: Calculate confidence intervals (optional, for error bars)\n",
    "# Using Wilson score interval for binomial proportions\n",
    "def wilson_ci(successes, n, confidence=0.95):\n",
    "    from scipy import stats\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    p_hat = successes / n\n",
    "    denominator = 1 + z**2 / n\n",
    "    center = (p_hat + z**2 / (2*n)) / denominator\n",
    "    margin = z * np.sqrt((p_hat * (1 - p_hat) + z**2 / (4*n)) / n) / denominator\n",
    "    return center * 100, margin * 100\n",
    "\n",
    "# Calculate number of deaths per score\n",
    "sofa_mortality['deaths'] = (sofa_mortality['mortality_rate'] / 100) * sofa_mortality['count']\n",
    "\n",
    "# Calculate confidence intervals\n",
    "ci_data = [wilson_ci(deaths, n) if n > 0 else (0, 0) \n",
    "           for deaths, n in zip(sofa_mortality['deaths'], sofa_mortality['count'])]\n",
    "sofa_mortality['ci_center'] = [x[0] for x in ci_data]\n",
    "sofa_mortality['ci_margin'] = [x[1] for x in ci_data]\n",
    "\n",
    "# Step 3: Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Create bar chart\n",
    "bars = ax.bar(sofa_mortality['sofa_score'], \n",
    "              sofa_mortality['mortality_rate'],\n",
    "              color='#7FA8B8',  # Steel blue color similar to the image\n",
    "              edgecolor='black',\n",
    "              linewidth=0.5,\n",
    "              alpha=0.9)\n",
    "\n",
    "# Add error bars\n",
    "ax.errorbar(sofa_mortality['sofa_score'], \n",
    "            sofa_mortality['mortality_rate'],\n",
    "            yerr=sofa_mortality['ci_margin'],\n",
    "            fmt='none',\n",
    "            ecolor='black',\n",
    "            capsize=3,\n",
    "            capthick=1,\n",
    "            alpha=0.7)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('SOFA Score', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Mortality, %', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Mortality by SOFA Score (First 24hr of ICU admission)', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Add grid for readability\n",
    "ax.yaxis.grid(True, linestyle='-', alpha=0.3, color='gray')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Set x-axis ticks to show all SOFA scores\n",
    "ax.set_xticks(range(int(sofa_mortality['sofa_score'].min()), \n",
    "                    int(sofa_mortality['sofa_score'].max()) + 1))\n",
    "\n",
    "# Add count labels below x-axis\n",
    "counts_text = '\\n'.join([\n",
    "    'No. of patients per score',\n",
    "    '  '.join([f'{int(count)}' for count in sofa_mortality['count']])\n",
    "])\n",
    "\n",
    "# Create a second table-like annotation below the plot\n",
    "fig.text(0.1, -0.05, 'No. of patients per score', \n",
    "         ha='left', fontsize=10, weight='bold')\n",
    "\n",
    "# Add individual counts\n",
    "x_positions = np.linspace(0.15, 0.9, len(sofa_mortality))\n",
    "for i, (score, count) in enumerate(zip(sofa_mortality['sofa_score'], sofa_mortality['count'])):\n",
    "    if i < len(x_positions):\n",
    "        fig.text(x_positions[i], -0.08, f'{int(count)}', \n",
    "                ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)  # Make room for patient counts\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('../output/final/tableone/sofa_mortality_histogram.png', \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Prepare data for CSV export\n",
    "# Calculate lower and upper confidence interval bounds\n",
    "sofa_mortality['ci_lower'] = sofa_mortality['mortality_rate'] - sofa_mortality['ci_margin']\n",
    "sofa_mortality['ci_upper'] = sofa_mortality['mortality_rate'] + sofa_mortality['ci_margin']\n",
    "\n",
    "# Ensure CI bounds are within valid range [0, 100]\n",
    "sofa_mortality['ci_lower'] = sofa_mortality['ci_lower'].clip(lower=0)\n",
    "sofa_mortality['ci_upper'] = sofa_mortality['ci_upper'].clip(upper=100)\n",
    "sofa_mortality['total_encounters'] = sofa_scores['encounter_block'].nunique()\n",
    "# Create export dataframe with all relevant columns\n",
    "sofa_export = sofa_mortality[[\n",
    "    'sofa_score', \n",
    "    'total_encounters',\n",
    "    'count',\n",
    "    'deaths',\n",
    "    'mortality_rate', \n",
    "    'ci_lower',\n",
    "    'ci_upper',\n",
    "    'ci_margin'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "sofa_export.columns = [\n",
    "    'sofa_score',\n",
    "    'total_encounters',\n",
    "    'n_encounters',\n",
    "    'n_deaths',\n",
    "    'mortality_rate_percent',\n",
    "    'ci_lower_95',\n",
    "    'ci_upper_95',\n",
    "    'ci_margin_95'\n",
    "]\n",
    "\n",
    "# Round numeric columns for readability\n",
    "sofa_export['n_encounters'] = sofa_export['n_encounters'].astype(int)\n",
    "sofa_export['total_encounters'] = sofa_export['total_encounters'].astype(int)\n",
    "sofa_export['n_deaths'] = sofa_export['n_deaths'].round(0).astype(int)\n",
    "sofa_export['mortality_rate_percent'] = sofa_export['mortality_rate_percent'].round(2)\n",
    "sofa_export['ci_lower_95'] = sofa_export['ci_lower_95'].round(2)\n",
    "sofa_export['ci_upper_95'] = sofa_export['ci_upper_95'].round(2)\n",
    "sofa_export['ci_margin_95'] = sofa_export['ci_margin_95'].round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edbaa349",
   "metadata": {},
   "source": [
    "# SOFA 97 polars testing final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the local compute_sofa_polars function, supporting reload via importlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Add parent directory to path if not already there\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent  # Go up from code/archives to code/\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import and reload compute_sofa_polars from sofa_polars using importlib\n",
    "import sofa_polars_test\n",
    "importlib.reload(sofa_polars_test)\n",
    "compute_sofa_polars = sofa_polars_test.compute_sofa_polars\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare SOFA cohort for Polars computation\n",
    "print(\"Preparing SOFA cohort for Polars computation...\")\n",
    "sofa_cohort_pandas = final_tableone_df[final_tableone_df['icu_enc'] == 1][\n",
    "    ['hospitalization_id', 'encounter_block', 'first_icu_in_dttm']\n",
    "].copy()\n",
    "\n",
    "sofa_cohort_pandas['start_dttm'] = sofa_cohort_pandas['first_icu_in_dttm']\n",
    "sofa_cohort_pandas['end_dttm'] = sofa_cohort_pandas['start_dttm'] + pd.Timedelta(hours=24)\n",
    "\n",
    "# Convert to Polars\n",
    "sofa_cohort_pl = pl.from_pandas(\n",
    "    sofa_cohort_pandas[['hospitalization_id', 'encounter_block', 'start_dttm', 'end_dttm']]\n",
    ")\n",
    "\n",
    "print(f\"Cohort shape: {sofa_cohort_pl.shape}\")\n",
    "print(f\"Cohort columns: {sofa_cohort_pl.columns}\")\n",
    "\n",
    "# Compute SOFA scores using Polars\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Computing SOFA scores with Polars...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "sofa_scores_pl = compute_sofa_polars(\n",
    "    data_directory=config['tables_path'],\n",
    "    cohort_df=sofa_cohort_pl,\n",
    "    filetype=config['file_type'],\n",
    "    id_name='encounter_block',\n",
    "    extremal_type='worst',\n",
    "    fill_na_scores_with_zero=True,\n",
    "    remove_outliers=True,\n",
    "    timezone=config['timezone']\n",
    ")\n",
    "\n",
    "# Convert back to Pandas if needed for compatibility\n",
    "sofa_scores_pandas_test = sofa_scores_pl.to_pandas()\n",
    "\n",
    "print(f\"\\n✅ SOFA computation complete!\")\n",
    "print(f\"Result shape: {sofa_scores_pandas_test.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(sofa_scores_pandas_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SOFA scores between Polars and clifpy implementations\n",
    "import numpy as np\n",
    "\n",
    "# Merge the two dataframes on encounter_block\n",
    "comparison_df = sofa_scores_pandas_test[['encounter_block', 'sofa_total']].merge(\n",
    "    sofa_scores[['encounter_block', 'sofa_total']], \n",
    "    on='encounter_block', \n",
    "    how='outer',\n",
    "    suffixes=('_polars', '_clifpy')\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SOFA SCORE COMPARISON: Polars vs clifpy\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nTotal encounter_blocks:\")\n",
    "print(f\"  Polars:  {len(sofa_scores_pandas_test):,}\")\n",
    "print(f\"  clifpy:  {len(sofa_scores):,}\")\n",
    "print(f\"  Merged:  {len(comparison_df):,}\")\n",
    "\n",
    "# Check for missing matches\n",
    "only_polars = comparison_df['sofa_total_clifpy'].isna().sum()\n",
    "only_clifpy = comparison_df['sofa_total_polars'].isna().sum()\n",
    "both_present = (~comparison_df['sofa_total_polars'].isna() & ~comparison_df['sofa_total_clifpy'].isna()).sum()\n",
    "\n",
    "print(f\"\\nMatching:\")\n",
    "print(f\"  Both present:     {both_present:,}\")\n",
    "print(f\"  Only in Polars:   {only_polars:,}\")\n",
    "print(f\"  Only in clifpy:   {only_clifpy:,}\")\n",
    "\n",
    "# For records present in both, compare the values\n",
    "comparison_both = comparison_df[\n",
    "    ~comparison_df['sofa_total_polars'].isna() & \n",
    "    ~comparison_df['sofa_total_clifpy'].isna()\n",
    "].copy()\n",
    "\n",
    "if len(comparison_both) > 0:\n",
    "    # Calculate differences\n",
    "    comparison_both['difference'] = comparison_both['sofa_total_polars'] - comparison_both['sofa_total_clifpy']\n",
    "    comparison_both['abs_difference'] = comparison_both['difference'].abs()\n",
    "    \n",
    "    # Identify exact matches vs differences\n",
    "    exact_matches = (comparison_both['difference'] == 0).sum()\n",
    "    any_difference = (comparison_both['difference'] != 0).sum()\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"COMPARISON OF {len(comparison_both):,} MATCHING RECORDS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nExact matches:    {exact_matches:,} ({100*exact_matches/len(comparison_both):.2f}%)\")\n",
    "    print(f\"Any difference:   {any_difference:,} ({100*any_difference/len(comparison_both):.2f}%)\")\n",
    "    \n",
    "    if any_difference > 0:\n",
    "        print(f\"\\nDifference statistics:\")\n",
    "        print(f\"  Mean difference:     {comparison_both['difference'].mean():.4f}\")\n",
    "        print(f\"  Median difference:   {comparison_both['difference'].median():.4f}\")\n",
    "        print(f\"  Std deviation:       {comparison_both['difference'].std():.4f}\")\n",
    "        print(f\"  Max difference:      {comparison_both['difference'].max():.4f}\")\n",
    "        print(f\"  Min difference:      {comparison_both['difference'].min():.4f}\")\n",
    "        print(f\"  Mean abs difference: {comparison_both['abs_difference'].mean():.4f}\")\n",
    "        \n",
    "        print(f\"\\nDistribution of differences:\")\n",
    "        print(comparison_both['difference'].value_counts().sort_index().head(20))\n",
    "        \n",
    "        print(f\"\\nTop 10 largest positive differences (Polars > clifpy):\")\n",
    "        print(comparison_both.nlargest(10, 'difference')[\n",
    "            ['encounter_block', 'sofa_total_polars', 'sofa_total_clifpy', 'difference']\n",
    "        ])\n",
    "        \n",
    "        print(f\"\\nTop 10 largest negative differences (clifpy > Polars):\")\n",
    "        print(comparison_both.nsmallest(10, 'difference')[\n",
    "            ['encounter_block', 'sofa_total_polars', 'sofa_total_clifpy', 'difference']\n",
    "        ])\n",
    "        \n",
    "        # Breakdown by magnitude of difference\n",
    "        print(f\"\\nBreakdown by magnitude of difference:\")\n",
    "        print(f\"  Difference = 0:     {(comparison_both['abs_difference'] == 0).sum():,}\")\n",
    "        print(f\"  Difference < 1:     {(comparison_both['abs_difference'] < 1).sum():,}\")\n",
    "        print(f\"  Difference 1-2:     {((comparison_both['abs_difference'] >= 1) & (comparison_both['abs_difference'] < 2)).sum():,}\")\n",
    "        print(f\"  Difference 2-5:     {((comparison_both['abs_difference'] >= 2) & (comparison_both['abs_difference'] < 5)).sum():,}\")\n",
    "        print(f\"  Difference >= 5:    {(comparison_both['abs_difference'] >= 5).sum():,}\")\n",
    "        \n",
    "        # Save detailed comparison to file\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"Saving detailed comparison...\")\n",
    "        comparison_both.sort_values('abs_difference', ascending=False).to_csv(\n",
    "            'sofa_comparison_polars_vs_clifpy.csv', index=False\n",
    "        )\n",
    "        print(\"✅ Saved to: sofa_comparison_polars_vs_clifpy.csv\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No matching records found to compare!\")\n",
    "\n",
    "# Scatter plot comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(comparison_both) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(comparison_both['sofa_total_clifpy'], comparison_both['sofa_total_polars'], \n",
    "                    alpha=0.5, s=10)\n",
    "    axes[0].plot([0, 24], [0, 24], 'r--', label='Perfect agreement')\n",
    "    axes[0].set_xlabel('SOFA Total (clifpy)')\n",
    "    axes[0].set_ylabel('SOFA Total (Polars)')\n",
    "    axes[0].set_title('SOFA Score Comparison: Polars vs clifpy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Difference distribution\n",
    "    axes[1].hist(comparison_both['difference'], bins=50, edgecolor='black')\n",
    "    axes[1].axvline(0, color='red', linestyle='--', label='No difference')\n",
    "    axes[1].set_xlabel('Difference (Polars - clifpy)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Distribution of Differences')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sofa_comparison_plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n✅ Saved plot to: sofa_comparison_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'hospice_or_expired' to 'death_enc', if present\n",
    "if 'hospice_or_expired' in final_tableone_df.columns:\n",
    "    final_tableone_df = final_tableone_df.rename(columns={'hospice_or_expired': 'death_enc'})\n",
    "\n",
    "sofa_scores_2 = sofa_scores_pandas_test.merge(\n",
    "    final_tableone_df[['encounter_block', 'death_enc']], \n",
    "    how='left', \n",
    "    on='encounter_block'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#  Prepare the data\n",
    "# Group by SOFA score and calculate mortality rate and counts\n",
    "sofa_mortality = sofa_scores_2.groupby('sofa_total').agg({\n",
    "    'death_enc': ['mean', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "sofa_mortality.columns = ['sofa_score', 'mortality_rate', 'count']\n",
    "sofa_mortality['mortality_rate'] = sofa_mortality['mortality_rate'] * 100  # Convert to percentage\n",
    "\n",
    "# Step 2: Calculate confidence intervals (optional, for error bars)\n",
    "# Using Wilson score interval for binomial proportions\n",
    "def wilson_ci(successes, n, confidence=0.95):\n",
    "    from scipy import stats\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    p_hat = successes / n\n",
    "    denominator = 1 + z**2 / n\n",
    "    center = (p_hat + z**2 / (2*n)) / denominator\n",
    "    margin = z * np.sqrt((p_hat * (1 - p_hat) + z**2 / (4*n)) / n) / denominator\n",
    "    return center * 100, margin * 100\n",
    "\n",
    "# Calculate number of deaths per score\n",
    "sofa_mortality['deaths'] = (sofa_mortality['mortality_rate'] / 100) * sofa_mortality['count']\n",
    "\n",
    "# Calculate confidence intervals\n",
    "ci_data = [wilson_ci(deaths, n) if n > 0 else (0, 0) \n",
    "           for deaths, n in zip(sofa_mortality['deaths'], sofa_mortality['count'])]\n",
    "sofa_mortality['ci_center'] = [x[0] for x in ci_data]\n",
    "sofa_mortality['ci_margin'] = [x[1] for x in ci_data]\n",
    "\n",
    "# Step 3: Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Create bar chart\n",
    "bars = ax.bar(sofa_mortality['sofa_score'], \n",
    "              sofa_mortality['mortality_rate'],\n",
    "              color='#7FA8B8',  # Steel blue color similar to the image\n",
    "              edgecolor='black',\n",
    "              linewidth=0.5,\n",
    "              alpha=0.9)\n",
    "\n",
    "# Add error bars\n",
    "ax.errorbar(sofa_mortality['sofa_score'], \n",
    "            sofa_mortality['mortality_rate'],\n",
    "            yerr=sofa_mortality['ci_margin'],\n",
    "            fmt='none',\n",
    "            ecolor='black',\n",
    "            capsize=3,\n",
    "            capthick=1,\n",
    "            alpha=0.7)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('SOFA Score', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Mortality, %', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Mortality by SOFA Score (First 24hr of ICU admission)', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Add grid for readability\n",
    "ax.yaxis.grid(True, linestyle='-', alpha=0.3, color='gray')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Set x-axis ticks to show all SOFA scores\n",
    "ax.set_xticks(range(int(sofa_mortality['sofa_score'].min()), \n",
    "                    int(sofa_mortality['sofa_score'].max()) + 1))\n",
    "\n",
    "# Add count labels below x-axis\n",
    "counts_text = '\\n'.join([\n",
    "    'No. of patients per score',\n",
    "    '  '.join([f'{int(count)}' for count in sofa_mortality['count']])\n",
    "])\n",
    "\n",
    "# Create a second table-like annotation below the plot\n",
    "fig.text(0.1, -0.05, 'No. of patients per score', \n",
    "         ha='left', fontsize=10, weight='bold')\n",
    "\n",
    "# Add individual counts\n",
    "x_positions = np.linspace(0.15, 0.9, len(sofa_mortality))\n",
    "for i, (score, count) in enumerate(zip(sofa_mortality['sofa_score'], sofa_mortality['count'])):\n",
    "    if i < len(x_positions):\n",
    "        fig.text(x_positions[i], -0.08, f'{int(count)}', \n",
    "                ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)  # Make room for patient counts\n",
    "\n",
    "# Save the figure\n",
    "# plt.savefig('../output/final/tableone/sofa_mortality_histogram.png', \n",
    "#             dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Prepare data for CSV export\n",
    "# Calculate lower and upper confidence interval bounds\n",
    "sofa_mortality['ci_lower'] = sofa_mortality['mortality_rate'] - sofa_mortality['ci_margin']\n",
    "sofa_mortality['ci_upper'] = sofa_mortality['mortality_rate'] + sofa_mortality['ci_margin']\n",
    "\n",
    "# Ensure CI bounds are within valid range [0, 100]\n",
    "sofa_mortality['ci_lower'] = sofa_mortality['ci_lower'].clip(lower=0)\n",
    "sofa_mortality['ci_upper'] = sofa_mortality['ci_upper'].clip(upper=100)\n",
    "sofa_mortality['total_encounters'] = sofa_scores['encounter_block'].nunique()\n",
    "# Create export dataframe with all relevant columns\n",
    "sofa_export = sofa_mortality[[\n",
    "    'sofa_score', \n",
    "    'total_encounters',\n",
    "    'count',\n",
    "    'deaths',\n",
    "    'mortality_rate', \n",
    "    'ci_lower',\n",
    "    'ci_upper',\n",
    "    'ci_margin'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "sofa_export.columns = [\n",
    "    'sofa_score',\n",
    "    'total_encounters',\n",
    "    'n_encounters',\n",
    "    'n_deaths',\n",
    "    'mortality_rate_percent',\n",
    "    'ci_lower_95',\n",
    "    'ci_upper_95',\n",
    "    'ci_margin_95'\n",
    "]\n",
    "\n",
    "# Round numeric columns for readability\n",
    "sofa_export['n_encounters'] = sofa_export['n_encounters'].astype(int)\n",
    "sofa_export['total_encounters'] = sofa_export['total_encounters'].astype(int)\n",
    "sofa_export['n_deaths'] = sofa_export['n_deaths'].round(0).astype(int)\n",
    "sofa_export['mortality_rate_percent'] = sofa_export['mortality_rate_percent'].round(2)\n",
    "sofa_export['ci_lower_95'] = sofa_export['ci_lower_95'].round(2)\n",
    "sofa_export['ci_upper_95'] = sofa_export['ci_upper_95'].round(2)\n",
    "sofa_export['ci_margin_95'] = sofa_export['ci_margin_95'].round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bd7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a7e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53667e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d8b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6158a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join sofa_scores with final_tableone_df on 'encounter_block'\n",
    "final_tableone_df = final_tableone_df.merge(sofa_scores, on='encounter_block', how='left')\n",
    "final_tableone_df = final_tableone_df.drop(columns=['death_enc_y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cdf95c",
   "metadata": {},
   "source": [
    "# Outside of Table1- on whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3b44a",
   "metadata": {},
   "source": [
    "## Hospice Use vs Mortality Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "# ============================================================================\n",
    "# Prepare Comprehensive Hospice Trend Data\n",
    "# ============================================================================\n",
    "\n",
    "# Extract year from admission_dttm\n",
    "final_tableone_df['admission_year'] = final_tableone_df['admission_dttm'].dt.year\n",
    "\n",
    "# Create outcome variables\n",
    "final_tableone_df['hospice_outcome'] = (\n",
    "    final_tableone_df['discharge_category'].str.lower() == 'hospice'\n",
    ").astype(int)\n",
    "\n",
    "final_tableone_df['expired_outcome'] = (\n",
    "    final_tableone_df['discharge_category'].str.lower() == 'expired'\n",
    ").astype(int)\n",
    "\n",
    "final_tableone_df['hospice_or_expired'] = (\n",
    "    final_tableone_df['discharge_category'].str.lower().isin(['hospice', 'expired'])\n",
    ").astype(int)\n",
    "\n",
    "# ============================================================================\n",
    "# Aggregate All Metrics by Year (Single DataFrame)\n",
    "# ============================================================================\n",
    "\n",
    "hospice_trends = final_tableone_df.groupby('admission_year').agg({\n",
    "    'encounter_block': 'count',  # Total encounters\n",
    "    'hospice_outcome': 'sum',    # Hospice discharges\n",
    "    'expired_outcome': 'sum',    # Deaths\n",
    "    'hospice_or_expired': 'sum'  # Combined end-of-life\n",
    "}).reset_index()\n",
    "\n",
    "hospice_trends.columns = ['year', 'total_encounters', 'hospice', 'expired', 'hospice_or_expired']\n",
    "\n",
    "# Calculate percentages (of all encounters)\n",
    "hospice_trends['hospice_pct'] = (hospice_trends['hospice'] / hospice_trends['total_encounters'] * 100)\n",
    "hospice_trends['mortality_pct'] = (hospice_trends['expired'] / hospice_trends['total_encounters'] * 100)\n",
    "\n",
    "# Calculate hospice proportion among end-of-life patients\n",
    "hospice_trends['hospice_among_eol_pct'] = (\n",
    "    hospice_trends['hospice'] / hospice_trends['hospice_or_expired'] * 100\n",
    ")\n",
    "\n",
    "# Calculate Wilson score confidence intervals\n",
    "def calculate_ci(successes, n, confidence=0.95):\n",
    "    \"\"\"Calculate Wilson score confidence interval for proportions\"\"\"\n",
    "    if n == 0:\n",
    "        return 0, 0\n",
    "    ci_low, ci_upp = proportion_confint(successes, n, alpha=1-confidence, method='wilson')\n",
    "    return ci_low * 100, ci_upp * 100\n",
    "\n",
    "# CIs for hospice among end-of-life\n",
    "ci_results = [\n",
    "    calculate_ci(row['hospice'], row['hospice_or_expired'])\n",
    "    for _, row in hospice_trends.iterrows()\n",
    "]\n",
    "hospice_trends['hospice_among_eol_ci_lower'] = [x[0] for x in ci_results]\n",
    "hospice_trends['hospice_among_eol_ci_upper'] = [x[1] for x in ci_results]\n",
    "\n",
    "# ============================================================================\n",
    "# Save Results DataFrame\n",
    "# ============================================================================\n",
    "\n",
    "output_dir = '../output/final/tableone/'\n",
    "hospice_trends.to_csv(f'{output_dir}hospice_trends_summary.csv', index=False)\n",
    "print(f\"✅ Saved: {output_dir}hospice_trends_summary.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create Combined Figure\n",
    "# ============================================================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "x_labels = hospice_trends['year'].astype(str).tolist()\n",
    "x_pos = np.arange(len(x_labels))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TOP PANEL: Mortality vs Hospice Trends (% of all encounters)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "mortality_pct = hospice_trends['mortality_pct'].values\n",
    "hospice_pct = hospice_trends['hospice_pct'].values\n",
    "\n",
    "# Plot MORTALITY line\n",
    "ax1.plot(x_pos, mortality_pct, \n",
    "         marker='o', markersize=14, \n",
    "         color='#666666', \n",
    "         linewidth=3,\n",
    "         markerfacecolor='#666666',\n",
    "         markeredgecolor='black',\n",
    "         markeredgewidth=1,\n",
    "         label='MORTALITY',\n",
    "         zorder=3)\n",
    "\n",
    "# Plot HOSPICE line\n",
    "ax1.plot(x_pos, hospice_pct,\n",
    "         marker='s', markersize=12,\n",
    "         color='#000000',\n",
    "         linewidth=3,\n",
    "         markerfacecolor='#000000',\n",
    "         markeredgecolor='black',\n",
    "         markeredgewidth=1,\n",
    "         label='HOSPICE',\n",
    "         zorder=3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (mort, hosp) in enumerate(zip(mortality_pct, hospice_pct)):\n",
    "    ax1.text(i, mort + 0.2, f'{mort:.2f}%', \n",
    "             ha='center', va='bottom', \n",
    "             fontsize=11, fontweight='bold',\n",
    "             color='#333333')\n",
    "    \n",
    "    ax1.text(i, hosp - 0.2, f'{hosp:.2f}%',\n",
    "             ha='center', va='top',\n",
    "             fontsize=11, fontweight='bold',\n",
    "             color='#000000')\n",
    "\n",
    "# Add text labels\n",
    "mid_y_mortality = np.mean(mortality_pct)\n",
    "mid_y_hospice = np.mean(hospice_pct)\n",
    "\n",
    "ax1.text(len(x_pos)/2, mid_y_mortality + 0.1, 'MORTALITY', \n",
    "         fontsize=16, fontweight='normal',\n",
    "         color='#666666', ha='center',\n",
    "         style='italic')\n",
    "\n",
    "ax1.text(len(x_pos)/2, mid_y_hospice - 0.1, 'HOSPICE',\n",
    "         fontsize=16, fontweight='normal',\n",
    "         color='#000000', ha='center',\n",
    "         style='italic')\n",
    "\n",
    "# Customize axes\n",
    "ax1.set_ylabel('PERCENT OF ALL ENCOUNTERS', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(x_labels, fontsize=12, fontweight='bold')\n",
    "ax1.set_ylim(hospice_pct.min() - 0.5, mortality_pct.max() + 1)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['left'].set_linewidth(2)\n",
    "ax1.spines['bottom'].set_linewidth(2)\n",
    "ax1.yaxis.grid(True, linestyle=':', alpha=0.3, linewidth=1, color='gray')\n",
    "ax1.set_axisbelow(True)\n",
    "ax1.set_title('End-of-Life Outcomes Over Time', fontsize=16, fontweight='bold', pad=15)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# BOTTOM PANEL: Hospice Proportion Among End-of-Life Patients\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "hospice_among_eol_pct = hospice_trends['hospice_among_eol_pct'].values\n",
    "ci_lower = hospice_trends['hospice_among_eol_ci_lower'].values\n",
    "ci_upper = hospice_trends['hospice_among_eol_ci_upper'].values\n",
    "\n",
    "# Plot line with confidence interval\n",
    "ax2.plot(x_pos, hospice_among_eol_pct,\n",
    "         marker='D', markersize=12,\n",
    "         color='#2E86AB',\n",
    "         linewidth=3,\n",
    "         markerfacecolor='#2E86AB',\n",
    "         markeredgecolor='black',\n",
    "         markeredgewidth=1,\n",
    "         zorder=3)\n",
    "\n",
    "# Add confidence interval shading\n",
    "ax2.fill_between(x_pos, ci_lower, ci_upper,\n",
    "                 alpha=0.3, color='#2E86AB')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, pct in enumerate(hospice_among_eol_pct):\n",
    "    ax2.text(i, pct + 1.5, f'{pct:.1f}%',\n",
    "             ha='center', va='bottom',\n",
    "             fontsize=11, fontweight='bold',\n",
    "             color='#2E86AB')\n",
    "\n",
    "# Customize axes\n",
    "ax2.set_xlabel('YEAR', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax2.set_ylabel('PERCENT', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(x_labels, fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim(0, max(ci_upper) + 5)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['left'].set_linewidth(2)\n",
    "ax2.spines['bottom'].set_linewidth(2)\n",
    "ax2.yaxis.grid(True, linestyle=':', alpha=0.3, linewidth=1, color='gray')\n",
    "ax2.set_axisbelow(True)\n",
    "ax2.set_title('Hospice Discharge Among End-of-Life Patients\\n(Hospice / [Hospice + Expired])',\n",
    "             fontsize=16, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}hospice_mortality_combined_trends.png', \n",
    "            dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Hospice trends analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3d832",
   "metadata": {},
   "source": [
    "## CCI Hospice Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "# ============================================================================\n",
    "# Merge CCI Results with Final TableOne (if not already done)\n",
    "# ============================================================================\n",
    "# final_tableone_df['admission_year'] = final_tableone_df['admission_dttm'].dt.year\n",
    "# tableone_with_cci = final_tableone_df.merge(\n",
    "#     cci_results[['hospitalization_id', 'cci_score']], \n",
    "#     on='hospitalization_id', \n",
    "#     how='left'\n",
    "# )\n",
    "tableone_with_cci = final_tableone_df[['encounter_block','cci_score','admission_year',\n",
    "                                        'expired_outcome', 'hospice_or_expired', 'hospice_outcome']]\n",
    "# Create CCI Categories\n",
    "tableone_with_cci['cci_category'] = pd.cut(\n",
    "    tableone_with_cci['cci_score'],\n",
    "    bins=[-np.inf, 0, 2, 4, np.inf],\n",
    "    labels=['0 (No comorbidity)', '1-2 (Mild)', '3-4 (Moderate)', '5+ (Severe)']\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Create Comprehensive Summary by CCI Category and Year\n",
    "# ============================================================================\n",
    "\n",
    "cci_summary = tableone_with_cci.groupby(['admission_year', 'cci_category']).agg({\n",
    "    'encounter_block': 'count',\n",
    "    'hospice_outcome': 'sum',\n",
    "    'expired_outcome': 'sum',\n",
    "    'hospice_or_expired': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "cci_summary.columns = ['year', 'cci_category', 'total_encounters', \n",
    "                       'hospice_count', 'expired_count', 'hospice_or_expired_count']\n",
    "\n",
    "# Calculate all key metrics\n",
    "cci_summary['mortality_pct'] = (\n",
    "    cci_summary['expired_count'] / cci_summary['total_encounters'] * 100\n",
    ")\n",
    "cci_summary['hospice_pct'] = (\n",
    "    cci_summary['hospice_count'] / cci_summary['total_encounters'] * 100\n",
    ")\n",
    "cci_summary['combined_eol_pct'] = (\n",
    "    cci_summary['hospice_or_expired_count'] / cci_summary['total_encounters'] * 100\n",
    ")\n",
    "cci_summary['hospice_among_eol_pct'] = (\n",
    "    cci_summary['hospice_count'] / cci_summary['hospice_or_expired_count'] * 100\n",
    ")\n",
    "cci_summary['hospice_capture_rate'] = (\n",
    "    cci_summary['hospice_count'] / cci_summary['expired_count'] * 100\n",
    ")\n",
    "\n",
    "# Calculate confidence intervals for hospice among EOL\n",
    "def calculate_ci(row):\n",
    "    if row['hospice_or_expired_count'] == 0:\n",
    "        return pd.Series({'hospice_eol_ci_lower': np.nan, 'hospice_eol_ci_upper': np.nan})\n",
    "    ci_low, ci_upp = proportion_confint(\n",
    "        row['hospice_count'], row['hospice_or_expired_count'], \n",
    "        alpha=0.05, method='wilson'\n",
    "    )\n",
    "    return pd.Series({\n",
    "        'hospice_eol_ci_lower': ci_low * 100, \n",
    "        'hospice_eol_ci_upper': ci_upp * 100\n",
    "    })\n",
    "\n",
    "cci_summary[['hospice_eol_ci_lower', 'hospice_eol_ci_upper']] = cci_summary.apply(calculate_ci, axis=1)\n",
    "\n",
    "# Reorder columns for clarity\n",
    "cci_summary = cci_summary[[\n",
    "    'year', 'cci_category', 'total_encounters',\n",
    "    'expired_count', 'mortality_pct',\n",
    "    'hospice_count', 'hospice_pct',\n",
    "    'hospice_or_expired_count', 'combined_eol_pct',\n",
    "    'hospice_among_eol_pct', 'hospice_eol_ci_lower', 'hospice_eol_ci_upper',\n",
    "    'hospice_capture_rate'\n",
    "]]\n",
    "\n",
    "# Save comprehensive summary\n",
    "output_dir = '../output/final/tableone/'\n",
    "cci_summary.to_csv(f'{output_dir}cci_hospice_mortality_comprehensive_summary.csv', index=False)\n",
    "print(f\"✅ Saved: {output_dir}cci_hospice_mortality_comprehensive_summary.csv\")\n",
    "\n",
    "# Save the plotting data (\"data behind the figure\") to a separate CSV file\n",
    "# This replicates the data used for each panel in the grid:\n",
    "plot_data = []\n",
    "categories = ['0 (No comorbidity)', '1-2 (Mild)', '3-4 (Moderate)', '5+ (Severe)']\n",
    "\n",
    "for category in categories:\n",
    "    cat_data = cci_summary[cci_summary['cci_category'] == category].sort_values('year')\n",
    "    plot_data.append(\n",
    "        cat_data.assign(cci_category_label=category)\n",
    "    )\n",
    "\n",
    "plot_data_df = pd.concat(plot_data, axis=0)\n",
    "\n",
    "plot_data_df.to_csv(f'{output_dir}cci_mortality_hospice_trends_by_year_category_plotdata.csv', index=False)\n",
    "print(f\"✅ Saved plotting data for figure: {output_dir}cci_mortality_hospice_trends_by_year_category_plotdata.csv\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE CCI HOSPICE-MORTALITY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal records: {len(cci_summary)}\")\n",
    "print(f\"Years covered: {cci_summary['year'].min():.0f} - {cci_summary['year'].max():.0f}\")\n",
    "print(f\"CCI categories: {cci_summary['cci_category'].nunique()}\")\n",
    "print(f\"\\nTotal encounters across all years: {cci_summary['total_encounters'].sum():,}\")\n",
    "print(f\"Total deaths: {cci_summary['expired_count'].sum():,}\")\n",
    "print(f\"Total hospice discharges: {cci_summary['hospice_count'].sum():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create Unified Visualization: 2x2 Grid with Mortality vs Hospice\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors_mortality = '#666666'\n",
    "colors_hospice = '#000000'\n",
    "\n",
    "for i, (category, ax) in enumerate(zip(categories, axes)):\n",
    "    data = cci_summary[cci_summary['cci_category'] == category].sort_values('year')\n",
    "    \n",
    "    x_labels = data['year'].astype(str).tolist()\n",
    "    x_pos = np.arange(len(x_labels))\n",
    "    \n",
    "    mortality_pct = data['mortality_pct'].values\n",
    "    hospice_pct = data['hospice_pct'].values\n",
    "    \n",
    "    # Plot MORTALITY line\n",
    "    ax.plot(x_pos, mortality_pct, \n",
    "            marker='o', markersize=12, \n",
    "            color=colors_mortality, \n",
    "            linewidth=3,\n",
    "            markerfacecolor=colors_mortality,\n",
    "            markeredgecolor='black',\n",
    "            markeredgewidth=1,\n",
    "            label='MORTALITY',\n",
    "            zorder=3)\n",
    "    \n",
    "    # Plot HOSPICE line\n",
    "    ax.plot(x_pos, hospice_pct,\n",
    "            marker='s', markersize=10,\n",
    "            color=colors_hospice,\n",
    "            linewidth=3,\n",
    "            markerfacecolor=colors_hospice,\n",
    "            markeredgecolor='black',\n",
    "            markeredgewidth=1,\n",
    "            label='HOSPICE',\n",
    "            zorder=3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for j, (mort, hosp) in enumerate(zip(mortality_pct, hospice_pct)):\n",
    "        ax.text(j, mort + 0.5, f'{mort:.1f}%', \n",
    "                ha='center', va='bottom', \n",
    "                fontsize=9, fontweight='bold',\n",
    "                color='#333333')\n",
    "        \n",
    "        ax.text(j, hosp - 0.5, f'{hosp:.1f}%',\n",
    "                ha='center', va='top',\n",
    "                fontsize=9, fontweight='bold',\n",
    "                color='#000000')\n",
    "    \n",
    "    # Add text labels in plot area\n",
    "    mid_y_mortality = np.mean(mortality_pct)\n",
    "    mid_y_hospice = np.mean(hospice_pct)\n",
    "    \n",
    "    if mid_y_mortality - mid_y_hospice > 3:\n",
    "        ax.text(len(x_pos)/2, mid_y_mortality + 1, 'MORTALITY', \n",
    "                fontsize=13, fontweight='normal',\n",
    "                color='#666666', ha='center',\n",
    "                style='italic', alpha=0.7)\n",
    "        \n",
    "        ax.text(len(x_pos)/2, mid_y_hospice - 1, 'HOSPICE',\n",
    "                fontsize=13, fontweight='normal',\n",
    "                color='#000000', ha='center',\n",
    "                style='italic', alpha=0.7)\n",
    "    \n",
    "    # Customize axes\n",
    "    ax.set_ylabel('PERCENT OF ALL ENCOUNTERS', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('YEAR', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'CCI: {category}', fontsize=14, fontweight='bold', pad=10)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(x_labels, fontsize=10, fontweight='bold')\n",
    "    \n",
    "    y_max = max(mortality_pct.max(), hospice_pct.max())\n",
    "    ax.set_ylim(0, y_max + 3)\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    \n",
    "    ax.yaxis.grid(True, linestyle=':', alpha=0.3, linewidth=1, color='gray')\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.legend(loc='upper left', fontsize=10, frameon=True, \n",
    "                 fancybox=True, shadow=True)\n",
    "\n",
    "plt.suptitle('Mortality vs Hospice Trends by Comorbidity Burden', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}cci_mortality_hospice_comprehensive.png', \n",
    "            dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93846e80",
   "metadata": {},
   "source": [
    "# TableOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = final_tableone_df.columns\n",
    "for x in cols:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0861a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIMIZED: Prepare Data and Create Table One\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Prepare final_tableone_df for Table One (Optimized)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARING DATA FOR TABLE ONE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nOriginal final_tableone_df shape: {final_tableone_df.shape}\")\n",
    "print(f\"Unique encounter_blocks: {final_tableone_df['encounter_block'].nunique()}\")\n",
    "\n",
    "# ✅ OPTIMIZATION: Check and deduplicate in one step\n",
    "duplicates = final_tableone_df['encounter_block'].duplicated().sum()\n",
    "print(f\"Duplicate encounter_blocks: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\n⚠️  Multiple rows per encounter_block found. Keeping last value...\")\n",
    "    tableone_df = final_tableone_df.drop_duplicates(subset=['encounter_block'], keep='last')\n",
    "else:\n",
    "    tableone_df = final_tableone_df\n",
    "\n",
    "print(f\"\\nFinal tableone_df shape: {tableone_df.shape}\")\n",
    "\n",
    "# ✅ OPTIMIZATION: Create patient demographics once, reuse throughout\n",
    "patient_df = tableone_df[['patient_id', 'race_category', 'ethnicity_category', 'sex_category', 'age_at_admission']].drop_duplicates('patient_id')\n",
    "print(f\"Unique patients: {len(patient_df):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: OPTIMIZED Table One Generation Function\n",
    "# ============================================================================\n",
    "\n",
    "def make_table_one_optimized(df, patient_demographics, id_col='encounter_block'):\n",
    "    \"\"\"\n",
    "    Optimized Table One generation - pre-computes values, minimizes iterations.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    # ✅ PRE-COMPUTE: Common values used throughout\n",
    "    N_enc = len(df)\n",
    "    N_pat = len(patient_demographics)\n",
    "    \n",
    "    # ✅ OPTIMIZATION: Compute all summations once\n",
    "    flag_sums = {}\n",
    "    for col in df.columns:\n",
    "        if col.endswith('_flag') or col in ['icu_enc', 'death_enc', 'high_support_enc', \n",
    "                                             'vaso_support_enc', 'other_critically_ill', \n",
    "                                             'on_vent', 'on_crrt', 'hospice_outcome', \n",
    "                                             'expired_outcome']:\n",
    "            if col in df.columns:\n",
    "                flag_sums[col] = df[col].sum()\n",
    "    \n",
    "    # ✅ OPTIMIZATION: Compute all medians/quantiles once for continuous vars\n",
    "    continuous_stats = {}\n",
    "    for col in ['age_at_admission', 'first_icu_los_days', 'hospital_length_of_stay_days', 'cci_score',\n",
    "                'p_f', 'p_f_imputed', 'sofa_cv_97', 'sofa_coag', 'sofa_liver', \n",
    "                'sofa_resp', 'sofa_cns', 'sofa_renal', 'sofa_total']:\n",
    "        if col in df.columns:\n",
    "            data = df[col].dropna() if col != 'age_at_admission' else patient_demographics['age_at_admission'].dropna()\n",
    "            if len(data) > 0:\n",
    "                continuous_stats[col] = {\n",
    "                    'median': data.median(),\n",
    "                    'q1': data.quantile(0.25),\n",
    "                    'q3': data.quantile(0.75)\n",
    "                }\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. Sample Size\n",
    "    # -------------------------------------------------------------------------\n",
    "    rows.append((\"N: Encounter blocks\", f\"{N_enc:,}\"))\n",
    "    rows.append((\"N: Unique patients\", f\"{N_pat:,}\"))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. Demographics\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Age\n",
    "    if 'age_at_admission' in continuous_stats:\n",
    "        s = continuous_stats['age_at_admission']\n",
    "        rows.append((\"Age at admission, median [Q1, Q3]\",\n",
    "                     f\"{s['median']:.0f} [{s['q1']:.0f}, {s['q3']:.0f}]\"))\n",
    "    \n",
    "    # ✅ OPTIMIZATION: Vectorized categorical function with pre-computed denominator\n",
    "    def cat_n_pct_fast(data, col, title, denominator):\n",
    "        vc = data[col].value_counts(dropna=False)\n",
    "        for lvl, cnt in vc.items():\n",
    "            pct = 100 * cnt / denominator\n",
    "            lvl_str = str(lvl) if pd.notna(lvl) else 'Missing'\n",
    "            rows.append((f\"  {title}: {lvl_str}\", f\"{cnt:,} ({pct:.1f}%)\"))\n",
    "    \n",
    "    cat_n_pct_fast(patient_demographics, 'race_category', 'Race', N_pat)\n",
    "    cat_n_pct_fast(patient_demographics, 'ethnicity_category', 'Ethnicity', N_pat)\n",
    "    cat_n_pct_fast(patient_demographics, 'sex_category', 'Sex', N_pat)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. Encounter Types (use pre-computed sums)\n",
    "    # -------------------------------------------------------------------------\n",
    "    rows.append((\"Encounter Types\", \"\"))\n",
    "    \n",
    "    for flag, label in [\n",
    "        ('icu_enc', 'ICU encounters'),\n",
    "        ('high_support_enc', 'Advanced respiratory support'),\n",
    "        ('vaso_support_enc', 'Vasoactive support'),\n",
    "        ('other_critically_ill', 'Other critically ill')\n",
    "    ]:\n",
    "        if flag in flag_sums:\n",
    "            n = flag_sums[flag]\n",
    "            rows.append((f\"  {label}, n (%)\", f\"{n:,} ({100*n/N_enc:.1f}%)\"))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. Mortality (use pre-computed sums)\n",
    "    # -------------------------------------------------------------------------\n",
    "    if 'death_enc' in flag_sums:\n",
    "        mort_n = flag_sums['death_enc']\n",
    "        rows.append((\"Hospital mortality, n (%)\", f\"{mort_n:,} ({100*mort_n/N_enc:.1f}%)\"))\n",
    "        \n",
    "        for flag, label in [('hospice_outcome', 'Discharged to hospice'),\n",
    "                            ('expired_outcome', 'Expired')]:\n",
    "            if flag in flag_sums:\n",
    "                n = flag_sums[flag]\n",
    "                rows.append((f\"  {label}, n (%)\", f\"{n:,} ({100*n/N_enc:.1f}%)\"))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5. Admission and Location\n",
    "    # -------------------------------------------------------------------------\n",
    "    cat_n_pct_fast(df, 'first_admission_location', 'First admission location', N_enc)\n",
    "    if 'admission_type_category' in df.columns:\n",
    "        cat_n_pct_fast(df, 'admission_type_category', 'Admission type', N_enc)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6. Length of Stay (use pre-computed stats)\n",
    "    # -------------------------------------------------------------------------\n",
    "    for col, label in [('first_icu_los_days', 'ICU length of stay (days)'),\n",
    "                       ('hospital_length_of_stay_days', 'Hospital length of stay (days)')]:\n",
    "        if col in continuous_stats:\n",
    "            s = continuous_stats[col]\n",
    "            rows.append((f\"{label}, median [Q1, Q3]\",\n",
    "                        f\"{s['median']:.1f} [{s['q1']:.1f}, {s['q3']:.1f}]\"))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7. Comorbidities\n",
    "    # -------------------------------------------------------------------------\n",
    "    if 'cci_score' in continuous_stats:\n",
    "        s = continuous_stats['cci_score']\n",
    "        rows.append((\"Charlson Comorbidity Index, median [Q1, Q3]\",\n",
    "                     f\"{s['median']:.0f} [{s['q1']:.0f}, {s['q3']:.0f}]\"))\n",
    "    \n",
    "    # ✅ OPTIMIZATION: Get all comorbidity columns at once\n",
    "    comorbidities = [\n",
    "        'myocardial_infarction', 'congestive_heart_failure', 'peripheral_vascular_disease',\n",
    "        'cerebrovascular_disease', 'dementia', 'chronic_pulmonary_disease',\n",
    "        'connective_tissue_disease', 'peptic_ulcer_disease', 'mild_liver_disease',\n",
    "        'diabetes_uncomplicated', 'diabetes_with_complications', 'hemiplegia',\n",
    "        'renal_disease', 'cancer', 'moderate_severe_liver_disease',\n",
    "        'metastatic_solid_tumor', 'aids'\n",
    "    ]\n",
    "    \n",
    "    comorb_present = [c for c in comorbidities if c in df.columns]\n",
    "    if comorb_present:\n",
    "        # ✅ OPTIMIZATION: Sum all at once with one operation\n",
    "        comorb_sums = df[comorb_present].sum()\n",
    "        comorb_with_counts = comorb_sums[comorb_sums > 0].sort_values(ascending=False)\n",
    "        \n",
    "        if len(comorb_with_counts) > 0:\n",
    "            rows.append((\"Comorbidities, n (%)\", \"\"))\n",
    "            for comorb, n in comorb_with_counts.items():\n",
    "                name = comorb.replace('_', ' ').title()\n",
    "                rows.append((f\"  {name}\", f\"{int(n):,} ({100*n/N_enc:.1f}%)\"))\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8. SOFA Scores\n",
    "    # -------------------------------------------------------------------------\n",
    "    rows.append((\"SOFA Scores\", \"\"))\n",
    "    \n",
    "    # Total SOFA\n",
    "    if 'sofa_total' in continuous_stats:\n",
    "        s = continuous_stats['sofa_total']\n",
    "        rows.append((\"  Total SOFA score, median [Q1, Q3]\",\n",
    "                     f\"{s['median']:.1f} [{s['q1']:.1f}, {s['q3']:.1f}]\"))\n",
    "    \n",
    "    # Individual SOFA components\n",
    "    sofa_components = [\n",
    "        ('sofa_resp', 'Respiratory'),\n",
    "        ('sofa_coag', 'Coagulation'),\n",
    "        ('sofa_liver', 'Liver'),\n",
    "        ('sofa_cv_97', 'Cardiovascular'),\n",
    "        ('sofa_cns', 'CNS'),\n",
    "        ('sofa_renal', 'Renal')\n",
    "    ]\n",
    "    \n",
    "    for col, label in sofa_components:\n",
    "        if col in continuous_stats:\n",
    "            s = continuous_stats[col]\n",
    "            rows.append((f\"    {label}, median [Q1, Q3]\",\n",
    "                        f\"{s['median']:.1f} [{s['q1']:.1f}, {s['q3']:.1f}]\"))\n",
    "    \n",
    "    # P/F ratio (optional, if you want to include it)\n",
    "    if 'p_f' in continuous_stats:\n",
    "        s = continuous_stats['p_f']\n",
    "        rows.append((\"  P/F ratio, median [Q1, Q3]\",\n",
    "                     f\"{s['median']:.0f} [{s['q1']:.0f}, {s['q3']:.0f}]\"))\n",
    "    \n",
    "    if 'p_f_imputed' in continuous_stats:\n",
    "        s = continuous_stats['p_f_imputed']\n",
    "        rows.append((\"  P/F ratio (imputed), median [Q1, Q3]\",\n",
    "                     f\"{s['median']:.0f} [{s['q1']:.0f}, {s['q3']:.0f}]\"))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8. CRRT\n",
    "    # -------------------------------------------------------------------------\n",
    "    if 'on_crrt' in flag_sums:\n",
    "        crrt_n = flag_sums['on_crrt']\n",
    "        rows.append((\"CRRT, n (%)\", f\"{crrt_n:,} ({100*crrt_n/N_enc:.1f}%)\"))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9. Mechanical Ventilation\n",
    "    # -------------------------------------------------------------------------\n",
    "    imv_n = flag_sums.get('on_vent', 0)\n",
    "    rows.append((\"Invasive mechanical ventilation, n (%)\", f\"{imv_n:,} ({100*imv_n/N_enc:.1f}%)\"))\n",
    "    \n",
    "    if imv_n > 0:\n",
    "        # ✅ OPTIMIZATION: Filter once, reuse\n",
    "        imv_subset = df[df['on_vent'] == 1]\n",
    "        \n",
    "        cat_n_pct_fast(imv_subset, 'first_location_imv', 'First location at IMV start', imv_n)\n",
    "        cat_n_pct_fast(imv_subset, 'initial_mode_category', 'Initial ventilator mode', imv_n)\n",
    "        \n",
    "        # ✅ OPTIMIZATION: Compute all vent settings at once\n",
    "        vent_settings = {\n",
    "            'fio2_set': 'FiO2 (%)',\n",
    "            'peep_set': 'PEEP (cmH2O)',\n",
    "            'resp_rate_set': 'Respiratory rate (breaths/min)',\n",
    "            'tidal_volume_set': 'Tidal volume (mL)'\n",
    "        }\n",
    "        \n",
    "        for setting, label in vent_settings.items():\n",
    "            med_col = f'{setting}_median'\n",
    "            if med_col in imv_subset.columns:\n",
    "                # ✅ OPTIMIZATION: Calculate all quantiles in one go\n",
    "                stats = imv_subset[[med_col, f'{setting}_q1', f'{setting}_q3']].median()\n",
    "                rows.append((f\"  {label}, median [Q1, Q3]\",\n",
    "                            f\"{stats[0]:.1f} [{stats[1]:.1f}, {stats[2]:.1f}]\"))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10. Vasopressors\n",
    "    # -------------------------------------------------------------------------\n",
    "    vaso_flags = {\n",
    "        'norepinephrine': 'Norepinephrine',\n",
    "        'epinephrine': 'Epinephrine',\n",
    "        'phenylephrine': 'Phenylephrine',\n",
    "        'vasopressin': 'Vasopressin',\n",
    "        'dopamine': 'Dopamine'\n",
    "    }\n",
    "    \n",
    "    # ✅ OPTIMIZATION: Check if any vasopressor flags exist first\n",
    "    vaso_flag_cols = [f'{v}_flag' for v in vaso_flags.keys()]\n",
    "    vaso_flags_present = [c for c in vaso_flag_cols if c in df.columns]\n",
    "    \n",
    "    if vaso_flags_present:\n",
    "        # ✅ OPTIMIZATION: Compute any_vasopressor using max (vectorized)\n",
    "        vaso_enc_n = df[vaso_flags_present].max(axis=1).sum()\n",
    "        rows.append((\"Vasopressor encounters, n (%)\", f\"{vaso_enc_n:,} ({100*vaso_enc_n/N_enc:.1f}%)\"))\n",
    "        \n",
    "        for vaso, name in vaso_flags.items():\n",
    "            flag_col = f'{vaso}_flag'\n",
    "            if flag_col in flag_sums:\n",
    "                n = flag_sums[flag_col]\n",
    "                rows.append((f\"  {name}, n (%)\", f\"{n:,} ({100*n/N_enc:.1f}%)\"))\n",
    "                \n",
    "                # Dose statistics\n",
    "                if f'{vaso}_median' in df.columns and n > 0:\n",
    "                    vaso_subset = df[df[flag_col] == 1]\n",
    "                    stats = vaso_subset[[f'{vaso}_median', f'{vaso}_q1', f'{vaso}_q3']].median()\n",
    "                    rows.append((f\"    {name} dose (mcg/kg/min), median [Q1, Q3]\",\n",
    "                                f\"{stats[0]:.2f} [{stats[1]:.2f}, {stats[2]:.2f}]\"))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11. Sedatives and Analgesics (use pre-computed sums)\n",
    "    # -------------------------------------------------------------------------\n",
    "    sedation_meds = {\n",
    "        'propofol_flag': 'Propofol',\n",
    "        'midazolam_flag': 'Midazolam',\n",
    "        'lorazepam_flag': 'Lorazepam',\n",
    "        'dexmedetomidine_flag': 'Dexmedetomidine',\n",
    "        'fentanyl_flag': 'Fentanyl'\n",
    "    }\n",
    "    \n",
    "    sed_present = {k: v for k, v in sedation_meds.items() if k in flag_sums}\n",
    "    if sed_present:\n",
    "        rows.append((\"Sedatives and analgesics, n (%)\", \"\"))\n",
    "        for flag_col, name in sed_present.items():\n",
    "            n = flag_sums[flag_col]\n",
    "            rows.append((f\"  {name}\", f\"{n:,} ({100*n/N_enc:.1f}%)\"))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12. Neuromuscular Blocking Agents (use pre-computed sums)\n",
    "    # -------------------------------------------------------------------------\n",
    "    nmba_meds = {\n",
    "        'cisatracurium_flag': 'Cisatracurium',\n",
    "        'rocuronium_flag': 'Rocuronium'\n",
    "    }\n",
    "    \n",
    "    nmba_present = {k: v for k, v in nmba_meds.items() if k in flag_sums}\n",
    "    if nmba_present:\n",
    "        rows.append((\"Neuromuscular blocking agents, n (%)\", \"\"))\n",
    "        for flag_col, name in nmba_present.items():\n",
    "            n = flag_sums[flag_col]\n",
    "            rows.append((f\"  {name}\", f\"{n:,} ({100*n/N_enc:.1f}%)\"))\n",
    "    \n",
    "    # Assemble DataFrame\n",
    "    return pd.DataFrame(rows, columns=[\"Variable\", \"Overall\"])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Generate Table One (Optimized)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING TABLE ONE (OPTIMIZED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate overall table\n",
    "tbl_overall = make_table_one_optimized(tableone_df, patient_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE ONE - OVERALL\")\n",
    "print(\"=\"*80)\n",
    "print(tbl_overall.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "tbl_overall.to_csv('../output/final/tableone/table_one_overall.csv', index=False)\n",
    "print(f\"\\n✅ Saved: ../output/final/tableone/table_one_overall.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Generate by Year (Optimized)\n",
    "# ============================================================================\n",
    "\n",
    "if 'admission_year' in tableone_df.columns:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING TABLE ONE BY YEAR (OPTIMIZED)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ✅ OPTIMIZATION: Get unique years once, sort once\n",
    "    years = sorted(tableone_df['admission_year'].dropna().unique())\n",
    "    print(f\"Years found: {years}\")\n",
    "    \n",
    "    var_order = tbl_overall[\"Variable\"].tolist()\n",
    "    results = {\"Overall\": tbl_overall.set_index(\"Variable\")[\"Overall\"]}\n",
    "    \n",
    "    # ✅ OPTIMIZATION: Group once, iterate over groups\n",
    "    for yr in years:\n",
    "        # Filter year data\n",
    "        df_year = tableone_df[tableone_df[\"admission_year\"] == yr]\n",
    "        pat_year = patient_df[patient_df['patient_id'].isin(df_year['patient_id'])]\n",
    "        \n",
    "        # Generate table for this year\n",
    "        tbl_year = make_table_one_optimized(df_year, pat_year)\n",
    "        results[str(int(yr))] = tbl_year.set_index(\"Variable\")[\"Overall\"]\n",
    "    \n",
    "    # Create wide DataFrame\n",
    "    table_by_year = (\n",
    "        pd.DataFrame(results)\n",
    "        .reindex(var_order)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"Variable\"})\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TABLE ONE - BY YEAR\")\n",
    "    print(\"=\"*80)\n",
    "    print(table_by_year.to_string(index=False))\n",
    "    \n",
    "    # Save\n",
    "    table_by_year.to_csv('../output/final/tableone/table_one_by_year.csv', index=False)\n",
    "    print(f\"\\n✅ Saved: ../output/final/tableone/table_one_by_year.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ TABLE ONE GENERATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42adb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tableone_df.to_parquet('../output/intermediate/final_tableone_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a91b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "del patient_df\n",
    "del adt_df\n",
    "del hosp_df\n",
    "del all_encounters\n",
    "del advanced_support_df\n",
    "del all_meds\n",
    "del closest_adt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
