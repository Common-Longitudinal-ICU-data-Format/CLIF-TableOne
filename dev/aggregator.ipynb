{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a259ef94",
   "metadata": {},
   "source": [
    "# Table One Overall Aggregator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_FOLDER_PATH = 'path/to/box'\n",
    "\n",
    "# Convert to Path object\n",
    "root_dir = Path(BOX_FOLDER_PATH).expanduser()\n",
    "\n",
    "# Verify the path exists\n",
    "if not root_dir.exists():\n",
    "    raise FileNotFoundError(f\"Path does not exist: {root_dir}\")\n",
    "\n",
    "print(f\"Searching in: {root_dir.absolute()}\")\n",
    "print()\n",
    "\n",
    "# Find all table_one_by_year.csv files\n",
    "csv_files = list(root_dir.glob('**/table_one_overall.csv'))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for file in csv_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b414459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store dataframes\n",
    "dfs = {}\n",
    "\n",
    "# Read each CSV and store with folder name\n",
    "for csv_file in csv_files:\n",
    "    # Get part after 'CLIF-TableOne-2025/'\n",
    "    parts = csv_file.parts\n",
    "    try:\n",
    "        clif_idx = parts.index('CLIF-TableOne-2025')\n",
    "        folder_name = parts[clif_idx + 1]\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"'CLIF-TableOne-2025' not found in path: {csv_file}\")\n",
    "    except IndexError:\n",
    "        raise ValueError(f\"No folder after 'CLIF-TableOne-2025' in path: {csv_file}\")\n",
    "\n",
    "    # Read the CSV\n",
    "    df = pl.read_csv(csv_file)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    dfs[folder_name] = df\n",
    "    print(f\"Loaded {folder_name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80884388",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_dfs = []\n",
    "\n",
    "for folder_name, df in dfs.items():\n",
    "    overall_col = None\n",
    "    for c in df.columns:\n",
    "        if \"Overall\" in c:\n",
    "            overall_col = c\n",
    "            break\n",
    "    df_overall = df[['Variable', overall_col]].clone()\n",
    "    # Properly rename the column using a dictionary.\n",
    "    df_overall = df_overall.rename({overall_col: f\"{folder_name}\"})\n",
    "    overall_dfs.append(df_overall)\n",
    "    print(f\"Extracted from {folder_name}: {df_overall.shape}\")\n",
    "\n",
    "# Convert all values in the 'Variable' column to lower case for each overall_df\n",
    "for i in range(len(overall_dfs)):\n",
    "    if 'Variable' in overall_dfs[i].columns:\n",
    "        overall_dfs[i] = overall_dfs[i].with_columns(\n",
    "            pl.col('Variable').str.to_lowercase().alias('Variable')\n",
    "        )\n",
    "\n",
    "unaggregated = overall_dfs[0]\n",
    "for df_overall in overall_dfs[1:]:\n",
    "    # Only the 'Variable' column will match; all other columns are uniquely named\n",
    "    unaggregated = unaggregated.join(df_overall, on=\"Variable\", how=\"left\")\n",
    "\n",
    "unaggregated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd7aeb",
   "metadata": {},
   "source": [
    "# Aggregate consortium results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413349ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_numeric(value):\n",
    "    \"\"\"\n",
    "    Extract numeric value at start of string before '(' if pattern matches, else return None.\n",
    "    Example: '164 (0.1%)' -> 164\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        match = re.match(r\"^\\s*([-\\d,\\.]+)\\s*(?:\\([^\\)]*\\))?\\s*$\", value)\n",
    "        if match:\n",
    "            num_str = match.group(1).replace(\",\", \"\")\n",
    "            try:\n",
    "                # Some values may be decimal floats, just cast to float first\n",
    "                return float(num_str)\n",
    "            except:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def value_has_iqr(value):\n",
    "    # Check for pattern [something, something]\n",
    "    if not isinstance(value, str):\n",
    "        return False\n",
    "    return bool(re.search(r\"\\[.*?,.*?\\]\", value))\n",
    "\n",
    "def aggregate_overalls(df):\n",
    "    # Get all columns that are site columns (not 'Variable')\n",
    "    site_cols = [c for c in df.columns if c != \"Variable\"]\n",
    "\n",
    "    # First pass: calculate all numeric sums\n",
    "    numeric_sums = {}\n",
    "    for row in df.iter_rows(named=True):\n",
    "        variable = row['Variable']\n",
    "        values = [row[c] for c in site_cols]\n",
    "\n",
    "        # If any value has [min, max] (IQR), skip aggregation\n",
    "        if any(value_has_iqr(val) for val in values):\n",
    "            numeric_sums[variable] = None\n",
    "            continue\n",
    "\n",
    "        # For columns with values like \"164 (0.1%)\" or '68,359 (57.6%)'\n",
    "        numbers = []\n",
    "        for val in values:\n",
    "            num = get_numeric(val)\n",
    "            if num is not None:\n",
    "                numbers.append(num)\n",
    "        if numbers:\n",
    "            s = int(sum(numbers))\n",
    "            numeric_sums[variable] = s\n",
    "        else:\n",
    "            numeric_sums[variable] = None\n",
    "\n",
    "    # Get denominators\n",
    "    denominator_patients = numeric_sums.get(\"n: unique patients\")\n",
    "    denominator_encounters = numeric_sums.get(\"n: encounter blocks\")\n",
    "\n",
    "    # Define variables that should NOT have percentages\n",
    "    no_percentage_vars = [\n",
    "        \"n: encounter blocks\",\n",
    "        \"n: unique patients\",\n",
    "        \"n: hospitals\"\n",
    "    ]\n",
    "\n",
    "    # Define variables that use patients as denominator\n",
    "    mortality_vars = [\n",
    "        \"hospital mortality, n (%)\",\n",
    "        \"discharged to hospice, n (%)\",\n",
    "        \"expired, n (%)\"\n",
    "    ]\n",
    "\n",
    "    # Second pass: format with percentages\n",
    "    values_cons = []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        variable = row['Variable']\n",
    "        value = numeric_sums[variable]\n",
    "\n",
    "        if value is None:\n",
    "            values_cons.append(None)\n",
    "            continue\n",
    "\n",
    "        # Skip percentage for denominator variables themselves\n",
    "        if variable in no_percentage_vars:\n",
    "            formatted = f\"{value:,}\"\n",
    "            values_cons.append(formatted)\n",
    "            continue\n",
    "\n",
    "        # Determine denominator: patients or encounters?\n",
    "        use_patients_denom = (\n",
    "            any(term in variable.lower() for term in [\"race:\", \"ethnicity:\", \"sex:\"]) or\n",
    "            variable in mortality_vars\n",
    "        )\n",
    "\n",
    "        if use_patients_denom:\n",
    "            denominator = denominator_patients\n",
    "        else:\n",
    "            denominator = denominator_encounters\n",
    "\n",
    "        # Calculate percentage if denominator exists\n",
    "        if denominator and denominator > 0:\n",
    "            percentage = (value / denominator) * 100\n",
    "            formatted = f\"{value:,} ({percentage:.1f}%)\"\n",
    "        else:\n",
    "            formatted = f\"{value:,}\"\n",
    "\n",
    "        values_cons.append(formatted)\n",
    "\n",
    "    # Create a new polars Series/column for consortium\n",
    "    unaggregated_with_cons = df.with_columns(\n",
    "        pl.Series(\"consortium\", values_cons)\n",
    "    )\n",
    "    return unaggregated_with_cons\n",
    "\n",
    "\n",
    "consortium_overalls = aggregate_overalls(unaggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce239de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_median_q1_q3(value):\n",
    "    \"\"\"\n",
    "    Extracts median, q1, q3 from a value string formatted as 'median [q1, q3]'.\n",
    "    Returns a tuple of floats (median, q1, q3), or None if not applicable.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, str):\n",
    "        return None\n",
    "    match = re.match(r'^\\s*([-\\d\\.]+)\\s*\\[\\s*([-\\d\\.]+)\\s*,\\s*([-\\d\\.]+)\\s*\\]', value)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1)), float(match.group(2)), float(match.group(3))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def aggregate_median_q1_q3(df):\n",
    "    site_cols = [c for c in df.columns if c not in (\"Variable\", \"consortium\")]\n",
    "    cons_values = []\n",
    "    \n",
    "    for row in df.iter_rows(named=True):\n",
    "        variable = row['Variable']\n",
    "        values = [row[c] for c in site_cols]\n",
    "        # Filter to only those with median[q1, q3] format\n",
    "        extracted = [extract_median_q1_q3(val) for val in values]\n",
    "        non_null = [r for r in extracted if r is not None]\n",
    "        if non_null:\n",
    "            # Calculate median of medians, min of q1, max of q3\n",
    "            medians = [v[0] for v in non_null]\n",
    "            q1s = [v[1] for v in non_null]\n",
    "            q3s = [v[2] for v in non_null]\n",
    "            median_median = np.median(medians)\n",
    "            q1_min = min(q1s)\n",
    "            q3_max = max(q3s)\n",
    "            # Preserve integer if possible\n",
    "            frac = lambda x: int(x) if float(x).is_integer() else round(x, 2)\n",
    "            cons_val = f'{frac(median_median)} [{frac(q1_min)},{frac(q3_max)}]'\n",
    "            cons_values.append(cons_val)\n",
    "        else:\n",
    "            cons_values.append(row.get('consortium', None))\n",
    "    # Return new DataFrame with consortium column updated\n",
    "    return df.with_columns(pl.Series(\"consortium\", cons_values))\n",
    "\n",
    "# Recompute consortium_overalls with the above median[q1,q3] logic\n",
    "consortium_overalls_final = aggregate_median_q1_q3(consortium_overalls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b66f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consortium_overalls_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfad225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a row called 'years' capturing min-to-max year for each site, inferred from all 'table_one_by_year.csv' files\n",
    "\n",
    "csv_files = list(root_dir.glob('**/table_one_by_year.csv'))\n",
    "site_years = {}\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # Determine site name from folder, following logic as in lines 5-21\n",
    "    parts = csv_file.parts\n",
    "    try:\n",
    "        clif_idx = parts.index('CLIF-TableOne-2025')\n",
    "        folder_name = parts[clif_idx + 1]\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"'CLIF-TableOne-2025' not found in path: {csv_file}\")\n",
    "    except IndexError:\n",
    "        raise ValueError(f\"No folder after 'CLIF-TableOne-2025' in path: {csv_file}\")\n",
    "\n",
    "    df_tmp = pl.read_csv(csv_file)\n",
    "    year_columns = [col for col in df_tmp.columns if col not in (\"Variable\", \"Overall\")]\n",
    "    years = [int(col) for col in year_columns if str(col).isdigit()]\n",
    "    if years:\n",
    "        min_year, max_year = min(years), max(years)\n",
    "        if min_year != max_year:\n",
    "            years_string = f\"{min_year}-{max_year}\"\n",
    "        else:\n",
    "            years_string = str(min_year)\n",
    "    else:\n",
    "        years_string = \"\"\n",
    "    site_years[folder_name] = years_string\n",
    "\n",
    "# Also gather overall min/max year across all sites for the 'consortium' column\n",
    "all_years_flat = []\n",
    "for site, ystr in site_years.items():\n",
    "    if site == \"mimic_iv\":\n",
    "        continue\n",
    "    if '-' in ystr:\n",
    "        y1, y2 = ystr.split('-')\n",
    "        all_years_flat.extend([int(y1), int(y2)])\n",
    "    elif ystr:\n",
    "        all_years_flat.append(int(ystr))\n",
    "if all_years_flat:\n",
    "    cons_min, cons_max = min(all_years_flat), max(all_years_flat)\n",
    "    cons_years_string = f\"{cons_min}-{cons_max}\" if cons_min != cons_max else str(cons_min)\n",
    "else:\n",
    "    cons_years_string = \"\"\n",
    "\n",
    "# Use the SAME order and complete set of columns as in consortium_overalls_final, which includes:\n",
    "# ['Variable','upenn','nu','rush','ohsu','ucmc','umn','emory','umich','mimic_iv','consortium']\n",
    "years_row = {}\n",
    "for col in consortium_overalls_final.columns:\n",
    "    if col == \"Variable\":\n",
    "        years_row[col] = \"years\"\n",
    "    elif col == \"consortium\":\n",
    "        years_row[col] = cons_years_string\n",
    "    else:\n",
    "        years_row[col] = site_years.get(col, \"\")\n",
    "\n",
    "# Make sure years_row has keys for ALL columns in consortium_overalls_final.\n",
    "years_row_df = pl.DataFrame([years_row]).select(consortium_overalls_final.columns)\n",
    "\n",
    "# Insert the 'years' row as the first row\n",
    "final = pl.concat([years_row_df, consortium_overalls_final], how=\"vertical\")\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move 'consortium' column to the second position\n",
    "cols = final.columns\n",
    "if 'consortium' in cols:\n",
    "    new_cols = []\n",
    "    for i, col in enumerate(cols):\n",
    "        if i == 1:\n",
    "            new_cols.append('consortium')\n",
    "        if col != 'consortium':\n",
    "            new_cols.append(col)\n",
    "    final = final.select(new_cols)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.write_csv(\"overall.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17c58c",
   "metadata": {},
   "source": [
    "<!-- unaggregated  -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
