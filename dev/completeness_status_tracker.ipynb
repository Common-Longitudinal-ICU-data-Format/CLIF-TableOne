{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a259ef94",
   "metadata": {},
   "source": [
    "# Status Tracker Consolidator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_FOLDER_PATH = '/Users/dema/Library/CloudStorage/Box-Box/CLIF/projects/CLIF-TableOne-2025'\n",
    "\n",
    "# Convert to Path object\n",
    "root_dir = Path(BOX_FOLDER_PATH).expanduser()\n",
    "\n",
    "# Verify the path exists\n",
    "if not root_dir.exists():\n",
    "    raise FileNotFoundError(f\"Path does not exist: {root_dir}\")\n",
    "\n",
    "print(f\"Searching in: {root_dir.absolute()}\")\n",
    "print()\n",
    "\n",
    "# Find all JSON files inside any results/ folder\n",
    "json_files = list(root_dir.glob('**/results/*.json'))\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files in results/ folders:\")\n",
    "for file in json_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b414459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store table_name/status unique pairs for each folder\n",
    "unique_statuses = {}\n",
    "\n",
    "# Folder names to exclude (use lowercase for matching)\n",
    "excluded_folders = {}\n",
    "\n",
    "# First, gather per-folder mappings from json_files for easier lookup\n",
    "from collections import defaultdict\n",
    "\n",
    "# We'll build, for each folder_name, a dict: table_name -> (path, file_type)\n",
    "folder_table_type_files = defaultdict(dict)\n",
    "\n",
    "for json_file in json_files:\n",
    "    parts = json_file.parts\n",
    "    try:\n",
    "        clif_idx = parts.index('CLIF-TableOne-2025')\n",
    "        folder_name = parts[clif_idx + 1]\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"'CLIF-TableOne-2025' not found in path: {json_file}\")\n",
    "    except IndexError:\n",
    "        raise ValueError(f\"No folder after 'CLIF-TableOne-2025' in path: {json_file}\")\n",
    "\n",
    "    if folder_name.lower() in excluded_folders:\n",
    "        continue\n",
    "\n",
    "    name = json_file.name\n",
    "    table_name = None\n",
    "    file_type = None\n",
    "    if \"validation_response\" in name:\n",
    "        prefix = name.split('_validation_response')[0]\n",
    "        table_name = prefix\n",
    "        file_type = 'validation_response'\n",
    "    elif name.endswith('_summary_validation.json'):\n",
    "        table_name = name.split('_summary_validation.json')[0]\n",
    "        file_type = 'summary_validation'\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Only store one per type -- if multiple, last wins (shouldn't be the case)\n",
    "    folder_table_type_files[folder_name].setdefault(table_name, {})\n",
    "    folder_table_type_files[folder_name][table_name][file_type] = json_file\n",
    "\n",
    "# Now, for each folder, prefer validation_response for a table (if exists), else summary_validation\n",
    "for folder_name, table_files in folder_table_type_files.items():\n",
    "    status_records = []\n",
    "    for table_name, file_types in table_files.items():\n",
    "        if 'validation_response' in file_types:\n",
    "            json_file = file_types['validation_response']\n",
    "            status_key = 'adjusted_status'\n",
    "        elif 'summary_validation' in file_types:\n",
    "            json_file = file_types['summary_validation']\n",
    "            status_key = 'status'\n",
    "        else:\n",
    "            continue  # Should never happen\n",
    "\n",
    "        # Try to load status\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            status = data.get(status_key, None)\n",
    "            if status is None:\n",
    "                print(f\"No '{status_key}' key in {json_file}\")\n",
    "                continue\n",
    "            status_records.append({\n",
    "                \"table_name\": table_name,\n",
    "                \"status\": status\n",
    "            })\n",
    "            print(f\"Loaded status for table '{table_name}' from {folder_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read JSON file {json_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if status_records:\n",
    "        unique_statuses[folder_name] = pl.DataFrame(status_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80884388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all tables in unique_statuses on the 'table_name' column\n",
    "\n",
    "# First, convert all table_name values to lowercase for consistent joins\n",
    "for folder_name in unique_statuses:\n",
    "    if 'table_name' in unique_statuses[folder_name].columns:\n",
    "        unique_statuses[folder_name] = unique_statuses[folder_name].with_columns(\n",
    "            pl.col('table_name').str.to_lowercase().alias('table_name')\n",
    "        )\n",
    "\n",
    "# Prepare dfs for join, renaming the 'status' column to each site's/folder's name\n",
    "status_dfs = []\n",
    "for folder_name, df in unique_statuses.items():\n",
    "    df_status = df.clone()\n",
    "    df_status = df_status.rename({'status': f\"{folder_name}\"})\n",
    "    # To make sure there is never a duplicate \"table_name\" column, remove extra occurrences\n",
    "    # which may occur from previous joins, using select (and order is not strictly guaranteed, but fine)\n",
    "    if [col for col in df_status.columns if col == \"table_name\"].count(\"table_name\") > 1:\n",
    "        df_status = df_status.select(pl.col(\"table_name\"), pl.col(f\"{folder_name}\"))\n",
    "    status_dfs.append(df_status)\n",
    "\n",
    "# Perform a full outer join across all dataframes on 'table_name',\n",
    "# using a different suffix to avoid DuplicateError on table_name/right\n",
    "if len(status_dfs) > 0:\n",
    "    joined_status = status_dfs[0]\n",
    "    for i, next_df in enumerate(status_dfs[1:]):\n",
    "        # Use a custom suffix for each join, highly unlikely to collide with a real name\n",
    "        suffix = f\"_dupsite{i+1}\"\n",
    "        joined_status = joined_status.join(\n",
    "            next_df,\n",
    "            on=\"table_name\",\n",
    "            how=\"outer\",\n",
    "            suffix=suffix\n",
    "        )\n",
    "        # After the join, drop any duplicate 'table_name' columns (with the suffix)\n",
    "        right_table_name = f\"table_name{suffix}\"\n",
    "        if right_table_name in joined_status.columns:\n",
    "            joined_status = joined_status.drop(right_table_name)\n",
    "else:\n",
    "    joined_status = None\n",
    "\n",
    "joined_status.shape if joined_status is not None else (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_status = joined_status.sort(\"table_name\") if joined_status is not None else None\n",
    "\n",
    "if joined_status is not None:\n",
    "    counts = {}\n",
    "    row_count = 18  # as specified\n",
    "\n",
    "    # Gather columns to analyze (excluding 'table_name')\n",
    "    value_columns = [col for col in joined_status.columns if col != \"table_name\"]\n",
    "\n",
    "    # Count for each value: complete, partial, incomplete\n",
    "    for value in [\"complete\", \"partial\", \"incomplete\"]:\n",
    "        counts[value] = [\n",
    "            (joined_status[col] == value).sum() for col in value_columns\n",
    "        ]\n",
    "\n",
    "    # \"Missing\" is now defined as the number of missing entries (null or N/A) in each column\n",
    "    counts[\"missing\"] = [\n",
    "        joined_status[col].is_null().sum() for col in value_columns\n",
    "    ]\n",
    "\n",
    "    # Construct summary rows for each metric, showing counts instead of percentages\n",
    "    summary_rows = []\n",
    "    labels = [\"#complete\", \"#partial\", \"#incomplete\", \"#missing\"]\n",
    "    keys = [\"complete\", \"partial\", \"incomplete\", \"missing\"]\n",
    "    for i, label in enumerate(labels):\n",
    "        row = {\"table_name\": label}\n",
    "        current_counts = counts[keys[i]]\n",
    "        for j, col in enumerate(value_columns):\n",
    "            # Format the value as a string to match column type expectations\n",
    "            row[col] = str(current_counts[j])\n",
    "        summary_rows.append(row)\n",
    "\n",
    "    # Build summary_df as a DataFrame of string type\n",
    "    summary_df = pl.DataFrame(summary_rows)\n",
    "    # Ensure all columns (besides 'table_name') are UTF8 (string), for safe vertical concat\n",
    "    for col in value_columns:\n",
    "        if summary_df.schema[col] != pl.String:\n",
    "            summary_df = summary_df.with_columns([pl.col(col).cast(pl.String)])\n",
    "\n",
    "    # Also, ensure joined_status's value columns are all String for vertical concat\n",
    "    for col in value_columns:\n",
    "        if joined_status.schema[col] != pl.String:\n",
    "            joined_status = joined_status.with_columns([pl.col(col).cast(pl.String)])\n",
    "\n",
    "    # Prepare to append: re-order columns of summary_df to match joined_status\n",
    "    summary_df = summary_df.select(joined_status.columns)\n",
    "\n",
    "    # Append the summary (now types should match)\n",
    "    display_df = pl.concat([joined_status, summary_df])\n",
    "\n",
    "    display_df\n",
    "else:\n",
    "    joined_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f769d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_df.write_csv(\"../output/overall_status_by_site.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17c58c",
   "metadata": {},
   "source": [
    "<!-- unaggregated  -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
